{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment-03 First Step of Machine Learning: Model and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同学们，今天我们的学习了基本的机器学习概念，相比你已经对机器学习的这些方法有一个基本的认识了。值得说明的是，机器学习不仅仅是一系列方法，更重要的是一种思维体系，即：依据以往的、现有的数据，构建某种方法来解决未见过的问题。而且决策树，贝叶斯只是实现这个目标的一个方法，包括之后的神经网络。很有可能有一天，神经网络也会被淘汰，但是重要的是我们要理解机器学习的目标，就是尽可能的自动化解决未知的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1571556399207&di=4a97dc15ad08dd49d3748d1edf6109b3&imgtype=0&src=http%3A%2F%2Fc.hiphotos.baidu.com%2Fzhidao%2Fwh%3D450%2C600%2Fsign%3Dae742c6aedcd7b89e93932873a146e91%2F5d6034a85edf8db1b16050c40223dd54574e74c7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-1 Programming Review 编程回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Re-code the Linear-Regression Model using scikit-learning(10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>： \n",
    "> + 是否完成线性回归模型 (4')\n",
    "+ 能够进行预测新数据(3')\n",
    "+ 能够进行可视化操作(3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "import sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1655623 , 0.93586586],\n",
       "       [0.67527942, 0.87447707],\n",
       "       [0.16497231, 0.39573267],\n",
       "       [0.63439958, 0.28106088],\n",
       "       [0.8365015 , 0.91220784],\n",
       "       [0.07610268, 0.98449464],\n",
       "       [0.22045617, 0.44537457],\n",
       "       [0.63209226, 0.61428627],\n",
       "       [0.42364862, 0.65711382],\n",
       "       [0.90721036, 0.85993005],\n",
       "       [0.7744063 , 0.21390624],\n",
       "       [0.29038649, 0.93449802],\n",
       "       [0.29324167, 0.45231074],\n",
       "       [0.45921065, 0.72154028],\n",
       "       [0.52497536, 0.37159286],\n",
       "       [0.25317798, 0.49964007],\n",
       "       [0.98554464, 0.21965036],\n",
       "       [0.86420033, 0.3811978 ],\n",
       "       [0.44975615, 0.49731343],\n",
       "       [0.01473983, 0.08918558],\n",
       "       [0.70210817, 0.54113707],\n",
       "       [0.8330567 , 0.97353143],\n",
       "       [0.35380936, 0.63926557],\n",
       "       [0.17659138, 0.36882481],\n",
       "       [0.72929432, 0.84009297],\n",
       "       [0.1833514 , 0.31598801],\n",
       "       [0.01248422, 0.24273235],\n",
       "       [0.6495241 , 0.98687847],\n",
       "       [0.918223  , 0.36699047],\n",
       "       [0.10653746, 0.5649059 ],\n",
       "       [0.91800417, 0.25331002],\n",
       "       [0.60034016, 0.81636954],\n",
       "       [0.36192173, 0.41898925],\n",
       "       [0.17983583, 0.50488154],\n",
       "       [0.19185453, 0.65770456],\n",
       "       [0.61469467, 0.22765999],\n",
       "       [0.7273489 , 0.37739582],\n",
       "       [0.56546449, 0.58502147],\n",
       "       [0.95388606, 0.16168771],\n",
       "       [0.22416395, 0.11943501],\n",
       "       [0.45919192, 0.3276578 ],\n",
       "       [0.5772993 , 0.01394517],\n",
       "       [0.0590686 , 0.62918389],\n",
       "       [0.89598544, 0.20483522],\n",
       "       [0.72372593, 0.12928992],\n",
       "       [0.86985722, 0.21780602],\n",
       "       [0.21991392, 0.73328364],\n",
       "       [0.87374974, 0.298427  ],\n",
       "       [0.89788834, 0.73911506],\n",
       "       [0.8183925 , 0.5155154 ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_data = np.random.random((50, 2))\n",
    "random_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = random_data[:, 0], random_data[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: 8.6 * x + 3 + random.gauss(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [f(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x13bef7e9e10>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8066190172834417"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8.04035997]), 3.090330900606534)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13befb060f0>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.plot(X, model.coef_ * X + model.intercept_, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预测新数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.8943669 ,  7.11051088, 19.17105083])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([[0.1], [0.5], [2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Complete the unfinished KNN Model using pure python to solve the previous Line-Regression problem. (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>:\n",
    "> + 是否完成了KNN模型 (4')\n",
    "+ 是否能够预测新的数据 (4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "def model_zip(X, y):\n",
    "    return [(Xi, yi) for Xi, yi in zip(X, y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x1, x2):\n",
    "    return cosine(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, k=5):\n",
    "    most_similars = sorted(model_zip(X, y), key=lambda xi: distance(xi[0], x))[:k]\n",
    "    # 已经获得了最相似的数据集\n",
    "    # 然后呢，Counter() -> most_common() -> 就可以获得出现最多的这个y了 \n",
    "    return sum(i[1] for i in most_similars) / k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*线性回归的KNN取n个y的均值？*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.714575394312509"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(0.4, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Re-code the Decision Tree, which could sort the features by salience. (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否实现了信息熵 (1' )\n",
    "+ 是否实现了最优先特征点的选择(5')\n",
    "+ 是否实现了持续的特征选则(6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "from collections import Counter\n",
    "from icecream import ic\n",
    "\n",
    "def entropy(elements):\n",
    "    counter = Counter(elements)\n",
    "    probs = [counter[c] / len(elements) for c in elements]\n",
    "    return - sum(p * np.log(p) for p in probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**信息熵**\n",
    "$$entropy = \\sum - p\\cdot \\log p$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6986953329597843"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- sum(p*np.log(p) for p in [Counter([1,2,3,1,5])[c]/5 for c in [1,2,3,1,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6986953329597843"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([1,2,3,1,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0359469466922913"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([1,1,1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([3,3,3,3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data = {\n",
    "    'gender':['F', 'F', 'F', 'F', 'M', 'M', 'M'],\n",
    "    'income': ['+10', '-10', '+10', '+10', '+10', '+10', '-10'],\n",
    "    'family_number': [1, 1, 2, 1, 1, 1, 2],\n",
    "    'bought': [1, 1, 1, 0, 0, 0, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "1      F    -10              1       1\n",
       "2      F    +10              2       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0\n",
       "6      M    -10              2       1"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(mock_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9006824936634021\n",
      "1.6525187082781072\n",
      "1.6525187082781072\n",
      "-0.0\n"
     ]
    }
   ],
   "source": [
    "# split by gender:\n",
    "print(entropy([1,1,1,0]) + entropy([0,0,1]))\n",
    "\n",
    "# split by income:\n",
    "print(entropy([1,1,0,0,0]) + entropy([1,1]))\n",
    "\n",
    "# split by family number:\n",
    "print(entropy([1,1,0,0,0]) + entropy([1,1]))\n",
    "\n",
    "# spit by some feature:\n",
    "print(entropy([1, 1, 1, 1]) + entropy([0, 0, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树在选择决策过程，决策顺序的时候，其实是按照根据这个特征，进行分割以后，数据的熵最小的原则进行的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_min_spilter(training_data: pd.DataFrame, target: str) -> str:\n",
    "    '''\n",
    "    输入: Dataframe, target(y)\n",
    "    输出：最优分割\n",
    "    '''\n",
    "    x_fields = set(training_data.columns.tolist()) - {target}    # 减去target的所有列\n",
    "    \n",
    "    spliter = None\n",
    "    min_entropy = float('inf')\n",
    "    \n",
    "    for f in x_fields:\n",
    "        ic(f)\n",
    "        values = set(training_data[f])\n",
    "        ic(values)\n",
    "        for v in values:\n",
    "            sub_spliter_1 = training_data[training_data[f] == v][target].tolist()\n",
    "            ic(sub_spliter_1)\n",
    "            # split by the current feature and one value\n",
    "            \n",
    "            entropy_1 = entropy(sub_spliter_1)\n",
    "            ic(entropy_1)\n",
    "            \n",
    "            sub_spliter_2 = training_data[training_data[f] != v][target].tolist()\n",
    "            ic(sub_spliter_2)\n",
    "            \n",
    "            entropy_2 = entropy(sub_spliter_2)\n",
    "            ic(entropy_2)\n",
    "            \n",
    "            entropy_v = entropy_1 + entropy_2\n",
    "            ic(entropy_v)\n",
    "            \n",
    "            if entropy_v <= min_entropy:\n",
    "                min_entropy = entropy_v\n",
    "                spliter = (f, v)\n",
    "    \n",
    "    print('spliter is: {}'.format(spliter))\n",
    "    print('the min entropy is: {}'.format(min_entropy))\n",
    "    \n",
    "    return spliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'income'\n",
      "ic| values: {'+10', '-10'}\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 0]\n",
      "ic| entropy_1: 1.6525187082781072\n",
      "ic| sub_spliter_2: [1, 1]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 1.6525187082781072\n",
      "ic| sub_spliter_1: [1, 1]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_spliter_2: [1, 1, 0, 0, 0]\n",
      "ic| entropy_2: 1.6525187082781072\n",
      "ic| entropy_v: 1.6525187082781072\n",
      "ic| f: 'gender'\n",
      "ic| values: {'M', 'F'}\n",
      "ic| sub_spliter_1: [0, 0, 1]\n",
      "ic| entropy_1: 0.9068242403669224\n",
      "ic| sub_spliter_2: [1, 1, 1, 0]\n",
      "ic| entropy_2: 0.9938582532964797\n",
      "ic| entropy_v: 1.9006824936634021\n",
      "ic| sub_spliter_1: [1, 1, 1, 0]\n",
      "ic| entropy_1: 0.9938582532964797\n",
      "ic| sub_spliter_2: [0, 0, 1]\n",
      "ic| entropy_2: 0.9068242403669224\n",
      "ic| entropy_v: 1.9006824936634021\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1, 2}\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 0]\n",
      "ic| entropy_1: 1.6525187082781072\n",
      "ic| sub_spliter_2: [1, 1]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 1.6525187082781072\n",
      "ic| sub_spliter_1: [1, 1]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_spliter_2: [1, 1, 0, 0, 0]\n",
      "ic| entropy_2: 1.6525187082781072\n",
      "ic| entropy_v: 1.6525187082781072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('family_number', 2)\n",
      "the min entropy is: 1.6525187082781072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('family_number', 2)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_the_min_spilter(df, 'bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>M</td>\n",
       "      <td>-10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "2      F    +10              2       1\n",
       "6      M    -10              2       1"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['family_number'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "1      F    -10              1       1\n",
       "3      F    +10              1       0\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['family_number'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'income'\n",
      "ic| values: {'+10', '-10'}\n",
      "ic| sub_spliter_1: [1, 0, 0, 0]\n",
      "ic| entropy_1: 0.9938582532964797\n",
      "ic| sub_spliter_2: [1]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.9938582532964797\n",
      "ic| sub_spliter_1: [1]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_spliter_2: [1, 0, 0, 0]\n",
      "ic| entropy_2: 0.9938582532964797\n",
      "ic| entropy_v: 0.9938582532964797\n",
      "ic| f: 'gender'\n",
      "ic| values: {'M', 'F'}\n",
      "ic| sub_spliter_1: [0, 0]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_spliter_2: [1, 1, 0]\n",
      "ic| entropy_2: 0.9068242403669224\n",
      "ic| entropy_v: 0.9068242403669224\n",
      "ic| sub_spliter_1: [1, 1, 0]\n",
      "ic| entropy_1: 0.9068242403669224\n",
      "ic| sub_spliter_2: [0, 0]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.9068242403669224\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1}\n",
      "ic| sub_spliter_1: [1, 1, 0, 0, 0]\n",
      "ic| entropy_1: 1.6525187082781072\n",
      "ic| sub_spliter_2: []\n",
      "ic| entropy_2: 0\n",
      "ic| entropy_v: 1.6525187082781072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('gender', 'F')\n",
      "the min entropy is: 0.9068242403669224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('gender', 'F')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = df[df['family_number'] == 1]\n",
    "find_the_min_spilter(sub_df, 'bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "1      F    -10              1       1\n",
       "3      F    +10              1       0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df[sub_df['gender']=='F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>M</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "4      M    +10              1       0\n",
       "5      M    +10              1       0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df[sub_df['gender']=='M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| f: 'income'\n",
      "ic| values: {'+10', '-10'}\n",
      "ic| sub_spliter_1: [1, 0]\n",
      "ic| entropy_1: 0.6931471805599453\n",
      "ic| sub_spliter_2: [1]\n",
      "ic| entropy_2: -0.0\n",
      "ic| entropy_v: 0.6931471805599453\n",
      "ic| sub_spliter_1: [1]\n",
      "ic| entropy_1: -0.0\n",
      "ic| sub_spliter_2: [1, 0]\n",
      "ic| entropy_2: 0.6931471805599453\n",
      "ic| entropy_v: 0.6931471805599453\n",
      "ic| f: 'gender'\n",
      "ic| values: {'F'}\n",
      "ic| sub_spliter_1: [1, 1, 0]\n",
      "ic| entropy_1: 0.9068242403669224\n",
      "ic| sub_spliter_2: []\n",
      "ic| entropy_2: 0\n",
      "ic| entropy_v: 0.9068242403669224\n",
      "ic| f: 'family_number'\n",
      "ic| values: {1}\n",
      "ic| sub_spliter_1: [1, 1, 0]\n",
      "ic| entropy_1: 0.9068242403669224\n",
      "ic| sub_spliter_2: []\n",
      "ic| entropy_2: 0\n",
      "ic| entropy_v: 0.9068242403669224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spliter is: ('income', '-10')\n",
      "the min entropy is: 0.6931471805599453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('income', '-10')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_sub_df = sub_df[sub_df['gender']=='F']\n",
    "find_the_min_spilter(sub_sub_df, 'bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>-10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "1      F    -10              1       1"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_sub_df[sub_sub_df['income'] == '-10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>family_number</th>\n",
       "      <th>bought</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>+10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender income  family_number  bought\n",
       "0      F    +10              1       1\n",
       "3      F    +10              1       0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_sub_df[sub_sub_df['income'] == '+10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Finish the K-Means using 2-D matplotlib (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否完成了KMeans模型，基于scikit-learning (3')\n",
    "+ 是否完成了可视化任务（5'）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x13bf195da58>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = [random.randint(0, 100) for _ in range(100)]\n",
    "Y = [random.randint(0, 100) for _ in range(100)]\n",
    "\n",
    "plt.scatter(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "tranning_data = [[x, y] for x, y in zip(X, Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = KMeans(n_clusters=6, max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=500,\n",
       "       n_clusters=6, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.fit(tranning_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[83.05882353, 80.94117647],\n",
       "        [54.35      , 49.65      ],\n",
       "        [85.61111111, 28.44444444],\n",
       "        [25.95238095, 80.47619048],\n",
       "        [48.        ,  8.46153846],\n",
       "        [ 9.81818182, 28.27272727]]),\n",
       " array([1, 5, 5, 1, 1, 5, 1, 2, 3, 5, 0, 1, 4, 2, 1, 1, 1, 3, 3, 3, 2, 0,\n",
       "        5, 4, 3, 1, 2, 0, 4, 3, 5, 4, 3, 0, 4, 4, 2, 0, 1, 4, 2, 1, 2, 2,\n",
       "        3, 3, 1, 2, 3, 5, 4, 0, 1, 4, 0, 1, 1, 1, 0, 0, 0, 2, 2, 5, 5, 1,\n",
       "        3, 2, 4, 2, 0, 0, 2, 0, 3, 2, 2, 4, 0, 3, 3, 1, 1, 3, 5, 3, 3, 2,\n",
       "        3, 0, 5, 2, 3, 4, 0, 3, 3, 0, 4, 1]))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.cluster_centers_, cluster.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = defaultdict(list)\n",
    "for label, location in zip(cluster.labels_, tranning_data):\n",
    "    centers[label].append(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3de3Bc9ZXg8e/RC9MmCPwgAWx1mwpLkrGHh1UOeTAJEakimIdTFVKwyuBQ7GoqGWZwmK2ErCZFUVOqmWzNEmcqjy1tSGLWmhACCRiYJctqmMBmEogMBBkcArEl2QGCbYJJLDt6nf3j3nakVrfUj/v83fOpUkm6aqt/t1s+/etzz+/8RFUxxhjjlqa4B2CMMSZ4FtyNMcZBFtyNMcZBFtyNMcZBFtyNMcZBLXEPAGDFihVaKBTiHoYxxqTKzp07D6rqynI/S0RwLxQKDA0NxT0MY4xJFREZrfQzS8sYY4yDLLgbY4yDLLgbY4yDLLgbY4yDLLgbY4yDLLgbY4yDLLgbY4yDLLgbY4yDFg3uIvJNEXlNRHbNOrZMRB4RkRf9z6f6x0VE/klEXhKRZ0XkgjAHb9wwMDBAoVCgqamJQqHAwMBA3EMyThsACnjhr+B/755qZu7fBi4tOXYLMKiqZwOD/vcAHwHO9j96gK8HM0zjqoGBAXp6ehgdHUVVGR0dpaenxwK8CckAXmgaBdT/3IOLAX7R4K6qjwGvlxy+Ctjmf70N2DTr+J3q+SlwioicHtRgjXt6e3sZHx+fc2x8fJze3t6YRpQV2Zi9ztcLjJccG/ePu6XenPtbVfUVAP/zaf7xM4F9s2633z82j4j0iMiQiAwdOHCgzmGYtBsbG6vpuAlCdmav81X6u3Lv7y3oC6pS5ljZTVpVtV9VO1W1c+XKsk3NTAKEnQ/v6Oio6bgJQnZmr/NV+rty7++t3uD+m2K6xf/8mn98P7B61u1WAS/XPzwTpyjy4X19feRyuTnHcrkcfX19gd2HKZWd2et8fUCu5FjOP+6WeoP7DmCz//Vm4P5Zx6/zq2YuBA4X0zcmfaLIh3d3d9Pf308+n0dEyOfz9Pf3093dHdh9mFLZmb3O1w30A3m8REPe/969vzdRLZs1+eMNRL4DfBBYAfwGuBW4D7gb769hDLhaVV8XEQG+glddMw5cr6qLNmrv7OxU6+eePE1NTZT7+xARZmZmYhiRCUYx5z77hTuHq0HOZSKyU1U7y/1s0c06VPXaCj/qKnNbBf6ytuGZpOro6GB0dP5eAJYPT7tiAO/Fm5t14KUlLLDXb4CkPZ62QtVUZPlwl3UDI8CM/9kCe/2SWX1kwd1UZPlwk916+Foks/po0Zx7FCznbkwSWW6+Ok2Ur/gWvHdG4Vko524zd5NY1nMmbsmckSZPMquPLLibRLKeM0mQ5Xr4WiSzdt6Cu4lErbPwMGvs7R1BtcKakbqWx09o7byqxv6xfv16Ne7avn275nI5xUtMKqC5XE63b99e8d+IyJzbFz9EJPKxZNd2Vc3p3P+uOf94kn5ndgFDWiGu2gVVE7pCoVC2Xj6fzzMyMhLYvwlrLNkWdP12Aa9UsFQeryTT1MIuqJpY1dP5Mawae+tCWaug6+Etjx8VC+4mdPV0fgyrxt66UMYtmZUlLrLgbhq22AXKJK10TdJY3LTYxdJkVpY4qVIyPsoPu6CaXtVeoNy+fbvm83kVEc3n84tewAzzwmetYzHVqvZi6XZVzauq+J/t8a8XdkHVhMUufJo/KmAXS6NlF1RNaMK6QGkXPtPILpYmiQV305CwLlDahc80soulSWLB3TQkrAuUduEzjexiaZJYcDcNCatk0doNp1FCl+FnlF1QNcaYlLILqsYYE7hkN0BbdA9VY4wxpUo3MilurQdJSUPZzN0YY2qW/I1MLLgbY0zNkl/Tb8HdGGNqlvyafgvuxhhTs+TX9Dsd3G07NWNMOOqt6Y+uwsbZapniBsvFfTiLGywDthDGGBOAbmqrjIm2wsbZmXs9GyzbTD997DnLltFDR/jb+4ZZe+sPWXPLQ6y99Yf87X3DjB46EvfQqhBthY2zK1Sbmpood24iwszMzLzjpTN98HqZuLLkfWJsjEPf+hZv7niAmfFxmnI5Tr7yCpZffz1tKW3G5fpzZuZ69IXX+PT2p5icnmFq5o//t1uahNbmJr72iQu4+JzTYhzhYprwtiYoJXjbGNYukytUa+0qWM9MPy1+/9hj7LlqE2987x5mjhwBVWaOHOGN793Dnqs28fvHHqv4b2fPjFesWMGKFSsSM0t2+Tkzc40eOsKntz/F0cnpOYEdYGpGOTo5zae3P5XwGXy0FTYNBXcR+YyIPCciu0TkOyKyRETWiMgTIvKiiHxXRNqCGmwtau0q6Gr/8ImxMfbftAU9ehSmpub+cGoKPXqU/TdtYaLMeRZnxqOjo6gqhw4d4tChQ6jq8WsYcQZ4V58zM9//fHwPk9MLz24np2f4xuN7IxpRPaKtsKk7uIvImcBfA52quhZoBq4Bvgh8SVXPBn4L3BDEQGtVa1dBV/uHH/rWt9DJyQVvo5OTHPr2tnnHy82MZ4t7luzWc5bsPiVxu+/pl+fN2EtNzSg/ePrXEY2oHtF2zWw0LdMCnCgiLXgvQa8AHwLu8X++DdjU4H3Urbu7m5GREWZmZhgZGVkwD+tq//A3dzwwf8ZeamqKN3fsmHe4mhlwnLNkd56zYhXFKF5OtlhFYQG+6MgfFvkbLt5uorrbBa/aF+duvC0HZ/zP4V0bqju4q+qvgX/EW2/7CnAY2Am8oarFR3g/cGajg4yCq/3DZxaYec+53ZH5ucpqZsBxzpLdec6S36ckbktPqK5qe2lbHNXdyXxxbiQtcypwFbAGOANYCnykzE3LvpcSkR4RGRKRoQMHDtQ7jEDVMtNPi6ZcaY6vwu2WLp13rNzMeLYkzJLdeM6S36ckbpvOP4OWJlnwNi1NwkfPj2MumcwX50bSMpcAe1X1gKpOAt8H3guc4qdpAFYBL5f7x6rar6qdqtq5cuXKBoZhFnLylVdAyyKzmZYWTr7yynmHS2fGy5cvZ/ny5SmfJSdR8vuUxO0/X3QWrc0Lh6vW5ib+00VrIhrRbMl8cW4kuI8BF4pITkQE6AKeBx4FPubfZjNwf2NDNI1Yfv31SGvrgreR1laWf3Jz2Z/NnhkfPHiQgwcPpnyWnETJ71MSt/zypXztExdwYmvzvBl8S5NwYmszX/vEBeSXz38HGr5kvjg3knN/Au/C6VPAsP+7+oHPATeLyEvAcuCOAMZp6tTW0cGqL29FTjxx/gy+pQU58URWfXlrahcyucH2Hq3GxeecxsNbLuLaDR2cdEILInDSCS1cu6GDh7dcFOMCpmS+ODu7QrVaAwMD9Pb2MjY2RkdHB319fU7OSCfGxjj07W28uWMHM0eO0LR0KSdfeSXLP7nZArsJyQBe3nkMbxbbh7svWPGc60IrVDMd3G35ujFhKW2SBd5s1t6RBCmT7QeqYcvXy7NmXKZxyawgyRJnW/5Ww5avz2etkk0wkllBkiWZnrm7tXw9GPZuxgQjmRUkWZLp4O7O8vXg2LsZE4xkVpBkSaaD+0LL15Pc6jZM9m7GBCPu8k5rxIaqxv6xfv16TZLt27drLpdTvNYJ8z5yuZxu37497mGGoty5u3y+xkXbVTWnc8NMzj/uFmBIK8TVTJdCVlIoFBgdHV3wNvl8npGRkWgGFLGs1P4bVxXwmneVyuN1YnSH1bnXqNIWfbNV2q7PGBO34LezSyqrc69R0lvdJo3VxZtkqfR/U8lS/t2CexmLtboFuOyyyyIaTbKVbsWXhO33TNaVq9QpSkav9ShYWqaCYt65Uu7d5Zx7LSpdn7DHx8Sr2Oul0rUzN/LvlnNvQKX8u+XcPfb4mGRzO/++UHDPdPuBanR0dJSdmVrO3WOPzx+NHnydXftfZnxiklxbK2tXnUF+xbK4h5VxHZSfvbv/92k590XYKtaF2ePjGT34OjtHxhifmARgfGKSnSNjjB58PeaRZV12V8pacF+EO5swh8MeH8+u/S8zPTP37f/0jLJrf9ldJk1kFlsp6+5KVsu5GxOA7z35dMWfXb3h/AhHkmW1bpiR/p7zVuduTMhybeX3qa103AStGKhH8S6gVlPy6HbPeQvuxgRg7aozaC7ZuLm5SVi76oyYRpQ19QRqt3vOW3A3JgD5FctYX+g4PlPPtbWyvtBh1TKRqSdQu91z3kohjQlIfsUyC+axqafksY/yOXc3Kmls5m6McUA9JY9x95wPl83cjTEOKAbkWqpliv/OjWBeyoK7McYR7gbqelhwNyYBst26oNb6dFMNC+7GxKzYuqC4wrXYugDIQIAvXUhUrE8HC/CNsQuqKRDnZhi2EUf4st26wO2FRHGymXvCFTfDGB/3/gMUN8MAQu/fEud9Z0mx2Vi1x93i9kKiOFlvmYSLczMM24jDE3Y+/KFndpUN5Lm2Vjaetzaw+0mmAlnZzDoM1lsmxcbGys9gKh135b6TIopWvtluXZDdlrxhayi4i8gpInKPiPxCRHaLyHtEZJmIPCIiL/qfTw1qsK6pJp9dadOLKDbDiPO+kyKKfHi2Wxe4vZAoTo3O3L8MPKyq7wDOBXYDtwCDqno2MOh/b0pUu7F0nJth2EYc0eXD8yuWsfG8tVy94Xw2nrc2I4G9qBsvBTPjf7bAHoS6g7uInAz8GXAHgKpOqOobwFXANv9m24BNjQ7SRb29vccvVBaNj4/T2zu3SiDOzTBsIw5r5WvSq+4LqiJyHt77p+fxZu07gZuAX6vqKbNu91tVnZeaEZEe/ILWjo6O9eUu3LnMNpZOh9IadPDy4dlJm5gkC2uD7BbgAuCvVPUJEfkyNaRgVLUf78WBzs7O+Et2ImYbS8enluqX4vHsrh6tTbZX2iZLI8F9P7BfVZ/wv78HL7j/RkROV9VXROR04LVGB+mivr6+OTXkkL18dhzqWQ1qrXyrk+2VtslTd85dVV8F9onIOf6hLrwUzQ5gs39sM3B/QyN0lOWz45Ht1aDhGD34Og89s4sn94ym8LF1d4PsRleo/hUwICJtwB7gerxH6W4RuQFvmdnVDd6Hs7q7uy2YRyzbq0GDV+6aRKnkPrZu97VpKLir6jNAuWR+VyO/15iw5NpaK64GNbUr906oVHIf24X62qQ/uNsKVZMp2V4NGrzFZuXJfmzd7mtjwd1kSrZXgwZvoVl58h9b2yDbGKdUU/2ShpK+JIxx7aozUrwOwO0Nsi24G1MiDSV9SRljutcB1LvvajpYcDemxELlko0GraBm22GOsVbpXgfg7r6rFtyNKRFWuWSQs20r6TSLsQuqxpQIq1lYkAuorKGZWYwFd2NKhFUuGeRs20o6zWIsLWNMibAuEga5gCrdFzJrNzw8zODgIIcPH6a9vZ2uri7WrVsX97ASzYK7MWXUdZHw9T3w71+BZ++Gid9D20nwpx+H994Iy86qWDZY72w73Rcyqzc8PMwDDzzA5KT3wnj48GEeeOABAAvwC7C0jDFBePER+Pr74Kk7YeJ3gHqfn7rTO/7iI7aAqk6Dg4PHA3vR5OQkg4ODMY0oHWzmbkyjXt8Dd18Hk6V9SoCZSe/j7uvgUz8mv+IsC+Y1Onz4cE3Hjcdm7sY06t+/AtOLXBSdnoSffDWa8Timvb29puPGY8HdmEY9e7c3O1/IzCQ8+91oxuOYrq4uWlvnXnRubW2lq8uazy7E0jJ1GBgeoHewl7HDY3S0d9DX1Uf3OjdXuZkqTPw+2NvFKAn9akoVL5patUxtLLjXaGB4gJ4Hehj386ujh0fpecBr8G8BPqPaTvIvolZxuwRLSr+actatW2fBvEYW3GvUO9h7PLAXjU+O0zvYa8E9YEmcRZb1px/3qmIWSM3MSAt73trFC8/sivQ8ankMk9SvJknSWmNvOfcajR0u38i/0nFTn+IssrjopziLHD34eswjK+O9N0LzwguRZppa+OWaqyM9j1ofQ+tXM1+xxr5YmVOssR8eHo55ZIuz4F6jjvbyjfwrHTf1iWoj6+Lmzt978mkeemZXfUF32Vnw8TuhNQdNc4P8jLQw1byEn5x/G0eWnglEt2l0rY+h9auZL8019hbca9TX1UeuNTfnWK41R1+XGw3+kyKKWWSg7w7O/jB86sewfjOc8BYQgRPewp7Vl/N/3n8Hr5727jk3j2I2XOtjaP1q5ktzjb3l3GtUzKtbtUy4otjIOvAc87KzYON/9z58LzyzK7YNuWt9DLPWr6YaIoLq/A3ARaTMrZPFgnsdutd1WzAPWdB9WMqJ4t1BFOcR5H1npV9NtcoF9oWOJ4mlZUwi5VcsI798GcX5kQD55cEGnihyzHH2k7FeNo1L8+pYScIrUGdnpw4NDcU9jKrZIqbwldZcQ/AbLy90H2DpCTO/I+VsSSiLFJGdqtpZ7meWlqmRLWKKRhQ115VyzEBiF/OYaJWujp0t6a2HnU7LDAwPUNhaoOm2JgpbCwwMDzT8OxdaxBSFMM4piaKquc6vWMbG89Zy9Ybz2XjeWvIrlkVWhmnSYd26dWzZsqVsKibJZZHOztzDmmHHuYgpS+8aoqiWqcQW85hy0lYW6ezMPawZdpyLmOJ+1xClOGquiwuaKsnyYh6Tvourzgb3sGbYcS5iylLrg6grPUoXNJXK+mIek77Www2nZUSkGRgCfq2ql4vIGuAuYBnwFPDnqjrR6P3UqqO9g9HDo2WPNyLORUxhnVNSRVlzXS7PXmTVMgbS13o4iJz7TcBu4GT/+y8CX1LVu0TkfwA3AF8P4H5q0tfVNyc/DcHNsONaxBTmOWXdQvn0jeetjXAkJsnS1Hq4obSMiKwCNgLf8L8X4EPAPf5NtgGbGrmPenWv66b/in7y7XkEId+ep/+K/lRfeHTxnJLCmmYZ1zS0iElE7gH+HngL8F+ATwI/VdW3+z9fDfxvVZ039RGRHqAHoKOjY/3o6Px0gzFRiWLRlDFBC2URk4hcDrymqjtF5IPFw2VuWvbVQ1X7gX7wVqjWOw5jgmBNs9yS1g02gtRIzv19wJUichmwBC/nvhU4RURaVHUKWAXYyg+TCtY0yw2lLQOSvpI0LHXn3FX186q6SlULwDXAv6pqN/Ao8DH/ZpuB+xsepTHGVCnNG2wEKYw6988BN4vIS8By4I4Q7sMYY8pK20rSsATSfkBV/w34N//rPcCGIH6vMcbUqr29vWwgT+pK0rA4u0LVGJNNaVtJGhZnG4clifV/d8fowdetoibh0raSNCwW3EOWpU6Oriuthbc+78mVppWkYbHgHrKFOjlmKbinbcZbbrxRbCBi0i8pNfYW3EOWpU6OlaRtxltpvJUai1mfd1OUpBp7C+4hy1onx3LSNuOtNF6h/HJr6z+zuKTMZsO2UI191Odr1TIhi7P/e1KkbWejSuNSiHwDERcUZ7PF8sTibHZ4eDjmkQUvSTX2FtxDZp0c09dxcaHxRrmBiCuytGI0Sbs1WVomAnH1f0+KtavOKNtxMakz3oXGa/1napek2WxYZqedSsVVY2/B3YQubR0X0zbepHN9xWjpRdTZrFrGOC9tM960jTfJurq65gU/l1aMlks7gRfYt2zZEsOIPBbcjTGhinLF6L4397Ht+W08uOdBxifHybXmuPysy9n8rs2sPnl14PcHyU07ZTK419IO4I1XX2HooR+w+/FHmTh2jLYlS3jnRRfTufGjnPK20yMeuTHpFMWK0cf3P87NP7qZqekppnQKgCOTR7j3l/dy/6/u5/YP3M5Fqy4K/H6TmnbKXLVMsR3A6OFRFD3eDmBgeGDebfc+PcS2z97I8OAPmTh6FFSZOHqU4cEfsu2zN7L36aEYzsAYU2rfm/u4+Uc3c2zq2PHAXjSlUxybOsbNP7qZfW/uC/y+k9qorKE9VIPS2dmpQ0PRBMrC1kLZRUX59jwjW0aOf//Gq6+w7bM3MvWHP1T8XS0nnMDm//YVm8GbVHBlIVG587j/yP3c+8t75wX22USFd/zhHXzhvV8I/LzjemxD2UM1raptBzD00A+Ymar8hwIwMzXF0EP3cckNnwpsfMaEIUnL4htR6Tx2rN6xYGAHUFFebHsxlPNOYqOyzKVlKi37Lz2++/FHmZmeXvB3zUxPs/vxRwMbmzFhcWUhUaXzODp9tKp/PyVTqTzvemQuuFfbDmDi2LGqft/Eser+qIyJU1IrOmpVabwtWl0Soni7tJ13PTKXlilWxSxWLdO2ZIl3EXURbUtODGWcxgQpqRUdtap0Hm+feDsvnfjSojn3/O/zx39PkJJ4PSNzM3fwAvzIlhFmbp1hZMtI2TLId150MU3NzQv+nqbmZt550cVhDdOYwCS1oqNWlc7jL9b/BS3NC89Vm7SJs988O/DzTmpjtEwG92p0bvwoTS2L/LG0tNC5cVNEI/IMDA9Q2Fqg6bYmClsLZUs4E2PvANxXgH9u8j7vTfBYHbdu3TquuOKK4zPW9vZ2rrjiithnl7WqdB6XdF7C7R+4nSUtS2iRuf9vm2mmRVu48MCFnLn0zMDPO6nXMzJXClmLvU8PseNLf8/M1NSci6tNzc00tbRw5Wc+z5rzy1YhhaJ0yz7wrhckssvk3gF4sgemZ+1C1ZyDDf2wJmFjNc7Y9+Y+7nz+Th7c8yBHJo+wtHUpl591Ode967rQVqjedtttFX926623hnKfRQuVQlpwX4S3QvU+f4XqUdqWnOivUN0UeX17tTX6iXBfAcbnj5VcHjaNRD0aY0KzdevWitczwu4tY3XuDTjlbadzyQ2fSkQte6q27BuvMKZKx41JqaQ2RrOce4pUW6OfCLkKY6p03JiUSur1DJu5p0hfV1/ZnHsit+w7t698zv3cBI7VmAYlcYWqBfcUqbZGPxGKF01/3uulYnIdXmC3i6kmxZJYz16JXVA1xpgqlNtxqbW1NdYUzEIXVC3nbowxVUhqPXsldQd3EVktIo+KyG4ReU5EbvKPLxORR0TkRf/zqcEN1xhj4pG2/jyN5NyngL9R1adE5C3AThF5BPgkMKiq/yAitwC3AJ9rfKjGmIr2DtR0fSNNueOkSFt/nrpn7qr6iqo+5X/9O2A3cCZwFbDNv9k2INr1+cZkTXE18PgooN7nJ3sqtntIai+UpEtbf55Acu4iUgDOB54A3qqqr4D3AgCcFsR9GGMq+Hnv3JJT8L7/eW/Zm6ctd5wUSa1nr6ThUkgROQm4F9iiqm+KSLX/rgfoAejoCG5hSy2bXxvjhBpXA6ctd5wkSaxnr6ShmbuItOIF9gFV/b5/+Dcicrr/89OB18r9W1XtV9VOVe1cuXJlI8M4rpbNr41xRo2rgSvliJOaOzb1aaRaRoA7gN2qevusH+0ANvtfbwbur394tekd7J2zehNgfHKc3sHyb0+NccK5fd7q39kWWA2cltzx8PAwW7du5bbbbmPr1q12TaBGjaRl3gf8OTAsIs/4x/4r8A/A3SJyAzAGXN3YEKuXqsZaxgSlxtXAxbRCkqtlXNnQO051B3dV/X9ApQR7LFOAjvaOsi1xE9lYy5ggremuqbVD0nPHC130TfK4k8SpFarVbn5tjEk2u+jbOKeCe/e6bvqv6CffnkcQ8u35ZO5SZJLPtgiMlV30bZxzXSG713VbMDeNKd0isLgoCKyrZUSSugFGmjg1czcmEDUuCjLBS9uCoSRybuZuTMNsi8BESPpF36RzIrjbqtQMqLExVkNyHRU297aqKxdkpWla6tMytio1A2psjNWwGhcFmfTIUtO01Af3OFalDgwPUNhaoOm2JgpbC/ZCEraoc+BrumFDP+TygHifN/TbxVQHZKlpWurTMlGvSi2+Uyi+oBTfKQCWCgpLHDnwGhcFmXTIUv186mfulVafhrUqNXP9a6Ko917sPmpsjGVMJVmqn099cI96VWqm+tdEkeuu5j4sB24CkpamaUFIfXCPelVq1O8UYhVFrrua+7AcuAlIlurnRVXjHgOdnZ06NDQU9zCqUppzB++dgpNtDv65CSj39yHwH2fScx8VHD4wzjOP7OOFJ19l8tg0rUuaOWfD2zjvw6tpX5lb/BcYEzMR2amqneV+lvqZe9Qy1b8milx3TPn00V2HuOvvnuS5H7/M5LFpACaPTfPcj1/mrr97ktFdh0K9f2PCZsG9Dt3ruhnZMsLMrTOMbBlxM7BDNLnuGPLphw+M83D/MFMTM+j03HcNOq1MTczwcP8whw+MV/gNxiSfBXdTWRS57hjy6c88so/p6YXTkdPTyjP/d19oYzAmbKmvczchi6LeO+Ka8heefHXejL2UTiu/fOJVPnDtORGNyphg2czdZE4xx76YiT9Udztjkshm7iZzWpc0VxXg205ojmA0ZrasNPWKgs3cTeacs+FtSHOl7X890iz8h3e/LaIRGchWU68oWHA3mXPeh1fTvEhwb24WzrtkdUQjikECtxHMUlOvKFhwN5nTvjLHpT3raGlrmjeDl2ahpa2JS3vWubuQKeoWylXKUlOvKDgX3K0dr6lGfu1yrvnCBv7k/WfQtqQZBNqWNPMn7z+Da76wgfza5XEPMXjF2fpPPpHIbQSz1NQrCk5dULV2vKYW7StzfODac7JR7li66Xc5MW8jaJtiB8upmXvm2vGaYCUwDx2Ycg3aSsXcQjlLTb2i4NTMPVPteE2wSme2xTw0uNF9crFZeUJaKNum2MFxauaeqXa8WRbGDDvqrfyittCs3FooO8mp4B71xh0mBmFVesSxlV+UKjVoe8922DRigd1BTqVlihdNewd7GTs8Rkd7B31dfXYx1SULzbDrCVB7B/zZeYVeM65s5Vd8bH7e671g5Tq8gO9oULeVrrZZh0mbIDf3WKyCpDln6YoUKq50La26cfHibOSbdYjIpSLygoi8JCK3hHEfJqNq2dyjUm5+oXrv47/P8tBpZStdPYGnZUSkGfgq8GFgP/AzEdmhqs8HfV8mg87tmz/bLlfpUan65cCPYe+2RcoCxctDm1Syla6eMGbuG4CXVHWPqk4AdwFXhXA/Jouq3dyjUm7+V/2Jr/c2jbGVrp4wLqieCczewmY/8O7SG4lID9AD0NFh/5lMDarZ3KNSlYsu0uo3IaNHfKwAAAS7SURBVPXepn620tUTxsy9XLu9eVfAVLVfVTtVtXPlypUhDMNkWqXZtyzQo93y7E6wla6eMGbu+4HZvVJXAS+HcD/GVFYpN79m8/ycu1XFOMdWuoYzc/8ZcLaIrBGRNuAaYEcI92NMZZVy8xu+FvmG3MbEIfCZu6pOiciNwA+BZuCbqvpc0PdjzKIq5eYj3pDbmDiEskJVVf8F+JcwfrcxxpjFOdVbxhhjjMeCuzHGOMiCuzHGOMiCuzHGOMiCuzHGOMiCuzHGOMiCuzHGOCgRm3WIyAFgtMqbrwAOhjicJMvqudt5Z09Wz73W886ratnmXIkI7rUQkaFKO4+4LqvnbuedPVk99yDP29IyxhjjIAvuxhjjoDQG9/64BxCjrJ67nXf2ZPXcAzvv1OXcjTHGLC6NM3djjDGLsOBujDEOSlVwF5FLReQFEXlJRG6JezxhEZHVIvKoiOwWkedE5Cb/+DIReUREXvQ/nxr3WMMgIs0i8rSIPOh/v0ZEnvDP+7v+Dl/OEZFTROQeEfmF/9y/JwvPuYh8xv873yUi3xGRJa4+5yLyTRF5TUR2zTpW9jkWzz/58e5ZEbmglvtKTXAXkWbgq8BHgHcB14rIu+IdVWimgL9R1XcCFwJ/6Z/rLcCgqp4NDPrfu+gmYPes778IfMk/798CN8QyqvB9GXhYVd8BnIv3GDj9nIvImcBfA52quhZv97ZrcPc5/zZwacmxSs/xR4Cz/Y8e4Ou13FFqgjuwAXhJVfeo6gRwF3BVzGMKhaq+oqpP+V//Du8/+Zl457vNv9k2YFM8IwyPiKwCNgLf8L8X4EPAPf5NXD3vk4E/A+4AUNUJVX2DDDzneDvCnSgiLUAOeAVHn3NVfQx4veRwpef4KuBO9fwUOEVETq/2vtIU3M8E9s36fr9/zGkiUgDOB54A3qqqr4D3AgCcFt/IQrMV+Cww43+/HHhDVaf871193s8CDgDf8lNS3xCRpTj+nKvqr4F/BMbwgvphYCfZeM6LKj3HDcW8NAV3KXPM6TpOETkJuBfYoqpvxj2esInI5cBrqrpz9uEyN3XxeW8BLgC+rqrnA0dwLAVTjp9fvgpYA5wBLMVLR5Ry8TlfTEN/+2kK7vuB1bO+XwW8HNNYQicirXiBfUBVv+8f/k3xbZn/+bW4xheS9wFXisgIXtrtQ3gz+VP8t+zg7vO+H9ivqk/439+DF+xdf84vAfaq6gFVnQS+D7yXbDznRZWe44ZiXpqC+8+As/2r6G14F112xDymUPh55juA3ap6+6wf7QA2+19vBu6PemxhUtXPq+oqVS3gPb//qqrdwKPAx/ybOXfeAKr6KrBPRM7xD3UBz+P4c46XjrlQRHL+333xvJ1/zmep9BzvAK7zq2YuBA4X0zdVUdXUfACXAb8EfgX0xj2eEM/z/Xhvv54FnvE/LsPLPw8CL/qfl8U91hAfgw8CD/pfnwU8CbwEfA84Ie7xhXTO5wFD/vN+H3BqFp5z4DbgF8Au4H8BJ7j6nAPfwbu2MIk3M7+h0nOMl5b5qh/vhvEqiqq+L2s/YIwxDkpTWsYYY0yVLLgbY4yDLLgbY4yDLLgbY4yDLLgbY4yDLLgbY4yDLLgbY4yD/j+dcnu3cZLKQwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "color = ['#AAC9CE', 'green', 'grey', 'black', 'yellow', 'orange']\n",
    "\n",
    "# plot dots\n",
    "for i, c in enumerate(centers):\n",
    "    for location in centers[c]:\n",
    "        plt.scatter(*location, c=color[i])\n",
    "\n",
    "# plot centers\n",
    "for center in cluster.cluster_centers_:\n",
    "    plt.scatter(*center, s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-2 Question and Answer 问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What's the *model*? why  all the models are wrong, but some are useful? (5 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "模型是对现实世界规律的一种数学总结，模型可以很简单比如$y=x$也可以是一个模型，代表y和x完全相等，这样有x的值就可以用来预测y。\n",
    "所有模型都是错的是因为现实世界的规律是非常复杂的，因果关系错综复杂导致所有的模型都无法考虑到全部的情况（蝴蝶效应），因此模型只能一定程度上对现实世界的规律给出答案。一个模型总结规律的效果越好，那它就有用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对模型的理解是否正确,对模型的抽象性是否正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the underfitting and overfitting? List the reasons that could make model overfitting or underfitting. (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "欠拟合：模型对数据的拟合程度太差，导致Loss居高不下\n",
    "    原因：模型太简单\n",
    "过拟合：模型对训练数据的拟合得过于好，导致泛化性能下降，对于测试集的拟合程度反而降低\n",
    "    原因：模型太复杂，数据太少"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对过拟合和欠拟合的理解是否正确 (3')\n",
    "+ 对欠拟合产生的原因是否理解正确(2')\n",
    "+ 对过拟合产生的原因是否理解正确(5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What's the precision, recall, AUC, F1, F2score. What are they mainly target on? (12')"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "**Accuracy**: 准确率，公式为：$$\\frac{TP+TN}{TP+TN+FP+FN}$$，是一个模型分类正确的样本占总样本的比值。\n",
    "\n",
    "**Precision**: 精度，公式为：$$\\frac{TP}{TP+FP}$$，是模型分类正确的正例与所有被模型分为正例的比值。代表警察抓小偷，抓到的人里是小偷的比例\n",
    "\n",
    "**Recall**: 召回率，公式为：$$\\frac{TP}{TP+FN}$$，是模型分类正确的正例与所有真实情况为正例的样本的比值。代表警察抓小偷，从真小偷中抓出的小偷比例\n",
    "\n",
    "**AUC**: Area under Curve，y-axis是precision(TPR)，x-axis是threshold。引用一下一篇好文章：[从TP、FP、TN、FN到ROC曲线、miss rate、行人检测评估](http://www.mamicode.com/info-detail-1202377.html)。在Threshold往低处调时，正常来讲模型对正例更严格，我们希望它的精度越高越好，而当Threshold往高处调时，模型更宽松（理论上FP更少），我们依旧希望模型精度越高越好，所以可以画出一条曲线，这条曲线下面的面积越大则模型越好。\n",
    "\n",
    "![image.png](attachment:image.png)![]()\n",
    "\n",
    "\n",
    "**F1-score**: 公式为：$$F1-score=\\frac{2\\cdot precision\\cdot recall}{precision + recall}$$，综合考虑了Precision和Recall的指标。\n",
    "\n",
    "**F2-score**: 公式为：$$F2-score = 5 \\cdot \\frac{Precision \\cdot Recall}{4\\cdot Precision + Recall}$$\n",
    "\n",
    "\n",
    "$$F-Score = (1 + \\beta ^{2}) \\cdot \\frac{Precision \\cdot Recall}{\\beta ^{2}\\cdot Precision + Recall}$$\n",
    "\n",
    "当 $\\beta$ =1时，成为F1-Score，这时召回率和精确率都很重要，权重相同。当有些情况下我们认为精确率更为重要，那就调整 β 的值小于 1 ，如果我们认为召回率更加重要，那就调整 β的值大于1，比如F2-Score。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 对precision, recall, AUC, F1, F2 理解是否正确(6‘)\n",
    "+ 对precision, recall, AUC, F1, F2的使用侧重点是否理解正确 (6’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Based on our course and yourself mind, what's the machine learning?  (8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "机器学习是一种数据驱动的而非规则驱动的编程模式，它让数据能够自己说话，通过预先构建的模型，用数据进行拟合，最后得到一个能够预测的目的。\n",
    "机器学习是基于数学中的统计学，而传统编程用的是人为规则的编程实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，是否能说出来机器学习这种思维方式和传统的分析式编程的区别（8'）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. \"正确定义了机器学习模型的评价标准(evaluation)， 问题基本上就已经解决一半\". 这句话是否正确？你是怎么看待的？ (8‘)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "正确。一个好的评价模型能够在模型训练后更好的反应模型的好坏，以此来判断机器学习的模型是否需要改进，以及如何改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点> 开放式问题，主要看能理解评价指标对机器学习模型的重要性."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-03 Programming Practice 编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In our course and previous practice, we complete some importance components of Decision Tree. In this problem, you need to build a **completed** Decision Tree Model. You show finish a `predicate()` function, which accepts three parameters **<gender, income, family_number>**, and outputs the predicated 'bought': 1 or 0.  (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n",
    "def tree_predict(gender, income, family_number):\n",
    "    if family_number == 2: \n",
    "        return 1\n",
    "    elif family_number == 1:\n",
    "        if gender == 'M':\n",
    "            return 0\n",
    "        elif gender == 'F':\n",
    "            if income == '-10':\n",
    "                return 1\n",
    "            elif income == '+10':\n",
    "                return random.choice([1, 0])   # 最后一树枝不确定的时候根据分布随机选择，不知这样是否正确？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_predict('F', '+10', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<评阅点>\n",
    "> + 是否将之前的决策树模型的部分进行合并组装， predicate函数能够顺利运行(8')\n",
    "+ 是够能够输入未曾见过的X变量，例如gender, income, family_number 分别是： <M, -10, 1>, 模型能够预测出结果 (12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 将上一节课(第二节课)的线性回归问题中的Loss函数改成\"绝对值\"，并且改变其偏导的求值方式，观察其结果的变化。(19 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=dataset['data'],dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506,)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n",
       "       6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,\n",
       "       1.7800e+01, 3.9690e+02, 9.1400e+00])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1].shape\n",
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\""
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['DESCR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rm = x[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x13bf18eeda0>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the RM with respect to y\n",
    "plt.scatter(X_rm,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assume that the target funciton is a linear function\n",
    "$$ y = k*rm + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define target function\n",
    "def price(rm, k, b):\n",
    "    return k * rm + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define abs loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ loss = \\frac{1}{n} \\sum{|y_i - \\hat{y_i}|} \\\\= \\frac{1}{n} \\sum{|y_i - (kx_i + b_i)|}\\\\=\\begin{equation}\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "\\frac{1}{n} \\sum{(y_i - kx_i - b_i)}\\quad y_i > \\hat{y_i}\\\\\n",
    "\\frac{1}{n} \\sum{(- y_i + kx_i + b_i)}\\quad y_i < \\hat{y_i}\\\\\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function \n",
    "def loss(y,y_hat):\n",
    "    return sum((y_i - y_hat_i)**2 for y_i, y_hat_i in zip(list(y),list(y_hat)))/len(list(y))\n",
    "\n",
    "def abs_loss(y, y_hat):\n",
    "    return sum(abs(y_i - y_hat_i) for y_i, y_hat_i in zip(list(y),list(y_hat)))/len(list(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define partial derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial{loss}}{\\partial{k}} = \\begin{equation}\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "-\\frac{1}{n}\\sum x_i\\quad y_i > \\hat{y_i}\\\\\n",
    "\\frac{1}{n}\\sum x_i\\quad y_i < \\hat{y_i}\\\\\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial{loss}}{\\partial{b}} = \\begin{equation}\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "-\\frac{1}{n}\\sum 1 = -1 \\quad y_i > \\hat{y_i}\\\\\n",
    "\\frac{1}{n}\\sum 1 = 1 \\quad y_i < \\hat{y_i}\\\\\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define partial derivative \n",
    "def partial_derivative_k(x, y, y_hat):\n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for x_i, y_i, y_hat_i in zip(list(x),list(y),list(y_hat)):\n",
    "        if y_i >= y_hat_i:   \n",
    "            gradient += - x_i\n",
    "        else:\n",
    "            gradient += x_i\n",
    "        return 1/n * gradient\n",
    "\n",
    "def partial_derivative_b(y, y_hat):\n",
    "    n = len(y)\n",
    "    gradient = 0\n",
    "    for y_i, y_hat_i in zip(list(y),list(y_hat)):\n",
    "        if y_i >= y_hat_i:   \n",
    "            gradient += - 1\n",
    "        else:\n",
    "            gradient += 1\n",
    "    return 1/n * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, the loss is 366459.13453169103, parameters k is -95.00947814223315 and b is 18.737390281307526\n",
      "Iteration 1, the loss is 365159.0392405133, parameters k is -94.99648407108691 and b is 19.737390281307526\n",
      "Iteration 2, the loss is 363861.28410534654, parameters k is -94.98348999994067 and b is 20.737390281307526\n",
      "Iteration 3, the loss is 362565.869126192, parameters k is -94.97049592879443 and b is 21.737390281307526\n",
      "Iteration 4, the loss is 361272.7943030479, parameters k is -94.9575018576482 and b is 22.737390281307526\n",
      "Iteration 5, the loss is 359982.05963591463, parameters k is -94.94450778650196 and b is 23.737390281307526\n",
      "Iteration 6, the loss is 358693.6651247934, parameters k is -94.93151371535572 and b is 24.737390281307526\n",
      "Iteration 7, the loss is 357407.6107696831, parameters k is -94.91851964420948 and b is 25.737390281307526\n",
      "Iteration 8, the loss is 356123.89657058386, parameters k is -94.90552557306324 and b is 26.737390281307526\n",
      "Iteration 9, the loss is 354842.522527496, parameters k is -94.892531501917 and b is 27.737390281307526\n",
      "Iteration 10, the loss is 353563.488640419, parameters k is -94.87953743077077 and b is 28.737390281307526\n",
      "Iteration 11, the loss is 352286.7949093539, parameters k is -94.86654335962453 and b is 29.737390281307526\n",
      "Iteration 12, the loss is 351012.44133429904, parameters k is -94.85354928847829 and b is 30.737390281307526\n",
      "Iteration 13, the loss is 349740.42791525705, parameters k is -94.84055521733205 and b is 31.737390281307526\n",
      "Iteration 14, the loss is 348470.75465222506, parameters k is -94.82756114618581 and b is 32.737390281307526\n",
      "Iteration 15, the loss is 347203.42154520436, parameters k is -94.81456707503958 and b is 33.737390281307526\n",
      "Iteration 16, the loss is 345938.4285941957, parameters k is -94.80157300389334 and b is 34.737390281307526\n",
      "Iteration 17, the loss is 344675.7757991975, parameters k is -94.7885789327471 and b is 35.737390281307526\n",
      "Iteration 18, the loss is 343415.46316021035, parameters k is -94.77558486160086 and b is 36.737390281307526\n",
      "Iteration 19, the loss is 342157.4906772352, parameters k is -94.76259079045462 and b is 37.737390281307526\n",
      "Iteration 20, the loss is 340901.8583502707, parameters k is -94.74959671930839 and b is 38.737390281307526\n",
      "Iteration 21, the loss is 339648.5661793178, parameters k is -94.73660264816215 and b is 39.737390281307526\n",
      "Iteration 22, the loss is 338397.6141643757, parameters k is -94.72360857701591 and b is 40.737390281307526\n",
      "Iteration 23, the loss is 337149.00230544497, parameters k is -94.71061450586967 and b is 41.737390281307526\n",
      "Iteration 24, the loss is 335902.73060252605, parameters k is -94.69762043472343 and b is 42.737390281307526\n",
      "Iteration 25, the loss is 334658.7990556176, parameters k is -94.6846263635772 and b is 43.737390281307526\n",
      "Iteration 26, the loss is 333417.2076647205, parameters k is -94.67163229243096 and b is 44.737390281307526\n",
      "Iteration 27, the loss is 332177.9564298349, parameters k is -94.65863822128472 and b is 45.737390281307526\n",
      "Iteration 28, the loss is 330941.0453509604, parameters k is -94.64564415013848 and b is 46.737390281307526\n",
      "Iteration 29, the loss is 329706.4744280971, parameters k is -94.63265007899224 and b is 47.737390281307526\n",
      "Iteration 30, the loss is 328474.24366124463, parameters k is -94.619656007846 and b is 48.737390281307526\n",
      "Iteration 31, the loss is 327244.35305040434, parameters k is -94.60666193669977 and b is 49.737390281307526\n",
      "Iteration 32, the loss is 326016.8025955744, parameters k is -94.59366786555353 and b is 50.737390281307526\n",
      "Iteration 33, the loss is 324791.5922967563, parameters k is -94.58067379440729 and b is 51.737390281307526\n",
      "Iteration 34, the loss is 323568.72215394914, parameters k is -94.56767972326105 and b is 52.737390281307526\n",
      "Iteration 35, the loss is 322348.1921671534, parameters k is -94.55468565211481 and b is 53.737390281307526\n",
      "Iteration 36, the loss is 321130.0023363689, parameters k is -94.54169158096857 and b is 54.737390281307526\n",
      "Iteration 37, the loss is 319914.1526615958, parameters k is -94.52869750982234 and b is 55.737390281307526\n",
      "Iteration 38, the loss is 318700.6431428335, parameters k is -94.5157034386761 and b is 56.737390281307526\n",
      "Iteration 39, the loss is 317489.4737800824, parameters k is -94.50270936752986 and b is 57.737390281307526\n",
      "Iteration 40, the loss is 316280.6445733426, parameters k is -94.48971529638362 and b is 58.737390281307526\n",
      "Iteration 41, the loss is 315074.155522614, parameters k is -94.47672122523738 and b is 59.737390281307526\n",
      "Iteration 42, the loss is 313870.00662789703, parameters k is -94.46372715409115 and b is 60.737390281307526\n",
      "Iteration 43, the loss is 312668.197889191, parameters k is -94.45073308294491 and b is 61.737390281307526\n",
      "Iteration 44, the loss is 311468.72930649604, parameters k is -94.43773901179867 and b is 62.737390281307526\n",
      "Iteration 45, the loss is 310271.60087981273, parameters k is -94.42474494065243 and b is 63.737390281307526\n",
      "Iteration 46, the loss is 309076.8126091401, parameters k is -94.41175086950619 and b is 64.73739028130753\n",
      "Iteration 47, the loss is 307884.3644944792, parameters k is -94.39875679835995 and b is 65.73739028130753\n",
      "Iteration 48, the loss is 306694.2565358294, parameters k is -94.38576272721372 and b is 66.73739028130753\n",
      "Iteration 49, the loss is 305506.4887331905, parameters k is -94.37276865606748 and b is 67.73739028130753\n",
      "Iteration 50, the loss is 304321.0610865631, parameters k is -94.35977458492124 and b is 68.73739028130753\n",
      "Iteration 51, the loss is 303137.97359594685, parameters k is -94.346780513775 and b is 69.73739028130753\n",
      "Iteration 52, the loss is 301957.22626134206, parameters k is -94.33378644262876 and b is 70.73739028130753\n",
      "Iteration 53, the loss is 300778.81908274844, parameters k is -94.32079237148253 and b is 71.73739028130753\n",
      "Iteration 54, the loss is 299602.7520601659, parameters k is -94.30779830033629 and b is 72.73739028130753\n",
      "Iteration 55, the loss is 298429.02519359457, parameters k is -94.29480422919005 and b is 73.73739028130753\n",
      "Iteration 56, the loss is 297257.6384830343, parameters k is -94.28181015804381 and b is 74.73739028130753\n",
      "Iteration 57, the loss is 296088.5919284858, parameters k is -94.26881608689757 and b is 75.73739028130753\n",
      "Iteration 58, the loss is 294921.88552994834, parameters k is -94.25582201575133 and b is 76.73739028130753\n",
      "Iteration 59, the loss is 293757.51928742224, parameters k is -94.2428279446051 and b is 77.73739028130753\n",
      "Iteration 60, the loss is 292595.49320090725, parameters k is -94.22983387345886 and b is 78.73739028130753\n",
      "Iteration 61, the loss is 291435.8072704034, parameters k is -94.21683980231262 and b is 79.73739028130753\n",
      "Iteration 62, the loss is 290278.46149591036, parameters k is -94.20384573116638 and b is 80.73739028130753\n",
      "Iteration 63, the loss is 289123.45587742893, parameters k is -94.19085166002014 and b is 81.73739028130753\n",
      "Iteration 64, the loss is 287970.79041495867, parameters k is -94.1778575888739 and b is 82.73739028130753\n",
      "Iteration 65, the loss is 286820.4651084994, parameters k is -94.16486351772767 and b is 83.73739028130753\n",
      "Iteration 66, the loss is 285672.4799580519, parameters k is -94.15186944658143 and b is 84.73739028130753\n",
      "Iteration 67, the loss is 284526.83496361563, parameters k is -94.13887537543519 and b is 85.73739028130753\n",
      "Iteration 68, the loss is 283383.5301251903, parameters k is -94.12588130428895 and b is 86.73739028130753\n",
      "Iteration 69, the loss is 282242.5654427763, parameters k is -94.11288723314271 and b is 87.73739028130753\n",
      "Iteration 70, the loss is 281103.94091637357, parameters k is -94.09989316199648 and b is 88.73739028130753\n",
      "Iteration 71, the loss is 279967.65654598206, parameters k is -94.08689909085024 and b is 89.73739028130753\n",
      "Iteration 72, the loss is 278833.7123316015, parameters k is -94.073905019704 and b is 90.73739028130753\n",
      "Iteration 73, the loss is 277702.1082732326, parameters k is -94.06091094855776 and b is 91.73739028130753\n",
      "Iteration 74, the loss is 276572.84437087475, parameters k is -94.04791687741152 and b is 92.73739028130753\n",
      "Iteration 75, the loss is 275445.92062452785, parameters k is -94.03492280626529 and b is 93.73739028130753\n",
      "Iteration 76, the loss is 274321.3370341922, parameters k is -94.02192873511905 and b is 94.73739028130753\n",
      "Iteration 77, the loss is 273199.09359986824, parameters k is -94.00893466397281 and b is 95.73739028130753\n",
      "Iteration 78, the loss is 272079.19032155577, parameters k is -93.99594059282657 and b is 96.73739028130753\n",
      "Iteration 79, the loss is 270961.6271992535, parameters k is -93.98294652168033 and b is 97.73739028130753\n",
      "Iteration 80, the loss is 269846.4042329635, parameters k is -93.9699524505341 and b is 98.73739028130753\n",
      "Iteration 81, the loss is 268733.5214226841, parameters k is -93.95695837938786 and b is 99.73739028130753\n",
      "Iteration 82, the loss is 267622.9787684159, parameters k is -93.94396430824162 and b is 100.73739028130753\n",
      "Iteration 83, the loss is 266514.7762701592, parameters k is -93.93097023709538 and b is 101.73739028130753\n",
      "Iteration 84, the loss is 265408.91392791376, parameters k is -93.91797616594914 and b is 102.73739028130753\n",
      "Iteration 85, the loss is 264305.3917416794, parameters k is -93.9049820948029 and b is 103.73739028130753\n",
      "Iteration 86, the loss is 263204.20971145603, parameters k is -93.89198802365667 and b is 104.73739028130753\n",
      "Iteration 87, the loss is 262105.3678372444, parameters k is -93.87899395251043 and b is 105.73739028130753\n",
      "Iteration 88, the loss is 261008.8661190436, parameters k is -93.86599988136419 and b is 106.73739028130753\n",
      "Iteration 89, the loss is 259914.70455685424, parameters k is -93.85300581021795 and b is 107.73739028130753\n",
      "Iteration 90, the loss is 258822.8831506758, parameters k is -93.84001173907171 and b is 108.73739028130753\n",
      "Iteration 91, the loss is 257733.40190050908, parameters k is -93.82701766792547 and b is 109.73739028130753\n",
      "Iteration 92, the loss is 256646.2608063531, parameters k is -93.81402359677924 and b is 110.73739028130753\n",
      "Iteration 93, the loss is 255561.45986820885, parameters k is -93.801029525633 and b is 111.73739028130753\n",
      "Iteration 94, the loss is 254478.99908607564, parameters k is -93.78803545448676 and b is 112.73739028130753\n",
      "Iteration 95, the loss is 253398.87845995364, parameters k is -93.77504138334052 and b is 113.73739028130753\n",
      "Iteration 96, the loss is 252321.09798984288, parameters k is -93.76204731219428 and b is 114.73739028130753\n",
      "Iteration 97, the loss is 251245.65767574307, parameters k is -93.74905324104805 and b is 115.73739028130753\n",
      "Iteration 98, the loss is 250172.5575176549, parameters k is -93.73605916990181 and b is 116.73739028130753\n",
      "Iteration 99, the loss is 249101.7975155779, parameters k is -93.72306509875557 and b is 117.73739028130753\n",
      "Iteration 100, the loss is 248033.37766951206, parameters k is -93.71007102760933 and b is 118.73739028130753\n",
      "Iteration 101, the loss is 246967.29797945748, parameters k is -93.69707695646309 and b is 119.73739028130753\n",
      "Iteration 102, the loss is 245903.55844541377, parameters k is -93.68408288531685 and b is 120.73739028130753\n",
      "Iteration 103, the loss is 244842.15906738155, parameters k is -93.67108881417062 and b is 121.73739028130753\n",
      "Iteration 104, the loss is 243783.0998453609, parameters k is -93.65809474302438 and b is 122.73739028130753\n",
      "Iteration 105, the loss is 242726.38077935087, parameters k is -93.64510067187814 and b is 123.73739028130753\n",
      "Iteration 106, the loss is 241672.00186935245, parameters k is -93.6321066007319 and b is 124.73739028130753\n",
      "Iteration 107, the loss is 240619.96311536513, parameters k is -93.61911252958566 and b is 125.73739028130753\n",
      "Iteration 108, the loss is 239570.26451738924, parameters k is -93.60611845843943 and b is 126.73739028130753\n",
      "Iteration 109, the loss is 238522.90607542414, parameters k is -93.59312438729319 and b is 127.73739028130753\n",
      "Iteration 110, the loss is 237477.88778947075, parameters k is -93.58013031614695 and b is 128.7373902813075\n",
      "Iteration 111, the loss is 236435.20965952842, parameters k is -93.56713624500071 and b is 129.7373902813075\n",
      "Iteration 112, the loss is 235394.8716855973, parameters k is -93.55414217385447 and b is 130.7373902813075\n",
      "Iteration 113, the loss is 234356.8738676777, parameters k is -93.54114810270823 and b is 131.7373902813075\n",
      "Iteration 114, the loss is 233321.21620576884, parameters k is -93.528154031562 and b is 132.7373902813075\n",
      "Iteration 115, the loss is 232287.89869987132, parameters k is -93.51515996041576 and b is 133.7373902813075\n",
      "Iteration 116, the loss is 231256.92134998523, parameters k is -93.50216588926952 and b is 134.7373902813075\n",
      "Iteration 117, the loss is 230228.28415611008, parameters k is -93.48917181812328 and b is 135.7373902813075\n",
      "Iteration 118, the loss is 229201.98711824664, parameters k is -93.47617774697704 and b is 136.7373902813075\n",
      "Iteration 119, the loss is 228178.03023639377, parameters k is -93.4631836758308 and b is 137.7373902813075\n",
      "Iteration 120, the loss is 227156.41351055284, parameters k is -93.45018960468457 and b is 138.7373902813075\n",
      "Iteration 121, the loss is 226137.13694072267, parameters k is -93.43719553353833 and b is 139.7373902813075\n",
      "Iteration 122, the loss is 225120.2005269037, parameters k is -93.42420146239209 and b is 140.7373902813075\n",
      "Iteration 123, the loss is 224105.6042690962, parameters k is -93.41120739124585 and b is 141.7373902813075\n",
      "Iteration 124, the loss is 223093.3481672999, parameters k is -93.39821332009961 and b is 142.7373902813075\n",
      "Iteration 125, the loss is 222083.43222151487, parameters k is -93.38521924895338 and b is 143.7373902813075\n",
      "Iteration 126, the loss is 221075.8564317409, parameters k is -93.37222517780714 and b is 144.7373902813075\n",
      "Iteration 127, the loss is 220070.62079797822, parameters k is -93.3592311066609 and b is 145.7373902813075\n",
      "Iteration 128, the loss is 219067.7253202269, parameters k is -93.34623703551466 and b is 146.7373902813075\n",
      "Iteration 129, the loss is 218067.1699984868, parameters k is -93.33324296436842 and b is 147.7373902813075\n",
      "Iteration 130, the loss is 217068.95483275785, parameters k is -93.32024889322219 and b is 148.7373902813075\n",
      "Iteration 131, the loss is 216073.07982304, parameters k is -93.30725482207595 and b is 149.7373902813075\n",
      "Iteration 132, the loss is 215079.54496933336, parameters k is -93.29426075092971 and b is 150.7373902813075\n",
      "Iteration 133, the loss is 214088.35027163825, parameters k is -93.28126667978347 and b is 151.7373902813075\n",
      "Iteration 134, the loss is 213099.49572995407, parameters k is -93.26827260863723 and b is 152.7373902813075\n",
      "Iteration 135, the loss is 212112.98134428132, parameters k is -93.255278537491 and b is 153.7373902813075\n",
      "Iteration 136, the loss is 211128.80711461997, parameters k is -93.24228446634476 and b is 154.7373902813075\n",
      "Iteration 137, the loss is 210146.97304096955, parameters k is -93.22929039519852 and b is 155.7373902813075\n",
      "Iteration 138, the loss is 209167.47912333038, parameters k is -93.21629632405228 and b is 156.7373902813075\n",
      "Iteration 139, the loss is 208190.3253617023, parameters k is -93.20330225290604 and b is 157.7373902813075\n",
      "Iteration 140, the loss is 207215.51175608567, parameters k is -93.1903081817598 and b is 158.7373902813075\n",
      "Iteration 141, the loss is 206243.03830648022, parameters k is -93.17731411061357 and b is 159.7373902813075\n",
      "Iteration 142, the loss is 205272.90501288616, parameters k is -93.16432003946733 and b is 160.7373902813075\n",
      "Iteration 143, the loss is 204305.111875303, parameters k is -93.15132596832109 and b is 161.7373902813075\n",
      "Iteration 144, the loss is 203339.65889373125, parameters k is -93.13833189717485 and b is 162.7373902813075\n",
      "Iteration 145, the loss is 202376.54606817084, parameters k is -93.12533782602861 and b is 163.7373902813075\n",
      "Iteration 146, the loss is 201415.77339862144, parameters k is -93.11234375488237 and b is 164.7373902813075\n",
      "Iteration 147, the loss is 200457.34088508354, parameters k is -93.09934968373614 and b is 165.7373902813075\n",
      "Iteration 148, the loss is 199501.24852755672, parameters k is -93.0863556125899 and b is 166.7373902813075\n",
      "Iteration 149, the loss is 198547.4963260412, parameters k is -93.07336154144366 and b is 167.7373902813075\n",
      "Iteration 150, the loss is 197596.08428053657, parameters k is -93.06036747029742 and b is 168.7373902813075\n",
      "Iteration 151, the loss is 196647.01239104377, parameters k is -93.04737339915118 and b is 169.7373902813075\n",
      "Iteration 152, the loss is 195700.2806575618, parameters k is -93.03437932800495 and b is 170.7373902813075\n",
      "Iteration 153, the loss is 194755.88908009126, parameters k is -93.02138525685871 and b is 171.7373902813075\n",
      "Iteration 154, the loss is 193813.83765863185, parameters k is -93.00839118571247 and b is 172.7373902813075\n",
      "Iteration 155, the loss is 192874.1263931833, parameters k is -92.99539711456623 and b is 173.7373902813075\n",
      "Iteration 156, the loss is 191936.7552837464, parameters k is -92.98240304341999 and b is 174.7373902813075\n",
      "Iteration 157, the loss is 191001.72433032095, parameters k is -92.96940897227375 and b is 175.7373902813075\n",
      "Iteration 158, the loss is 190069.0335329063, parameters k is -92.95641490112752 and b is 176.7373902813075\n",
      "Iteration 159, the loss is 189138.68289150312, parameters k is -92.94342082998128 and b is 177.7373902813075\n",
      "Iteration 160, the loss is 188210.67240611097, parameters k is -92.93042675883504 and b is 178.7373902813075\n",
      "Iteration 161, the loss is 187285.00207673002, parameters k is -92.9174326876888 and b is 179.7373902813075\n",
      "Iteration 162, the loss is 186361.6719033605, parameters k is -92.90443861654256 and b is 180.7373902813075\n",
      "Iteration 163, the loss is 185440.68188600216, parameters k is -92.89144454539633 and b is 181.7373902813075\n",
      "Iteration 164, the loss is 184522.03202465503, parameters k is -92.87845047425009 and b is 182.7373902813075\n",
      "Iteration 165, the loss is 183605.72231931915, parameters k is -92.86545640310385 and b is 183.7373902813075\n",
      "Iteration 166, the loss is 182691.7527699947, parameters k is -92.85246233195761 and b is 184.7373902813075\n",
      "Iteration 167, the loss is 181780.12337668106, parameters k is -92.83946826081137 and b is 185.7373902813075\n",
      "Iteration 168, the loss is 180870.8341393787, parameters k is -92.82647418966513 and b is 186.7373902813075\n",
      "Iteration 169, the loss is 179963.88505808776, parameters k is -92.8134801185189 and b is 187.7373902813075\n",
      "Iteration 170, the loss is 179059.27613280795, parameters k is -92.80048604737266 and b is 188.7373902813075\n",
      "Iteration 171, the loss is 178157.0073635393, parameters k is -92.78749197622642 and b is 189.7373902813075\n",
      "Iteration 172, the loss is 177257.07875028267, parameters k is -92.77449790508018 and b is 190.7373902813075\n",
      "Iteration 173, the loss is 176359.4902930366, parameters k is -92.76150383393394 and b is 191.7373902813075\n",
      "Iteration 174, the loss is 175464.24199180142, parameters k is -92.7485097627877 and b is 192.7373902813075\n",
      "Iteration 175, the loss is 174571.33384657773, parameters k is -92.73551569164147 and b is 193.7373902813075\n",
      "Iteration 176, the loss is 173680.7658573655, parameters k is -92.72252162049523 and b is 194.7373902813075\n",
      "Iteration 177, the loss is 172792.53802416456, parameters k is -92.70952754934899 and b is 195.7373902813075\n",
      "Iteration 178, the loss is 171906.65034697452, parameters k is -92.69653347820275 and b is 196.7373902813075\n",
      "Iteration 179, the loss is 171023.1028257957, parameters k is -92.68353940705651 and b is 197.7373902813075\n",
      "Iteration 180, the loss is 170141.89546062838, parameters k is -92.67054533591028 and b is 198.7373902813075\n",
      "Iteration 181, the loss is 169263.02825147228, parameters k is -92.65755126476404 and b is 199.7373902813075\n",
      "Iteration 182, the loss is 168386.50119832723, parameters k is -92.6445571936178 and b is 200.7373902813075\n",
      "Iteration 183, the loss is 167512.3143011934, parameters k is -92.63156312247156 and b is 201.7373902813075\n",
      "Iteration 184, the loss is 166640.467560071, parameters k is -92.61856905132532 and b is 202.7373902813075\n",
      "Iteration 185, the loss is 165770.96097495957, parameters k is -92.60557498017909 and b is 203.7373902813075\n",
      "Iteration 186, the loss is 164903.79454585962, parameters k is -92.59258090903285 and b is 204.7373902813075\n",
      "Iteration 187, the loss is 164038.96827277087, parameters k is -92.57958683788661 and b is 205.7373902813075\n",
      "Iteration 188, the loss is 163176.48215569346, parameters k is -92.56659276674037 and b is 206.7373902813075\n",
      "Iteration 189, the loss is 162316.3361946269, parameters k is -92.55359869559413 and b is 207.7373902813075\n",
      "Iteration 190, the loss is 161458.53038957156, parameters k is -92.5406046244479 and b is 208.7373902813075\n",
      "Iteration 191, the loss is 160603.0647405278, parameters k is -92.52761055330166 and b is 209.7373902813075\n",
      "Iteration 192, the loss is 159749.93924749503, parameters k is -92.51461648215542 and b is 210.7373902813075\n",
      "Iteration 193, the loss is 158899.15391047375, parameters k is -92.50162241100918 and b is 211.7373902813075\n",
      "Iteration 194, the loss is 158050.70872946343, parameters k is -92.48862833986294 and b is 212.7373902813075\n",
      "Iteration 195, the loss is 157204.60370446465, parameters k is -92.4756342687167 and b is 213.7373902813075\n",
      "Iteration 196, the loss is 156360.83883547693, parameters k is -92.46264019757047 and b is 214.7373902813075\n",
      "Iteration 197, the loss is 155519.4141225003, parameters k is -92.44964612642423 and b is 215.7373902813075\n",
      "Iteration 198, the loss is 154680.329565535, parameters k is -92.43665205527799 and b is 216.7373902813075\n",
      "Iteration 199, the loss is 153843.58516458093, parameters k is -92.42365798413175 and b is 217.7373902813075\n",
      "Iteration 200, the loss is 153009.1809196379, parameters k is -92.41066391298551 and b is 218.7373902813075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 201, the loss is 152177.11683070648, parameters k is -92.39766984183927 and b is 219.7373902813075\n",
      "Iteration 202, the loss is 151347.3928977862, parameters k is -92.38467577069304 and b is 220.7373902813075\n",
      "Iteration 203, the loss is 150520.0091208771, parameters k is -92.3716816995468 and b is 221.7373902813075\n",
      "Iteration 204, the loss is 149694.96549997912, parameters k is -92.35868762840056 and b is 222.7373902813075\n",
      "Iteration 205, the loss is 148872.26203509248, parameters k is -92.34569355725432 and b is 223.7373902813075\n",
      "Iteration 206, the loss is 148051.89872621692, parameters k is -92.33269948610808 and b is 224.7373902813075\n",
      "Iteration 207, the loss is 147233.8755733528, parameters k is -92.31970541496185 and b is 225.7373902813075\n",
      "Iteration 208, the loss is 146418.19257649995, parameters k is -92.3067113438156 and b is 226.7373902813075\n",
      "Iteration 209, the loss is 145604.84973565803, parameters k is -92.29371727266937 and b is 227.7373902813075\n",
      "Iteration 210, the loss is 144793.8470508276, parameters k is -92.28072320152313 and b is 228.7373902813075\n",
      "Iteration 211, the loss is 143985.18452200812, parameters k is -92.26772913037689 and b is 229.7373902813075\n",
      "Iteration 212, the loss is 143178.8621492, parameters k is -92.25473505923065 and b is 230.7373902813075\n",
      "Iteration 213, the loss is 142374.87993240333, parameters k is -92.24174098808442 and b is 231.7373902813075\n",
      "Iteration 214, the loss is 141573.23787161775, parameters k is -92.22874691693818 and b is 232.7373902813075\n",
      "Iteration 215, the loss is 140773.93596684354, parameters k is -92.21575284579194 and b is 233.7373902813075\n",
      "Iteration 216, the loss is 139976.97421808017, parameters k is -92.2027587746457 and b is 234.7373902813075\n",
      "Iteration 217, the loss is 139182.35262532838, parameters k is -92.18976470349946 and b is 235.7373902813075\n",
      "Iteration 218, the loss is 138390.07118858752, parameters k is -92.17677063235323 and b is 236.7373902813075\n",
      "Iteration 219, the loss is 137600.12990785844, parameters k is -92.16377656120699 and b is 237.7373902813075\n",
      "Iteration 220, the loss is 136812.5287831399, parameters k is -92.15078249006075 and b is 238.7373902813075\n",
      "Iteration 221, the loss is 136027.2678144332, parameters k is -92.13778841891451 and b is 239.7373902813075\n",
      "Iteration 222, the loss is 135244.34700173722, parameters k is -92.12479434776827 and b is 240.7373902813075\n",
      "Iteration 223, the loss is 134463.76634505278, parameters k is -92.11180027662203 and b is 241.7373902813075\n",
      "Iteration 224, the loss is 133685.5258443794, parameters k is -92.0988062054758 and b is 242.7373902813075\n",
      "Iteration 225, the loss is 132909.62549971737, parameters k is -92.08581213432956 and b is 243.7373902813075\n",
      "Iteration 226, the loss is 132136.06531106657, parameters k is -92.07281806318332 and b is 244.7373902813075\n",
      "Iteration 227, the loss is 131364.84527842703, parameters k is -92.05982399203708 and b is 245.7373902813075\n",
      "Iteration 228, the loss is 130595.96540179841, parameters k is -92.04682992089084 and b is 246.7373902813075\n",
      "Iteration 229, the loss is 129829.42568118143, parameters k is -92.0338358497446 and b is 247.7373902813075\n",
      "Iteration 230, the loss is 129065.22611657566, parameters k is -92.02084177859837 and b is 248.7373902813075\n",
      "Iteration 231, the loss is 128303.36670798091, parameters k is -92.00784770745213 and b is 249.7373902813075\n",
      "Iteration 232, the loss is 127543.84745539728, parameters k is -91.99485363630589 and b is 250.7373902813075\n",
      "Iteration 233, the loss is 126786.66835882516, parameters k is -91.98185956515965 and b is 251.7373902813075\n",
      "Iteration 234, the loss is 126031.82941826415, parameters k is -91.96886549401341 and b is 252.7373902813075\n",
      "Iteration 235, the loss is 125279.33063371462, parameters k is -91.95587142286718 and b is 253.7373902813075\n",
      "Iteration 236, the loss is 124529.17200517585, parameters k is -91.94287735172094 and b is 254.7373902813075\n",
      "Iteration 237, the loss is 123781.35353264857, parameters k is -91.9298832805747 and b is 255.7373902813075\n",
      "Iteration 238, the loss is 123035.87521613252, parameters k is -91.91688920942846 and b is 256.7373902813075\n",
      "Iteration 239, the loss is 122292.73705562776, parameters k is -91.90389513828222 and b is 257.7373902813075\n",
      "Iteration 240, the loss is 121551.93905113413, parameters k is -91.89090106713599 and b is 258.7373902813075\n",
      "Iteration 241, the loss is 120813.48120265176, parameters k is -91.87790699598975 and b is 259.7373902813075\n",
      "Iteration 242, the loss is 120077.36351018058, parameters k is -91.86491292484351 and b is 260.7373902813075\n",
      "Iteration 243, the loss is 119343.58597372069, parameters k is -91.85191885369727 and b is 261.7373902813075\n",
      "Iteration 244, the loss is 118612.14859327205, parameters k is -91.83892478255103 and b is 262.7373902813075\n",
      "Iteration 245, the loss is 117883.05136883463, parameters k is -91.8259307114048 and b is 263.7373902813075\n",
      "Iteration 246, the loss is 117156.2943004083, parameters k is -91.81293664025856 and b is 264.7373902813075\n",
      "Iteration 247, the loss is 116431.87738799323, parameters k is -91.79994256911232 and b is 265.7373902813075\n",
      "Iteration 248, the loss is 115709.80063158955, parameters k is -91.78694849796608 and b is 266.7373902813075\n",
      "Iteration 249, the loss is 114990.06403119698, parameters k is -91.77395442681984 and b is 267.7373902813075\n",
      "Iteration 250, the loss is 114272.66758681575, parameters k is -91.7609603556736 and b is 268.7373902813075\n",
      "Iteration 251, the loss is 113557.61129844567, parameters k is -91.74796628452737 and b is 269.7373902813075\n",
      "Iteration 252, the loss is 112844.89516608695, parameters k is -91.73497221338113 and b is 270.7373902813075\n",
      "Iteration 253, the loss is 112134.51918973922, parameters k is -91.72197814223489 and b is 271.7373902813075\n",
      "Iteration 254, the loss is 111426.48336940286, parameters k is -91.70898407108865 and b is 272.7373902813075\n",
      "Iteration 255, the loss is 110720.78770507773, parameters k is -91.69598999994241 and b is 273.7373902813075\n",
      "Iteration 256, the loss is 110017.43219676378, parameters k is -91.68299592879617 and b is 274.7373902813075\n",
      "Iteration 257, the loss is 109316.41684446119, parameters k is -91.67000185764994 and b is 275.7373902813075\n",
      "Iteration 258, the loss is 108617.74164816979, parameters k is -91.6570077865037 and b is 276.7373902813075\n",
      "Iteration 259, the loss is 107921.40660788944, parameters k is -91.64401371535746 and b is 277.7373902813075\n",
      "Iteration 260, the loss is 107227.41172362043, parameters k is -91.63101964421122 and b is 278.7373902813075\n",
      "Iteration 261, the loss is 106535.75699536271, parameters k is -91.61802557306498 and b is 279.7373902813075\n",
      "Iteration 262, the loss is 105846.44242311614, parameters k is -91.60503150191875 and b is 280.7373902813075\n",
      "Iteration 263, the loss is 105159.4680068809, parameters k is -91.5920374307725 and b is 281.7373902813075\n",
      "Iteration 264, the loss is 104474.83374665669, parameters k is -91.57904335962627 and b is 282.7373902813075\n",
      "Iteration 265, the loss is 103792.53964244378, parameters k is -91.56604928848003 and b is 283.7373902813075\n",
      "Iteration 266, the loss is 103112.58569424239, parameters k is -91.55305521733379 and b is 284.7373902813075\n",
      "Iteration 267, the loss is 102434.97190205196, parameters k is -91.54006114618755 and b is 285.7373902813075\n",
      "Iteration 268, the loss is 101759.69826587276, parameters k is -91.52706707504132 and b is 286.7373902813075\n",
      "Iteration 269, the loss is 101086.76478570477, parameters k is -91.51407300389508 and b is 287.7373902813075\n",
      "Iteration 270, the loss is 100416.1714615481, parameters k is -91.50107893274884 and b is 288.7373902813075\n",
      "Iteration 271, the loss is 99747.91829340279, parameters k is -91.4880848616026 and b is 289.7373902813075\n",
      "Iteration 272, the loss is 99082.00528126844, parameters k is -91.47509079045636 and b is 290.7373902813075\n",
      "Iteration 273, the loss is 98418.43242514544, parameters k is -91.46209671931013 and b is 291.7373902813075\n",
      "Iteration 274, the loss is 97757.19972503386, parameters k is -91.44910264816389 and b is 292.7373902813075\n",
      "Iteration 275, the loss is 97098.30718093328, parameters k is -91.43610857701765 and b is 293.7373902813075\n",
      "Iteration 276, the loss is 96441.75479284389, parameters k is -91.42311450587141 and b is 294.7373902813075\n",
      "Iteration 277, the loss is 95787.54256076578, parameters k is -91.41012043472517 and b is 295.7373902813075\n",
      "Iteration 278, the loss is 95135.67048469905, parameters k is -91.39712636357893 and b is 296.7373902813075\n",
      "Iteration 279, the loss is 94486.13856464333, parameters k is -91.3841322924327 and b is 297.7373902813075\n",
      "Iteration 280, the loss is 93838.946800599, parameters k is -91.37113822128646 and b is 298.7373902813075\n",
      "Iteration 281, the loss is 93194.09519256577, parameters k is -91.35814415014022 and b is 299.7373902813075\n",
      "Iteration 282, the loss is 92551.58374054382, parameters k is -91.34515007899398 and b is 300.7373902813075\n",
      "Iteration 283, the loss is 91911.41244453321, parameters k is -91.33215600784774 and b is 301.7373902813075\n",
      "Iteration 284, the loss is 91273.58130453376, parameters k is -91.3191619367015 and b is 302.7373902813075\n",
      "Iteration 285, the loss is 90638.0903205455, parameters k is -91.30616786555527 and b is 303.7373902813075\n",
      "Iteration 286, the loss is 90004.93949256856, parameters k is -91.29317379440903 and b is 304.7373902813075\n",
      "Iteration 287, the loss is 89374.12882060274, parameters k is -91.28017972326279 and b is 305.7373902813075\n",
      "Iteration 288, the loss is 88745.65830464818, parameters k is -91.26718565211655 and b is 306.7373902813075\n",
      "Iteration 289, the loss is 88119.52794470495, parameters k is -91.25419158097031 and b is 307.7373902813075\n",
      "Iteration 290, the loss is 87495.73774077286, parameters k is -91.24119750982408 and b is 308.7373902813075\n",
      "Iteration 291, the loss is 86874.28769285197, parameters k is -91.22820343867784 and b is 309.7373902813075\n",
      "Iteration 292, the loss is 86255.17780094239, parameters k is -91.2152093675316 and b is 310.7373902813075\n",
      "Iteration 293, the loss is 85638.40806504406, parameters k is -91.20221529638536 and b is 311.7373902813075\n",
      "Iteration 294, the loss is 85023.97848515684, parameters k is -91.18922122523912 and b is 312.7373902813075\n",
      "Iteration 295, the loss is 84411.88906128089, parameters k is -91.17622715409289 and b is 313.7373902813075\n",
      "Iteration 296, the loss is 83802.13979341612, parameters k is -91.16323308294665 and b is 314.7373902813075\n",
      "Iteration 297, the loss is 83194.73068156271, parameters k is -91.15023901180041 and b is 315.7373902813075\n",
      "Iteration 298, the loss is 82589.66172572045, parameters k is -91.13724494065417 and b is 316.7373902813075\n",
      "Iteration 299, the loss is 81986.93292588956, parameters k is -91.12425086950793 and b is 317.7373902813075\n",
      "Iteration 300, the loss is 81386.54428206959, parameters k is -91.1112567983617 and b is 318.7373902813075\n",
      "Iteration 301, the loss is 80788.49579426125, parameters k is -91.09826272721546 and b is 319.7373902813075\n",
      "Iteration 302, the loss is 80192.78746246385, parameters k is -91.08526865606922 and b is 320.7373902813075\n",
      "Iteration 303, the loss is 79599.41928667773, parameters k is -91.07227458492298 and b is 321.7373902813075\n",
      "Iteration 304, the loss is 79008.39126690284, parameters k is -91.05928051377674 and b is 322.7373902813075\n",
      "Iteration 305, the loss is 78419.70340313931, parameters k is -91.0462864426305 and b is 323.7373902813075\n",
      "Iteration 306, the loss is 77833.35569538687, parameters k is -91.03329237148427 and b is 324.7373902813075\n",
      "Iteration 307, the loss is 77249.34814364578, parameters k is -91.02029830033803 and b is 325.7373902813075\n",
      "Iteration 308, the loss is 76667.6807479158, parameters k is -91.00730422919179 and b is 326.7373902813075\n",
      "Iteration 309, the loss is 76088.3535081971, parameters k is -90.99431015804555 and b is 327.7373902813075\n",
      "Iteration 310, the loss is 75511.36642448968, parameters k is -90.98131608689931 and b is 328.7373902813075\n",
      "Iteration 311, the loss is 74936.71949679343, parameters k is -90.96832201575307 and b is 329.7373902813075\n",
      "Iteration 312, the loss is 74364.41272510844, parameters k is -90.95532794460684 and b is 330.7373902813075\n",
      "Iteration 313, the loss is 73794.44610943466, parameters k is -90.9423338734606 and b is 331.7373902813075\n",
      "Iteration 314, the loss is 73226.81964977212, parameters k is -90.92933980231436 and b is 332.7373902813075\n",
      "Iteration 315, the loss is 72661.53334612078, parameters k is -90.91634573116812 and b is 333.7373902813075\n",
      "Iteration 316, the loss is 72098.58719848077, parameters k is -90.90335166002188 and b is 334.7373902813075\n",
      "Iteration 317, the loss is 71537.9812068519, parameters k is -90.89035758887564 and b is 335.7373902813075\n",
      "Iteration 318, the loss is 70979.71537123421, parameters k is -90.8773635177294 and b is 336.7373902813075\n",
      "Iteration 319, the loss is 70423.78969162783, parameters k is -90.86436944658317 and b is 337.7373902813075\n",
      "Iteration 320, the loss is 69870.20416803268, parameters k is -90.85137537543693 and b is 338.7373902813075\n",
      "Iteration 321, the loss is 69318.95880044879, parameters k is -90.83838130429069 and b is 339.7373902813075\n",
      "Iteration 322, the loss is 68770.05358887602, parameters k is -90.82538723314445 and b is 340.7373902813075\n",
      "Iteration 323, the loss is 68223.48853331454, parameters k is -90.81239316199822 and b is 341.7373902813075\n",
      "Iteration 324, the loss is 67679.26363376436, parameters k is -90.79939909085198 and b is 342.7373902813075\n",
      "Iteration 325, the loss is 67137.37889022539, parameters k is -90.78640501970574 and b is 343.7373902813075\n",
      "Iteration 326, the loss is 66597.8343026976, parameters k is -90.7734109485595 and b is 344.7373902813075\n",
      "Iteration 327, the loss is 66060.62987118101, parameters k is -90.76041687741326 and b is 345.7373902813075\n",
      "Iteration 328, the loss is 65525.765595675635, parameters k is -90.74742280626702 and b is 346.7373902813075\n",
      "Iteration 329, the loss is 64993.24147618162, parameters k is -90.73442873512079 and b is 347.7373902813075\n",
      "Iteration 330, the loss is 64463.057512698724, parameters k is -90.72143466397455 and b is 348.7373902813075\n",
      "Iteration 331, the loss is 63935.21370522706, parameters k is -90.70844059282831 and b is 349.7373902813075\n",
      "Iteration 332, the loss is 63409.71005376667, parameters k is -90.69544652168207 and b is 350.7373902813075\n",
      "Iteration 333, the loss is 62888.44935239917, parameters k is -90.68245245053583 and b is 351.7334377121376\n",
      "Iteration 334, the loss is 62369.5117368974, parameters k is -90.6694583793896 and b is 352.72948514296763\n",
      "Iteration 335, the loss is 61852.89720726132, parameters k is -90.65646430824336 and b is 353.7255325737977\n",
      "Iteration 336, the loss is 61338.60576349109, parameters k is -90.64347023709712 and b is 354.72158000462775\n",
      "Iteration 337, the loss is 60826.63740558647, parameters k is -90.63047616595088 and b is 355.7176274354578\n",
      "Iteration 338, the loss is 60316.99213354774, parameters k is -90.61748209480464 and b is 356.7136748662879\n",
      "Iteration 339, the loss is 59809.66994737464, parameters k is -90.6044880236584 and b is 357.70972229711793\n",
      "Iteration 340, the loss is 59304.67084706728, parameters k is -90.59149395251217 and b is 358.705769727948\n",
      "Iteration 341, the loss is 58801.99483262585, parameters k is -90.57849988136593 and b is 359.70181715877806\n",
      "Iteration 342, the loss is 58301.64190405009, parameters k is -90.56550581021969 and b is 360.6978645896081\n",
      "Iteration 343, the loss is 57803.612061339976, parameters k is -90.55251173907345 and b is 361.6939120204382\n",
      "Iteration 344, the loss is 57307.90530449567, parameters k is -90.53951766792721 and b is 362.68995945126824\n",
      "Iteration 345, the loss is 56814.52163351707, parameters k is -90.52652359678098 and b is 363.6860068820983\n",
      "Iteration 346, the loss is 56323.46104840423, parameters k is -90.51352952563474 and b is 364.68205431292836\n",
      "Iteration 347, the loss is 55834.72354915724, parameters k is -90.5005354544885 and b is 365.6781017437584\n",
      "Iteration 348, the loss is 55348.30913577593, parameters k is -90.48754138334226 and b is 366.6741491745885\n",
      "Iteration 349, the loss is 54864.21780826032, parameters k is -90.47454731219602 and b is 367.67019660541854\n",
      "Iteration 350, the loss is 54382.44956661057, parameters k is -90.46155324104978 and b is 368.6662440362486\n",
      "Iteration 351, the loss is 53903.004410826514, parameters k is -90.44855916990355 and b is 369.66229146707866\n",
      "Iteration 352, the loss is 53425.88234090817, parameters k is -90.43556509875731 and b is 370.6583388979087\n",
      "Iteration 353, the loss is 52951.08335685558, parameters k is -90.42257102761107 and b is 371.6543863287388\n",
      "Iteration 354, the loss is 52478.60745866875, parameters k is -90.40957695646483 and b is 372.65043375956884\n",
      "Iteration 355, the loss is 52010.1700437762, parameters k is -90.3965828853186 and b is 373.6425286212289\n",
      "Iteration 356, the loss is 51544.038707095315, parameters k is -90.38358881417236 and b is 374.63462348288897\n",
      "Iteration 357, the loss is 51080.21344862584, parameters k is -90.37059474302612 and b is 375.62671834454903\n",
      "Iteration 358, the loss is 50618.69426836783, parameters k is -90.35760067187988 and b is 376.6188132062091\n",
      "Iteration 359, the loss is 50159.481166321195, parameters k is -90.34460660073364 and b is 377.61090806786916\n",
      "Iteration 360, the loss is 49702.574142486155, parameters k is -90.3316125295874 and b is 378.6030029295292\n",
      "Iteration 361, the loss is 49247.97319686255, parameters k is -90.31861845844116 and b is 379.5950977911893\n",
      "Iteration 362, the loss is 48795.678329450435, parameters k is -90.30562438729493 and b is 380.58719265284935\n",
      "Iteration 363, the loss is 48345.68954024985, parameters k is -90.29263031614869 and b is 381.5792875145094\n",
      "Iteration 364, the loss is 47898.00682926066, parameters k is -90.27963624500245 and b is 382.5713823761695\n",
      "Iteration 365, the loss is 47452.63019648292, parameters k is -90.26664217385621 and b is 383.56347723782955\n",
      "Iteration 366, the loss is 47009.55964191671, parameters k is -90.25364810270997 and b is 384.5555720994896\n",
      "Iteration 367, the loss is 46568.79516556202, parameters k is -90.24065403156374 and b is 385.5476669611497\n",
      "Iteration 368, the loss is 46131.94184943668, parameters k is -90.2276599604175 and b is 386.5358092536398\n",
      "Iteration 369, the loss is 45697.37766635971, parameters k is -90.21466588927126 and b is 387.5239515461299\n",
      "Iteration 370, the loss is 45266.69081567725, parameters k is -90.20167181812502 and b is 388.5081412694501\n",
      "Iteration 371, the loss is 44838.2762153713, parameters k is -90.18867774697878 and b is 389.4923309927703\n",
      "Iteration 372, the loss is 44412.133865441945, parameters k is -90.17568367583254 and b is 390.4765207160905\n",
      "Iteration 373, the loss is 43988.26376588927, parameters k is -90.1626896046863 and b is 391.46071043941066\n",
      "Iteration 374, the loss is 43566.665916713064, parameters k is -90.14969553354007 and b is 392.44490016273085\n",
      "Iteration 375, the loss is 43147.340317913404, parameters k is -90.13670146239383 and b is 393.42908988605103\n",
      "Iteration 376, the loss is 42730.286969490386, parameters k is -90.12370739124759 and b is 394.4132796093712\n",
      "Iteration 377, the loss is 42315.505871443835, parameters k is -90.11071332010135 and b is 395.3974693326914\n",
      "Iteration 378, the loss is 41902.99702377394, parameters k is -90.09771924895512 and b is 396.3816590560116\n",
      "Iteration 379, the loss is 41492.76042648064, parameters k is -90.08472517780888 and b is 397.36584877933177\n",
      "Iteration 380, the loss is 41084.79607956386, parameters k is -90.07173110666264 and b is 398.35003850265196\n",
      "Iteration 381, the loss is 40679.10398302373, parameters k is -90.0587370355164 and b is 399.33422822597214\n",
      "Iteration 382, the loss is 40275.68413686016, parameters k is -90.04574296437016 and b is 400.3184179492923\n",
      "Iteration 383, the loss is 39874.53654107305, parameters k is -90.03274889322392 and b is 401.3026076726125\n",
      "Iteration 384, the loss is 39475.66119566256, parameters k is -90.01975482207769 and b is 402.2867973959327\n",
      "Iteration 385, the loss is 39080.51994552357, parameters k is -90.00676075093145 and b is 403.2670345500829\n",
      "Iteration 386, the loss is 38687.63412558051, parameters k is -89.99376667978521 and b is 404.24727170423307\n",
      "Iteration 387, the loss is 38297.00373583333, parameters k is -89.98077260863897 and b is 405.22750885838326\n",
      "Iteration 388, the loss is 37908.62877628213, parameters k is -89.96777853749273 and b is 406.20774601253345\n",
      "Iteration 389, the loss is 37522.50924692691, parameters k is -89.9547844663465 and b is 407.18798316668364\n",
      "Iteration 390, the loss is 37138.64514776756, parameters k is -89.94179039520026 and b is 408.1682203208338\n",
      "Iteration 391, the loss is 36757.03647880416, parameters k is -89.92879632405402 and b is 409.148457474984\n",
      "Iteration 392, the loss is 36377.68324003672, parameters k is -89.91580225290778 and b is 410.1286946291342\n",
      "Iteration 393, the loss is 36000.58543146524, parameters k is -89.90280818176154 and b is 411.1089317832844\n",
      "Iteration 394, the loss is 35625.74305308963, parameters k is -89.8898141106153 and b is 412.0891689374346\n",
      "Iteration 395, the loss is 35253.15610491002, parameters k is -89.87682003946907 and b is 413.06940609158477\n",
      "Iteration 396, the loss is 34882.82458692635, parameters k is -89.86382596832283 and b is 414.04964324573496\n",
      "Iteration 397, the loss is 34516.10964166891, parameters k is -89.85083189717659 and b is 415.0259278307152\n",
      "Iteration 398, the loss is 34151.63336891792, parameters k is -89.83783782603035 and b is 416.00221241569545\n",
      "Iteration 399, the loss is 33789.39576867348, parameters k is -89.82484375488411 and b is 416.9784970006757\n",
      "Iteration 400, the loss is 33429.396840935595, parameters k is -89.81184968373788 and b is 417.95478158565595\n",
      "Iteration 401, the loss is 33071.636585704226, parameters k is -89.79885561259164 and b is 418.9310661706362\n",
      "Iteration 402, the loss is 32716.115002979328, parameters k is -89.7858615414454 and b is 419.90735075561645\n",
      "Iteration 403, the loss is 32362.83209276097, parameters k is -89.77286747029916 and b is 420.8836353405967\n",
      "Iteration 404, the loss is 32011.787855049108, parameters k is -89.75987339915292 and b is 421.85991992557695\n",
      "Iteration 405, the loss is 31662.982289843803, parameters k is -89.74687932800668 and b is 422.8362045105572\n",
      "Iteration 406, the loss is 31316.415397144978, parameters k is -89.73388525686045 and b is 423.81248909553744\n",
      "Iteration 407, the loss is 30972.08717695271, parameters k is -89.72089118571421 and b is 424.7887736805177\n",
      "Iteration 408, the loss is 30629.997629266938, parameters k is -89.70789711456797 and b is 425.76505826549794\n",
      "Iteration 409, the loss is 30290.14675408767, parameters k is -89.69490304342173 and b is 426.7413428504782\n",
      "Iteration 410, the loss is 29952.534551414952, parameters k is -89.6819089722755 and b is 427.71762743545844\n",
      "Iteration 411, the loss is 29618.405109917945, parameters k is -89.66891490112926 and b is 428.6899594512687\n",
      "Iteration 412, the loss is 29286.49764572922, parameters k is -89.65592082998302 and b is 429.66229146707894\n",
      "Iteration 413, the loss is 28956.812158848854, parameters k is -89.64292675883678 and b is 430.6346234828892\n",
      "Iteration 414, the loss is 28629.34864927677, parameters k is -89.62993268769054 and b is 431.60695549869945\n",
      "Iteration 415, the loss is 28304.107117012973, parameters k is -89.6169386165443 and b is 432.5792875145097\n",
      "Iteration 416, the loss is 27981.087562057506, parameters k is -89.60394454539806 and b is 433.55161953031995\n",
      "Iteration 417, the loss is 27661.484112467304, parameters k is -89.59095047425183 and b is 434.51999897696027\n",
      "Iteration 418, the loss is 27344.086007478432, parameters k is -89.57795640310559 and b is 435.4883784236006\n",
      "Iteration 419, the loss is 27028.893247090844, parameters k is -89.56496233195935 and b is 436.4567578702409\n",
      "Iteration 420, the loss is 26715.905831304597, parameters k is -89.55196826081311 and b is 437.4251373168812\n",
      "Iteration 421, the loss is 26405.123760119677, parameters k is -89.53897418966687 and b is 438.3935167635215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 422, the loss is 26096.547033536037, parameters k is -89.52598011852064 and b is 439.36189621016183\n",
      "Iteration 423, the loss is 25790.17565155371, parameters k is -89.5129860473744 and b is 440.33027565680214\n",
      "Iteration 424, the loss is 25486.009614172675, parameters k is -89.49999197622816 and b is 441.29865510344246\n",
      "Iteration 425, the loss is 25184.04892139298, parameters k is -89.48699790508192 and b is 442.26703455008277\n",
      "Iteration 426, the loss is 24884.293573214607, parameters k is -89.47400383393568 and b is 443.2354139967231\n",
      "Iteration 427, the loss is 24586.74356963748, parameters k is -89.46100976278944 and b is 444.2037934433634\n",
      "Iteration 428, the loss is 24291.39891066174, parameters k is -89.4480156916432 and b is 445.1721728900037\n",
      "Iteration 429, the loss is 23998.259596287313, parameters k is -89.43502162049697 and b is 446.140552336644\n",
      "Iteration 430, the loss is 23707.32562651413, parameters k is -89.42202754935073 and b is 447.10893178328433\n",
      "Iteration 431, the loss is 23419.67495041515, parameters k is -89.40903347820449 and b is 448.0733586607547\n",
      "Iteration 432, the loss is 23134.213048701728, parameters k is -89.39603940705825 and b is 449.0377855382251\n",
      "Iteration 433, the loss is 22850.939921373752, parameters k is -89.38304533591202 and b is 450.00221241569545\n",
      "Iteration 434, the loss is 22569.85556843133, parameters k is -89.37005126476578 and b is 450.9666392931658\n",
      "Iteration 435, the loss is 22292.004892252575, parameters k is -89.35705719361954 and b is 451.9271136014662\n",
      "Iteration 436, the loss is 22016.326482734723, parameters k is -89.3440631224733 and b is 452.8875879097666\n",
      "Iteration 437, the loss is 21743.848797022572, parameters k is -89.33106905132706 and b is 453.844109648897\n",
      "Iteration 438, the loss is 21473.526932738063, parameters k is -89.31807498018082 and b is 454.80063138802745\n",
      "Iteration 439, the loss is 21207.3850699319, parameters k is -89.30508090903459 and b is 455.74924798881796\n",
      "Iteration 440, the loss is 20943.36632556026, parameters k is -89.29208683788835 and b is 456.69786458960846\n",
      "Iteration 441, the loss is 20681.470699623103, parameters k is -89.27909276674211 and b is 457.64648119039896\n",
      "Iteration 442, the loss is 20421.698192120428, parameters k is -89.26609869559587 and b is 458.59509779118946\n",
      "Iteration 443, the loss is 20164.048803052294, parameters k is -89.25310462444963 and b is 459.54371439197996\n",
      "Iteration 444, the loss is 19909.493946799168, parameters k is -89.2401105533034 and b is 460.4883784236005\n",
      "Iteration 445, the loss is 19658.009283589803, parameters k is -89.22711648215716 and b is 461.4290898860511\n",
      "Iteration 446, the loss is 19408.615285786742, parameters k is -89.21412241101092 and b is 462.36980134850165\n",
      "Iteration 447, the loss is 19162.25915298167, parameters k is -89.20112833986468 and b is 463.3065602417823\n",
      "Iteration 448, the loss is 18919.855944194118, parameters k is -89.18813426871844 and b is 464.23541399672297\n",
      "Iteration 449, the loss is 18679.495189954552, parameters k is -89.1751401975722 and b is 465.16426775166366\n",
      "Iteration 450, the loss is 18442.100156276458, parameters k is -89.16214612642597 and b is 466.0891689374344\n",
      "Iteration 451, the loss is 18206.731631842704, parameters k is -89.14915205527973 and b is 467.01407012320516\n",
      "Iteration 452, the loss is 17973.389616653294, parameters k is -89.13615798413349 and b is 467.9389713089759\n",
      "Iteration 453, the loss is 17742.074110708203, parameters k is -89.12316391298725 and b is 468.86387249474666\n",
      "Iteration 454, the loss is 17512.785114007478, parameters k is -89.11016984184101 and b is 469.7887736805174\n",
      "Iteration 455, the loss is 17285.522626551065, parameters k is -89.09717577069478 and b is 470.71367486628816\n",
      "Iteration 456, the loss is 17060.286648339028, parameters k is -89.08418169954854 and b is 471.6385760520589\n",
      "Iteration 457, the loss is 16837.077179371347, parameters k is -89.0711876284023 and b is 472.56347723782966\n",
      "Iteration 458, the loss is 16615.89421964797, parameters k is -89.05819355725606 and b is 473.4883784236004\n",
      "Iteration 459, the loss is 16396.73776916896, parameters k is -89.04519948610982 and b is 474.41327960937116\n",
      "Iteration 460, the loss is 16179.607827934293, parameters k is -89.03220541496358 and b is 475.3381807951419\n",
      "Iteration 461, the loss is 15965.34016588368, parameters k is -89.01921134381735 and b is 476.2591294117427\n",
      "Iteration 462, the loss is 15753.083130264962, parameters k is -89.00621727267111 and b is 477.18007802834353\n",
      "Iteration 463, the loss is 15543.656670696611, parameters k is -88.99322320152487 and b is 478.09707407577434\n",
      "Iteration 464, the loss is 15336.2250172389, parameters k is -88.98022913037863 and b is 479.01407012320516\n",
      "Iteration 465, the loss is 15130.788169891859, parameters k is -88.9672350592324 and b is 479.931066170636\n",
      "Iteration 466, the loss is 14928.142425906099, parameters k is -88.95424098808616 and b is 480.84410964889685\n",
      "Iteration 467, the loss is 14727.475730200955, parameters k is -88.94124691693992 and b is 481.7571531271577\n",
      "Iteration 468, the loss is 14528.788082776453, parameters k is -88.92825284579368 and b is 482.6701966054186\n",
      "Iteration 469, the loss is 14333.624992117388, parameters k is -88.91525877464744 and b is 483.57533494533953\n",
      "Iteration 470, the loss is 14140.409621552544, parameters k is -88.9022647035012 and b is 484.4804732852605\n",
      "Iteration 471, the loss is 13950.656401345159, parameters k is -88.88927063235496 and b is 485.3777064868415\n",
      "Iteration 472, the loss is 13762.819823010428, parameters k is -88.87627656120873 and b is 486.2749396884225\n",
      "Iteration 473, the loss is 13576.899886548319, parameters k is -88.86328249006249 and b is 487.1721728900035\n",
      "Iteration 474, the loss is 13392.896591958863, parameters k is -88.85028841891625 and b is 488.0694060915845\n",
      "Iteration 475, the loss is 13211.536248002942, parameters k is -88.83729434777001 and b is 488.96268672399555\n",
      "Iteration 476, the loss is 13032.077100545663, parameters k is -88.82430027662377 and b is 489.8559673564066\n",
      "Iteration 477, the loss is 12855.230075465199, parameters k is -88.81130620547754 and b is 490.74529541964773\n",
      "Iteration 478, the loss is 12680.972145305854, parameters k is -88.7983121343313 and b is 491.63067091371886\n",
      "Iteration 479, the loss is 12509.280376348692, parameters k is -88.78531806318506 and b is 492.51209383862005\n",
      "Iteration 480, the loss is 12339.44384271559, parameters k is -88.77232399203882 and b is 493.39351676352123\n",
      "Iteration 481, the loss is 12172.143016975171, parameters k is -88.75932992089258 and b is 494.2709871192525\n",
      "Iteration 482, the loss is 12007.355152882274, parameters k is -88.74633584974634 and b is 495.14450490581373\n",
      "Iteration 483, the loss is 11845.057597928377, parameters k is -88.7333417786001 and b is 496.01407012320504\n",
      "Iteration 484, the loss is 11685.22779334191, parameters k is -88.72034770745387 and b is 496.87968277142636\n",
      "Iteration 485, the loss is 11527.84327408799, parameters k is -88.70735363630763 and b is 497.74134285047774\n",
      "Iteration 486, the loss is 11372.238638024655, parameters k is -88.69435956516139 and b is 498.6030029295291\n",
      "Iteration 487, the loss is 11219.049458896594, parameters k is -88.68136549401515 and b is 499.46071043941055\n",
      "Iteration 488, the loss is 11070.138184025913, parameters k is -88.66837142286892 and b is 500.3026076726121\n",
      "Iteration 489, the loss is 10922.93300249264, parameters k is -88.65537735172268 and b is 501.1445049058137\n",
      "Iteration 490, the loss is 10778.047616673186, parameters k is -88.64238328057644 and b is 501.9824495698453\n",
      "Iteration 491, the loss is 10635.460217690403, parameters k is -88.6293892094302 and b is 502.816441664707\n",
      "Iteration 492, the loss is 10495.149090404113, parameters k is -88.61639513828396 and b is 503.6464811903987\n",
      "Iteration 493, the loss is 10357.092613410809, parameters k is -88.60340106713772 and b is 504.4725681469204\n",
      "Iteration 494, the loss is 10221.269259043873, parameters k is -88.59040699599149 and b is 505.2947025342722\n",
      "Iteration 495, the loss is 10087.657593373508, parameters k is -88.57741292484525 and b is 506.112884352454\n",
      "Iteration 496, the loss is 9955.66553542678, parameters k is -88.56441885369901 and b is 506.9310661706358\n",
      "Iteration 497, the loss is 9825.856712585923, parameters k is -88.55142478255277 and b is 507.7452954196477\n",
      "Iteration 498, the loss is 9697.653301918994, parameters k is -88.53843071140653 and b is 508.55952466865955\n",
      "Iteration 499, the loss is 9571.604797749755, parameters k is -88.5254366402603 and b is 509.3698013485014\n",
      "Iteration 500, the loss is 9447.147572695883, parameters k is -88.51244256911406 and b is 510.1800780283433\n",
      "Iteration 501, the loss is 9324.817050513888, parameters k is -88.49944849796782 and b is 510.98640213901524\n",
      "Iteration 502, the loss is 9205.649136640486, parameters k is -88.48645442682158 and b is 511.7808685421773\n",
      "Iteration 503, the loss is 9088.01659456055, parameters k is -88.47346035567534 and b is 512.5753349453394\n",
      "Iteration 504, the loss is 8973.463549700875, parameters k is -88.4604662845291 and b is 513.3579436409915\n",
      "Iteration 505, the loss is 8860.912541080568, parameters k is -88.44747221338287 and b is 514.1365997674737\n",
      "Iteration 506, the loss is 8750.844366203984, parameters k is -88.43447814223663 and b is 514.907350755616\n",
      "Iteration 507, the loss is 8644.207736845687, parameters k is -88.42148407109039 and b is 515.6622914670785\n",
      "Iteration 508, the loss is 8538.971085360901, parameters k is -88.40848999994415 and b is 516.417232178541\n",
      "Iteration 509, the loss is 8435.615802920582, parameters k is -88.39549592879791 and b is 517.1682203208335\n",
      "Iteration 510, the loss is 8334.597014297373, parameters k is -88.38250185765168 and b is 517.9113033247861\n",
      "Iteration 511, the loss is 8235.407186904557, parameters k is -88.36950778650544 and b is 518.6504337595687\n",
      "Iteration 512, the loss is 8138.0268552855205, parameters k is -88.3565137153592 and b is 519.3856116251814\n",
      "Iteration 513, the loss is 8042.436647720452, parameters k is -88.34351964421296 and b is 520.1168369216241\n",
      "Iteration 514, the loss is 7949.066422443171, parameters k is -88.33052557306672 and b is 520.8401570797269\n",
      "Iteration 515, the loss is 7857.435163230074, parameters k is -88.31753150192048 and b is 521.5595246686596\n",
      "Iteration 516, the loss is 7768.396972940254, parameters k is -88.30453743077425 and b is 522.2670345500825\n",
      "Iteration 517, the loss is 7680.604536660035, parameters k is -88.29154335962801 and b is 522.9745444315054\n",
      "Iteration 518, the loss is 7594.48197403224, parameters k is -88.27854928848177 and b is 523.6781017437584\n",
      "Iteration 519, the loss is 7511.682750141143, parameters k is -88.26555521733553 and b is 524.3618962101615\n",
      "Iteration 520, the loss is 7431.7039255041245, parameters k is -88.2525611461893 and b is 525.0298803998849\n",
      "Iteration 521, the loss is 7352.8492090234195, parameters k is -88.23956707504306 and b is 525.6978645896082\n",
      "Iteration 522, the loss is 7276.720074762664, parameters k is -88.22657300389682 and b is 526.3500385026517\n",
      "Iteration 523, the loss is 7201.668140070873, parameters k is -88.21357893275058 and b is 527.0022124156952\n",
      "Iteration 524, the loss is 7127.69340494804, parameters k is -88.20058486160434 and b is 527.6543863287387\n",
      "Iteration 525, the loss is 7055.561987406369, parameters k is -88.1875907904581 and b is 528.2986551034422\n",
      "Iteration 526, the loss is 6985.23945589986, parameters k is -88.17459671931186 and b is 528.9350187398059\n",
      "Iteration 527, the loss is 6916.319968909289, parameters k is -88.16160264816563 and b is 529.5674298069995\n",
      "Iteration 528, the loss is 6849.518965741668, parameters k is -88.14860857701939 and b is 530.1879831666832\n",
      "Iteration 529, the loss is 6785.147262763345, parameters k is -88.13561450587315 and b is 530.7927262496872\n",
      "Iteration 530, the loss is 6722.073414281575, parameters k is -88.12262043472691 and b is 531.3935167635212\n",
      "Iteration 531, the loss is 6659.931217738773, parameters k is -88.10962636358067 and b is 531.9943072773552\n",
      "Iteration 532, the loss is 6599.754635872527, parameters k is -88.09663229243444 and b is 532.5832400836794\n",
      "Iteration 533, the loss is 6540.81703340428, parameters k is -88.0836382212882 and b is 533.1682203208335\n",
      "Iteration 534, the loss is 6483.436808782211, parameters k is -88.07064415014196 and b is 533.7452954196477\n",
      "Iteration 535, the loss is 6427.911812002121, parameters k is -88.05765007899572 and b is 534.3105128109521\n",
      "Iteration 536, the loss is 6373.547902103895, parameters k is -88.04465600784948 and b is 534.8717776330865\n",
      "Iteration 537, the loss is 6320.967847262663, parameters k is -88.03166193670324 and b is 535.421184747711\n",
      "Iteration 538, the loss is 6269.184457745251, parameters k is -88.018667865557 and b is 535.9705918623355\n",
      "Iteration 539, the loss is 6218.815919223242, parameters k is -88.00567379441077 and b is 536.5120938386201\n",
      "Iteration 540, the loss is 6171.049965899501, parameters k is -87.99267972326453 and b is 537.029880399885\n",
      "Iteration 541, the loss is 6125.201694912767, parameters k is -87.97968565211829 and b is 537.53185668447\n",
      "Iteration 542, the loss is 6081.215741272916, parameters k is -87.96669158097205 and b is 538.0180226923751\n",
      "Iteration 543, the loss is 6038.747336352162, parameters k is -87.95369750982582 and b is 538.4923309927703\n",
      "Iteration 544, the loss is 5997.756924881124, parameters k is -87.94070343867958 and b is 538.9547815856557\n",
      "Iteration 545, the loss is 5958.205795221726, parameters k is -87.92770936753334 and b is 539.4053744710312\n",
      "Iteration 546, the loss is 5919.221424567441, parameters k is -87.9147152963871 and b is 539.8559673564067\n",
      "Iteration 547, the loss is 5882.174022697895, parameters k is -87.90172122524086 and b is 540.2867973959325\n",
      "Iteration 548, the loss is 5845.922169775114, parameters k is -87.88872715409462 and b is 540.7136748662882\n",
      "Iteration 549, the loss is 5811.518500116847, parameters k is -87.87573308294839 and b is 541.1207894907941\n",
      "Iteration 550, the loss is 5778.117610990712, parameters k is -87.86273901180215 and b is 541.5199989769601\n",
      "Iteration 551, the loss is 5745.1793649133615, parameters k is -87.84974494065591 and b is 541.9192084631261\n",
      "Iteration 552, the loss is 5713.468398617184, parameters k is -87.83675086950967 and b is 542.3065602417822\n",
      "Iteration 553, the loss is 5682.448782175146, parameters k is -87.82375679836343 and b is 542.6899594512684\n",
      "Iteration 554, the loss is 5651.861897804239, parameters k is -87.8107627272172 and b is 543.0733586607545\n",
      "Iteration 555, the loss is 5622.195601268115, parameters k is -87.79776865607096 and b is 543.4488527319007\n",
      "Iteration 556, the loss is 5593.66863257975, parameters k is -87.78477458492472 and b is 543.8124890955371\n",
      "Iteration 557, the loss is 5565.775347737031, parameters k is -87.77178051377848 and b is 544.1721728900035\n",
      "Iteration 558, the loss is 5538.738788046034, parameters k is -87.75878644263224 and b is 544.52395154613\n",
      "Iteration 559, the loss is 5512.538395009921, parameters k is -87.745792371486 and b is 544.8678250639165\n",
      "Iteration 560, the loss is 5487.380858994163, parameters k is -87.73279830033977 and b is 545.1998408741931\n",
      "Iteration 561, the loss is 5463.013050841903, parameters k is -87.71980422919353 and b is 545.5239515461299\n",
      "Iteration 562, the loss is 5439.415724371748, parameters k is -87.70681015804729 and b is 545.8401570797267\n",
      "Iteration 563, the loss is 5416.787477893163, parameters k is -87.69381608690105 and b is 546.1445049058137\n",
      "Iteration 564, the loss is 5394.671855808738, parameters k is -87.68082201575481 and b is 546.4449001627307\n",
      "Iteration 565, the loss is 5373.059797448657, parameters k is -87.66782794460858 and b is 546.7413428504777\n",
      "Iteration 566, the loss is 5351.942335879943, parameters k is -87.65483387346234 and b is 547.0338329690549\n",
      "Iteration 567, the loss is 5331.516205276257, parameters k is -87.6418398023161 and b is 547.318417949292\n",
      "Iteration 568, the loss is 5311.561259572333, parameters k is -87.62884573116986 and b is 547.5990503603592\n",
      "Iteration 569, the loss is 5292.068906782487, parameters k is -87.61585166002362 and b is 547.8757302022565\n",
      "Iteration 570, the loss is 5273.424960636497, parameters k is -87.60285758887738 and b is 548.1405523366439\n",
      "Iteration 571, the loss is 5255.410243682903, parameters k is -87.58986351773115 and b is 548.3974693326912\n",
      "Iteration 572, the loss is 5237.816815731946, parameters k is -87.57686944658491 and b is 548.6504337595687\n",
      "Iteration 573, the loss is 5220.63674095565, parameters k is -87.56387537543867 and b is 548.8994456172762\n",
      "Iteration 574, the loss is 5203.862177262847, parameters k is -87.55088130429243 and b is 549.1445049058137\n",
      "Iteration 575, the loss is 5187.485376299189, parameters k is -87.5378872331462 and b is 549.3856116251814\n",
      "Iteration 576, the loss is 5171.6802958594035, parameters k is -87.52489316199996 and b is 549.618813206209\n",
      "Iteration 577, the loss is 5156.252815800201, parameters k is -87.51189909085372 and b is 549.8480622180667\n",
      "Iteration 578, the loss is 5141.018834661391, parameters k is -87.49890501970748 and b is 550.0773112299245\n",
      "Iteration 579, the loss is 5126.152622693745, parameters k is -87.48591094856124 and b is 550.3026076726122\n",
      "Iteration 580, the loss is 5111.646900227007, parameters k is -87.472916877415 and b is 550.52395154613\n",
      "Iteration 581, the loss is 5097.324970453655, parameters k is -87.45992280626876 and b is 550.7452954196477\n",
      "Iteration 582, the loss is 5083.353948936621, parameters k is -87.44692873512253 and b is 550.9626867239956\n",
      "Iteration 583, the loss is 5069.561960736286, parameters k is -87.43393466397629 and b is 551.1800780283435\n",
      "Iteration 584, the loss is 5055.949005852654, parameters k is -87.42094059283005 and b is 551.3974693326913\n",
      "Iteration 585, the loss is 5042.8352247551875, parameters k is -87.40794652168381 and b is 551.6069554986992\n",
      "Iteration 586, the loss is 5030.048961223509, parameters k is -87.39495245053757 and b is 551.8124890955372\n",
      "Iteration 587, the loss is 5017.583404271459, parameters k is -87.38195837939134 and b is 552.0140701232052\n",
      "Iteration 588, the loss is 5005.738667602887, parameters k is -87.3689643082451 and b is 552.2037934433633\n",
      "Iteration 589, the loss is 4994.192715366885, parameters k is -87.35597023709886 and b is 552.3895641943515\n",
      "Iteration 590, the loss is 4982.789971117746, parameters k is -87.34297616595262 and b is 552.5753349453396\n",
      "Iteration 591, the loss is 4971.9718887211075, parameters k is -87.32998209480638 and b is 552.7492479888178\n",
      "Iteration 592, the loss is 4961.4298043335375, parameters k is -87.31698802366014 and b is 552.9192084631261\n",
      "Iteration 593, the loss is 4951.157750600221, parameters k is -87.3039939525139 and b is 553.0852163682645\n",
      "Iteration 594, the loss is 4941.291194064977, parameters k is -87.29099988136767 and b is 553.2433191350628\n",
      "Iteration 595, the loss is 4931.679255167555, parameters k is -87.27800581022143 and b is 553.3974693326913\n",
      "Iteration 596, the loss is 4922.31634150044, parameters k is -87.26501173907519 and b is 553.5476669611498\n",
      "Iteration 597, the loss is 4913.196954392929, parameters k is -87.25201766792895 and b is 553.6939120204383\n",
      "Iteration 598, the loss is 4904.449791248782, parameters k is -87.23902359678272 and b is 553.832251941387\n",
      "Iteration 599, the loss is 4895.931991472039, parameters k is -87.22602952563648 and b is 553.9666392931656\n",
      "Iteration 600, the loss is 4887.638431339275, parameters k is -87.21303545449024 and b is 554.0970740757743\n",
      "Iteration 601, the loss is 4879.693184460522, parameters k is -87.200041383344 and b is 554.2196037200431\n",
      "Iteration 602, the loss is 4872.08656580981, parameters k is -87.18704731219776 and b is 554.334228225972\n",
      "Iteration 603, the loss is 4864.6832025801705, parameters k is -87.17405324105152 and b is 554.4449001627308\n",
      "Iteration 604, the loss is 4857.603107008798, parameters k is -87.16105916990529 and b is 554.5476669611498\n",
      "Iteration 605, the loss is 4850.837531437919, parameters k is -87.14806509875905 and b is 554.6425286212288\n",
      "Iteration 606, the loss is 4844.13444414776, parameters k is -87.13507102761281 and b is 554.7373902813079\n",
      "Iteration 607, the loss is 4837.493845138316, parameters k is -87.12207695646657 and b is 554.8322519413869\n",
      "Iteration 608, the loss is 4830.915734409602, parameters k is -87.10908288532033 and b is 554.9271136014659\n",
      "Iteration 609, the loss is 4824.51773948817, parameters k is -87.0960888141741 and b is 555.018022692375\n",
      "Iteration 610, the loss is 4818.412093567263, parameters k is -87.08309474302786 and b is 555.1010266449441\n",
      "Iteration 611, the loss is 4812.36084442703, parameters k is -87.07010067188162 and b is 555.1840305975132\n",
      "Iteration 612, the loss is 4806.591530567853, parameters k is -87.05710660073538 and b is 555.2591294117425\n",
      "Iteration 613, the loss is 4800.87153161203, parameters k is -87.04411252958914 and b is 555.3342282259717\n",
      "Iteration 614, the loss is 4795.312185230757, parameters k is -87.0311184584429 and b is 555.405374471031\n",
      "Iteration 615, the loss is 4789.799706551001, parameters k is -87.01812438729667 and b is 555.4765207160902\n",
      "Iteration 616, the loss is 4784.334095572776, parameters k is -87.00513031615043 and b is 555.5476669611495\n",
      "Iteration 617, the loss is 4778.915352296093, parameters k is -86.99213624500419 and b is 555.6188132062088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 618, the loss is 4773.543476720932, parameters k is -86.97914217385795 and b is 555.689959451268\n",
      "Iteration 619, the loss is 4768.534549662724, parameters k is -86.96614810271171 and b is 555.7492479888174\n",
      "Iteration 620, the loss is 4763.5655236478215, parameters k is -86.95315403156548 and b is 555.8085365263668\n",
      "Iteration 621, the loss is 4758.636398676223, parameters k is -86.94015996041924 and b is 555.8678250639161\n",
      "Iteration 622, the loss is 4753.74717474792, parameters k is -86.927165889273 and b is 555.9271136014655\n",
      "Iteration 623, the loss is 4748.8978518629265, parameters k is -86.91417181812676 and b is 555.9864021390149\n",
      "Iteration 624, the loss is 4744.288164379911, parameters k is -86.90117774698052 and b is 556.0377855382244\n",
      "Iteration 625, the loss is 4739.8129082551095, parameters k is -86.88818367583428 and b is 556.0852163682639\n",
      "Iteration 626, the loss is 4735.371148936294, parameters k is -86.87518960468805 and b is 556.1326471983034\n",
      "Iteration 627, the loss is 4730.962886423462, parameters k is -86.86219553354181 and b is 556.1800780283429\n",
      "Iteration 628, the loss is 4726.683952746451, parameters k is -86.84920146239557 and b is 556.2235562892125\n",
      "Iteration 629, the loss is 4722.436506112076, parameters k is -86.83620739124933 and b is 556.2670345500821\n",
      "Iteration 630, the loss is 4718.220546520337, parameters k is -86.8232133201031 and b is 556.3105128109518\n",
      "Iteration 631, the loss is 4714.036073971226, parameters k is -86.81021924895686 and b is 556.3539910718214\n",
      "Iteration 632, the loss is 4709.883088464764, parameters k is -86.79722517781062 and b is 556.397469332691\n",
      "Iteration 633, the loss is 4705.85250698201, parameters k is -86.78423110666438 and b is 556.4369950243906\n",
      "Iteration 634, the loss is 4701.851465269767, parameters k is -86.77123703551814 and b is 556.4765207160902\n",
      "Iteration 635, the loss is 4697.879963328024, parameters k is -86.7582429643719 and b is 556.5160464077899\n",
      "Iteration 636, the loss is 4694.114180776175, parameters k is -86.74524889322566 and b is 556.5476669611495\n",
      "Iteration 637, the loss is 4690.374230924194, parameters k is -86.73225482207943 and b is 556.5792875145091\n",
      "Iteration 638, the loss is 4686.660113772078, parameters k is -86.71926075093319 and b is 556.6109080678688\n",
      "Iteration 639, the loss is 4683.0572794317695, parameters k is -86.70626667978695 and b is 556.6385760520585\n",
      "Iteration 640, the loss is 4679.478517992823, parameters k is -86.69327260864071 and b is 556.6662440362483\n",
      "Iteration 641, the loss is 4676.00758225991, parameters k is -86.68027853749447 and b is 556.689959451268\n",
      "Iteration 642, the loss is 4672.559022121066, parameters k is -86.66728446634824 and b is 556.7136748662878\n",
      "Iteration 643, the loss is 4669.132837576294, parameters k is -86.654290395202 and b is 556.7373902813075\n",
      "Iteration 644, the loss is 4665.729028625594, parameters k is -86.64129632405576 and b is 556.7611056963273\n",
      "Iteration 645, the loss is 4662.34759526897, parameters k is -86.62830225290952 and b is 556.784821111347\n",
      "Iteration 646, the loss is 4659.068156402488, parameters k is -86.61530818176328 and b is 556.8045839571968\n",
      "Iteration 647, the loss is 4655.809458314004, parameters k is -86.60231411061704 and b is 556.8243468030465\n",
      "Iteration 648, the loss is 4652.571501003508, parameters k is -86.5893200394708 and b is 556.8441096488963\n",
      "Iteration 649, the loss is 4649.35428447102, parameters k is -86.57632596832457 and b is 556.863872494746\n",
      "Iteration 650, the loss is 4646.310725963601, parameters k is -86.56333189717833 and b is 556.8757302022559\n",
      "Iteration 651, the loss is 4643.284826075661, parameters k is -86.55033782603209 and b is 556.8875879097658\n",
      "Iteration 652, the loss is 4640.276584807203, parameters k is -86.53734375488585 and b is 556.8994456172757\n",
      "Iteration 653, the loss is 4637.286002158224, parameters k is -86.52434968373962 and b is 556.9113033247855\n",
      "Iteration 654, the loss is 4634.313078128742, parameters k is -86.51135561259338 and b is 556.9231610322954\n",
      "Iteration 655, the loss is 4631.357812718729, parameters k is -86.49836154144714 and b is 556.9350187398053\n",
      "Iteration 656, the loss is 4628.420205928199, parameters k is -86.4853674703009 and b is 556.9468764473152\n",
      "Iteration 657, the loss is 4625.5002577571595, parameters k is -86.47237339915466 and b is 556.958734154825\n",
      "Iteration 658, the loss is 4622.668559345363, parameters k is -86.45937932800842 and b is 556.9666392931649\n",
      "Iteration 659, the loss is 4619.853072210611, parameters k is -86.44638525686219 and b is 556.9745444315048\n",
      "Iteration 660, the loss is 4617.053796352898, parameters k is -86.43339118571595 and b is 556.9824495698447\n",
      "Iteration 661, the loss is 4614.270731772231, parameters k is -86.42039711456971 and b is 556.9903547081846\n",
      "Iteration 662, the loss is 4611.503878468608, parameters k is -86.40740304342347 and b is 556.9982598465244\n",
      "Iteration 663, the loss is 4608.8203185853135, parameters k is -86.39440897227723 and b is 557.0022124156944\n",
      "Iteration 664, the loss is 4606.151585127835, parameters k is -86.381414901131 and b is 557.0061649848644\n",
      "Iteration 665, the loss is 4603.497678096175, parameters k is -86.36842082998476 and b is 557.0101175540344\n",
      "Iteration 666, the loss is 4600.858597490324, parameters k is -86.35542675883852 and b is 557.0140701232044\n",
      "Iteration 667, the loss is 4598.234343310301, parameters k is -86.34243268769228 and b is 557.0180226923744\n",
      "Iteration 668, the loss is 4595.624915556081, parameters k is -86.32943861654604 and b is 557.0219752615444\n",
      "Iteration 669, the loss is 4593.03031422769, parameters k is -86.3164445453998 and b is 557.0259278307144\n",
      "Iteration 670, the loss is 4590.450539325102, parameters k is -86.30345047425357 and b is 557.0298803998844\n",
      "Iteration 671, the loss is 4587.947289814751, parameters k is -86.29045640310733 and b is 557.0298803998844\n",
      "Iteration 672, the loss is 4585.51862902501, parameters k is -86.27746233196109 and b is 557.0259278307144\n",
      "Iteration 673, the loss is 4583.162714021086, parameters k is -86.26446826081485 and b is 557.0180226923745\n",
      "Iteration 674, the loss is 4580.877795605004, parameters k is -86.25147418966861 and b is 557.0061649848647\n",
      "Iteration 675, the loss is 4578.602789121947, parameters k is -86.23848011852238 and b is 556.9943072773548\n",
      "Iteration 676, the loss is 4576.337694571923, parameters k is -86.22548604737614 and b is 556.9824495698449\n",
      "Iteration 677, the loss is 4574.082511954932, parameters k is -86.2124919762299 and b is 556.970591862335\n",
      "Iteration 678, the loss is 4571.837241270965, parameters k is -86.19949790508366 and b is 556.9587341548252\n",
      "Iteration 679, the loss is 4569.601882520034, parameters k is -86.18650383393742 and b is 556.9468764473153\n",
      "Iteration 680, the loss is 4567.376435702129, parameters k is -86.17350976279118 and b is 556.9350187398054\n",
      "Iteration 681, the loss is 4565.21701908859, parameters k is -86.16051569164495 and b is 556.9192084631255\n",
      "Iteration 682, the loss is 4563.122070955078, parameters k is -86.14752162049871 and b is 556.8994456172758\n",
      "Iteration 683, the loss is 4561.090123314064, parameters k is -86.13452754935247 and b is 556.875730202256\n",
      "Iteration 684, the loss is 4559.065057894227, parameters k is -86.12153347820623 and b is 556.8520147872363\n",
      "Iteration 685, the loss is 4557.046874695544, parameters k is -86.10853940706 and b is 556.8282993722165\n",
      "Iteration 686, the loss is 4555.035573718038, parameters k is -86.09554533591376 and b is 556.8045839571968\n",
      "Iteration 687, the loss is 4553.031154961701, parameters k is -86.08255126476752 and b is 556.780868542177\n",
      "Iteration 688, the loss is 4551.033618426528, parameters k is -86.06955719362128 and b is 556.7571531271573\n",
      "Iteration 689, the loss is 4549.04296411252, parameters k is -86.05656312247504 and b is 556.7334377121375\n",
      "Iteration 690, the loss is 4547.111187538913, parameters k is -86.0435690513288 and b is 556.7057697279478\n",
      "Iteration 691, the loss is 4545.18540826494, parameters k is -86.03057498018256 and b is 556.678101743758\n",
      "Iteration 692, the loss is 4543.265626290598, parameters k is -86.01758090903633 and b is 556.6504337595683\n",
      "Iteration 693, the loss is 4541.351841615909, parameters k is -86.00458683789009 and b is 556.6227657753785\n",
      "Iteration 694, the loss is 4539.444054240841, parameters k is -85.99159276674385 and b is 556.5950977911888\n",
      "Iteration 695, the loss is 4537.542264165417, parameters k is -85.97859869559761 and b is 556.567429806999\n",
      "Iteration 696, the loss is 4535.695937126689, parameters k is -85.96560462445137 and b is 556.5358092536394\n",
      "Iteration 697, the loss is 4533.854784957287, parameters k is -85.95261055330513 and b is 556.5041887002798\n",
      "Iteration 698, the loss is 4532.067513455153, parameters k is -85.9396164821589 and b is 556.4686155777501\n",
      "Iteration 699, the loss is 4530.28465688325, parameters k is -85.92662241101266 and b is 556.4330424552205\n",
      "Iteration 700, the loss is 4528.506215241549, parameters k is -85.91362833986642 and b is 556.3974693326909\n",
      "Iteration 701, the loss is 4526.732188530063, parameters k is -85.90063426872018 and b is 556.3618962101613\n",
      "Iteration 702, the loss is 4524.962576748791, parameters k is -85.88764019757394 and b is 556.3263230876316\n",
      "Iteration 703, the loss is 4523.197379897735, parameters k is -85.8746461264277 and b is 556.290749965102\n",
      "Iteration 704, the loss is 4521.436597976899, parameters k is -85.86165205528147 and b is 556.2551768425724\n",
      "Iteration 705, the loss is 4519.72641760258, parameters k is -85.84865798413523 and b is 556.2156511508728\n",
      "Iteration 706, the loss is 4518.065839471373, parameters k is -85.83566391298899 and b is 556.1721728900031\n",
      "Iteration 707, the loss is 4516.453958016644, parameters k is -85.82266984184275 and b is 556.1247420599636\n",
      "Iteration 708, the loss is 4514.8445866220945, parameters k is -85.80967577069651 and b is 556.0773112299241\n",
      "Iteration 709, the loss is 4513.2377252877095, parameters k is -85.79668169955028 and b is 556.0298803998846\n",
      "Iteration 710, the loss is 4511.633374013501, parameters k is -85.78368762840404 and b is 555.9824495698451\n",
      "Iteration 711, the loss is 4510.031532799464, parameters k is -85.7706935572578 and b is 555.9350187398056\n",
      "Iteration 712, the loss is 4508.43220164559, parameters k is -85.75769948611156 and b is 555.8875879097661\n",
      "Iteration 713, the loss is 4506.8353805519, parameters k is -85.74470541496532 and b is 555.8401570797266\n",
      "Iteration 714, the loss is 4505.241069518376, parameters k is -85.73171134381909 and b is 555.7927262496871\n",
      "Iteration 715, the loss is 4503.649268545032, parameters k is -85.71871727267285 and b is 555.7452954196476\n",
      "Iteration 716, the loss is 4502.103187538925, parameters k is -85.70572320152661 and b is 555.6939120204381\n",
      "Iteration 717, the loss is 4500.559106618728, parameters k is -85.69272913038037 and b is 555.6425286212286\n",
      "Iteration 718, the loss is 4499.0170257844475, parameters k is -85.67973505923413 and b is 555.5911452220191\n",
      "Iteration 719, the loss is 4497.476945036076, parameters k is -85.6667409880879 and b is 555.5397618228096\n",
      "Iteration 720, the loss is 4495.93886437363, parameters k is -85.65374691694166 and b is 555.4883784236001\n",
      "Iteration 721, the loss is 4494.4448281281475, parameters k is -85.64075284579542 and b is 555.4330424552206\n",
      "Iteration 722, the loss is 4492.952344485538, parameters k is -85.62775877464918 and b is 555.3777064868411\n",
      "Iteration 723, the loss is 4491.461413445793, parameters k is -85.61476470350294 and b is 555.3223705184616\n",
      "Iteration 724, the loss is 4489.972035008926, parameters k is -85.6017706323567 and b is 555.2670345500821\n",
      "Iteration 725, the loss is 4488.484209174928, parameters k is -85.58877656121047 and b is 555.2116985817026\n",
      "Iteration 726, the loss is 4486.997935943793, parameters k is -85.57578249006423 and b is 555.1563626133232\n",
      "Iteration 727, the loss is 4485.513215315531, parameters k is -85.56278841891799 and b is 555.1010266449437\n",
      "Iteration 728, the loss is 4484.03004729015, parameters k is -85.54979434777175 and b is 555.0456906765642\n",
      "Iteration 729, the loss is 4482.548431867625, parameters k is -85.53680027662551 and b is 554.9903547081847\n",
      "Iteration 730, the loss is 4481.068369047976, parameters k is -85.52380620547927 and b is 554.9350187398052\n",
      "Iteration 731, the loss is 4479.589858831202, parameters k is -85.51081213433304 and b is 554.8796827714257\n",
      "Iteration 732, the loss is 4478.152687488037, parameters k is -85.4978180631868 and b is 554.8203942338763\n",
      "Iteration 733, the loss is 4476.716683755919, parameters k is -85.48482399204056 and b is 554.7611056963269\n",
      "Iteration 734, the loss is 4475.28184763483, parameters k is -85.47182992089432 and b is 554.7018171587775\n",
      "Iteration 735, the loss is 4473.848179124786, parameters k is -85.45883584974808 and b is 554.6425286212282\n",
      "Iteration 736, the loss is 4472.45478824967, parameters k is -85.44584177860185 and b is 554.5792875145088\n",
      "Iteration 737, the loss is 4471.062242484987, parameters k is -85.43284770745561 and b is 554.5160464077894\n",
      "Iteration 738, the loss is 4469.670541830708, parameters k is -85.41985363630937 and b is 554.45280530107\n",
      "Iteration 739, the loss is 4468.2796862868545, parameters k is -85.40685956516313 and b is 554.3895641943507\n",
      "Iteration 740, the loss is 4466.889675853414, parameters k is -85.3938654940169 and b is 554.3263230876313\n",
      "Iteration 741, the loss is 4465.538923662373, parameters k is -85.38087142287065 and b is 554.2591294117419\n",
      "Iteration 742, the loss is 4464.188756572328, parameters k is -85.36787735172442 and b is 554.1919357358526\n",
      "Iteration 743, the loss is 4462.839174583294, parameters k is -85.35488328057818 and b is 554.1247420599632\n",
      "Iteration 744, the loss is 4461.490177695275, parameters k is -85.34188920943194 and b is 554.0575483840738\n",
      "Iteration 745, the loss is 4460.141765908271, parameters k is -85.3288951382857 and b is 553.9903547081844\n",
      "Iteration 746, the loss is 4458.793939222272, parameters k is -85.31590106713946 and b is 553.9231610322951\n",
      "Iteration 747, the loss is 4457.446697637289, parameters k is -85.30290699599323 and b is 553.8559673564057\n",
      "Iteration 748, the loss is 4456.100041153308, parameters k is -85.28991292484699 and b is 553.7887736805163\n",
      "Iteration 749, the loss is 4454.753969770338, parameters k is -85.27691885370075 and b is 553.721580004627\n",
      "Iteration 750, the loss is 4453.4084834883815, parameters k is -85.26392478255451 and b is 553.6543863287376\n",
      "Iteration 751, the loss is 4452.063582307436, parameters k is -85.25093071140827 and b is 553.5871926528482\n",
      "Iteration 752, the loss is 4450.719266227501, parameters k is -85.23793664026203 and b is 553.5199989769588\n",
      "Iteration 753, the loss is 4449.375535248578, parameters k is -85.2249425691158 and b is 553.4528053010695\n",
      "Iteration 754, the loss is 4448.03238937066, parameters k is -85.21194849796956 and b is 553.3856116251801\n",
      "Iteration 755, the loss is 4446.689828593757, parameters k is -85.19895442682332 and b is 553.3184179492907\n",
      "Iteration 756, the loss is 4445.347852917865, parameters k is -85.18596035567708 and b is 553.2512242734014\n",
      "Iteration 757, the loss is 4444.006462342978, parameters k is -85.17296628453084 and b is 553.184030597512\n",
      "Iteration 758, the loss is 4442.66565686911, parameters k is -85.1599722133846 and b is 553.1168369216226\n",
      "Iteration 759, the loss is 4441.325436496244, parameters k is -85.14697814223837 and b is 553.0496432457333\n",
      "Iteration 760, the loss is 4439.985801224393, parameters k is -85.13398407109213 and b is 552.9824495698439\n",
      "Iteration 761, the loss is 4438.646751053545, parameters k is -85.12098999994589 and b is 552.9152558939545\n",
      "Iteration 762, the loss is 4437.3082859837195, parameters k is -85.10799592879965 and b is 552.8480622180651\n",
      "Iteration 763, the loss is 4435.970406014892, parameters k is -85.09500185765341 and b is 552.7808685421758\n",
      "Iteration 764, the loss is 4434.633111147083, parameters k is -85.08200778650718 and b is 552.7136748662864\n",
      "Iteration 765, the loss is 4433.296401380292, parameters k is -85.06901371536094 and b is 552.646481190397\n",
      "Iteration 766, the loss is 4431.9602767145025, parameters k is -85.0560196442147 and b is 552.5792875145077\n",
      "Iteration 767, the loss is 4430.624737149717, parameters k is -85.04302557306846 and b is 552.5120938386183\n",
      "Iteration 768, the loss is 4429.2897826859435, parameters k is -85.03003150192222 and b is 552.4449001627289\n",
      "Iteration 769, the loss is 4427.955413323183, parameters k is -85.01703743077599 and b is 552.3777064868395\n",
      "Iteration 770, the loss is 4426.621629061441, parameters k is -85.00404335962975 and b is 552.3105128109502\n",
      "Iteration 771, the loss is 4425.28842990071, parameters k is -84.99104928848351 and b is 552.2433191350608\n",
      "Iteration 772, the loss is 4423.955815840974, parameters k is -84.97805521733727 and b is 552.1761254591714\n",
      "Iteration 773, the loss is 4422.623786882259, parameters k is -84.96506114619103 and b is 552.1089317832821\n",
      "Iteration 774, the loss is 4421.2923430245555, parameters k is -84.9520670750448 and b is 552.0417381073927\n",
      "Iteration 775, the loss is 4419.961484267847, parameters k is -84.93907300389856 and b is 551.9745444315033\n",
      "Iteration 776, the loss is 4418.631210612169, parameters k is -84.92607893275232 and b is 551.907350755614\n",
      "Iteration 777, the loss is 4417.301522057493, parameters k is -84.91308486160608 and b is 551.8401570797246\n",
      "Iteration 778, the loss is 4415.972418603828, parameters k is -84.90009079045984 and b is 551.7729634038352\n",
      "Iteration 779, the loss is 4414.643900251175, parameters k is -84.8870967193136 and b is 551.7057697279458\n",
      "Iteration 780, the loss is 4413.3159669995275, parameters k is -84.87410264816737 and b is 551.6385760520565\n",
      "Iteration 781, the loss is 4411.988618848896, parameters k is -84.86110857702113 and b is 551.5713823761671\n",
      "Iteration 782, the loss is 4410.661855799276, parameters k is -84.84811450587489 and b is 551.5041887002777\n",
      "Iteration 783, the loss is 4409.335677850656, parameters k is -84.83512043472865 and b is 551.4369950243884\n",
      "Iteration 784, the loss is 4408.010085003056, parameters k is -84.82212636358241 and b is 551.369801348499\n",
      "Iteration 785, the loss is 4406.6850772564585, parameters k is -84.80913229243617 and b is 551.3026076726096\n",
      "Iteration 786, the loss is 4405.360654610881, parameters k is -84.79613822128994 and b is 551.2354139967202\n",
      "Iteration 787, the loss is 4404.036817066303, parameters k is -84.7831441501437 and b is 551.1682203208309\n",
      "Iteration 788, the loss is 4402.713564622748, parameters k is -84.77015007899746 and b is 551.1010266449415\n",
      "Iteration 789, the loss is 4401.390897280197, parameters k is -84.75715600785122 and b is 551.0338329690521\n",
      "Iteration 790, the loss is 4400.068815038653, parameters k is -84.74416193670498 and b is 550.9666392931628\n",
      "Iteration 791, the loss is 4398.74731789813, parameters k is -84.73116786555875 and b is 550.8994456172734\n",
      "Iteration 792, the loss is 4397.426405858603, parameters k is -84.71817379441251 and b is 550.832251941384\n",
      "Iteration 793, the loss is 4396.106078920092, parameters k is -84.70517972326627 and b is 550.7650582654946\n",
      "Iteration 794, the loss is 4394.786337082598, parameters k is -84.69218565212003 and b is 550.6978645896053\n",
      "Iteration 795, the loss is 4393.467180346108, parameters k is -84.6791915809738 and b is 550.6306709137159\n",
      "Iteration 796, the loss is 4392.148608710629, parameters k is -84.66619750982755 and b is 550.5634772378265\n",
      "Iteration 797, the loss is 4390.894731404019, parameters k is -84.65320343868132 and b is 550.4883784235973\n",
      "Iteration 798, the loss is 4389.641106653226, parameters k is -84.64020936753508 and b is 550.413279609368\n",
      "Iteration 799, the loss is 4388.387734458269, parameters k is -84.62721529638884 and b is 550.3381807951388\n",
      "Iteration 800, the loss is 4387.13461481915, parameters k is -84.6142212252426 and b is 550.2630819809095\n",
      "Iteration 801, the loss is 4385.881747735852, parameters k is -84.60122715409636 and b is 550.1879831666803\n",
      "Iteration 802, the loss is 4384.629133208388, parameters k is -84.58823308295013 and b is 550.112884352451\n",
      "Iteration 803, the loss is 4383.376771236764, parameters k is -84.57523901180389 and b is 550.0377855382218\n",
      "Iteration 804, the loss is 4382.124661820963, parameters k is -84.56224494065765 and b is 549.9626867239925\n",
      "Iteration 805, the loss is 4380.872804960991, parameters k is -84.54925086951141 and b is 549.8875879097633\n",
      "Iteration 806, the loss is 4379.621200656854, parameters k is -84.53625679836517 and b is 549.812489095534\n",
      "Iteration 807, the loss is 4378.369848908549, parameters k is -84.52326272721893 and b is 549.7373902813048\n",
      "Iteration 808, the loss is 4377.118749716073, parameters k is -84.5102686560727 and b is 549.6622914670755\n",
      "Iteration 809, the loss is 4375.867903079432, parameters k is -84.49727458492646 and b is 549.5871926528463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 810, the loss is 4374.617308998608, parameters k is -84.48428051378022 and b is 549.512093838617\n",
      "Iteration 811, the loss is 4373.366967473632, parameters k is -84.47128644263398 and b is 549.4369950243878\n",
      "Iteration 812, the loss is 4372.116878504476, parameters k is -84.45829237148774 and b is 549.3618962101585\n",
      "Iteration 813, the loss is 4370.86704209115, parameters k is -84.4452983003415 and b is 549.2867973959293\n",
      "Iteration 814, the loss is 4369.61745823367, parameters k is -84.43230422919527 and b is 549.2116985817\n",
      "Iteration 815, the loss is 4368.368126932006, parameters k is -84.41931015804903 and b is 549.1365997674708\n",
      "Iteration 816, the loss is 4367.119048186183, parameters k is -84.40631608690279 and b is 549.0615009532415\n",
      "Iteration 817, the loss is 4365.870221996186, parameters k is -84.39332201575655 and b is 548.9864021390123\n",
      "Iteration 818, the loss is 4364.621648362012, parameters k is -84.38032794461031 and b is 548.911303324783\n",
      "Iteration 819, the loss is 4363.373327283682, parameters k is -84.36733387346408 and b is 548.8362045105538\n",
      "Iteration 820, the loss is 4362.12525876118, parameters k is -84.35433980231784 and b is 548.7611056963245\n",
      "Iteration 821, the loss is 4360.877442794506, parameters k is -84.3413457311716 and b is 548.6860068820953\n",
      "Iteration 822, the loss is 4359.629879383658, parameters k is -84.32835166002536 and b is 548.610908067866\n",
      "Iteration 823, the loss is 4358.382568528649, parameters k is -84.31535758887912 and b is 548.5358092536368\n",
      "Iteration 824, the loss is 4357.135510229473, parameters k is -84.30236351773289 and b is 548.4607104394075\n",
      "Iteration 825, the loss is 4355.888704486123, parameters k is -84.28936944658665 and b is 548.3856116251783\n",
      "Iteration 826, the loss is 4354.642151298606, parameters k is -84.27637537544041 and b is 548.310512810949\n",
      "Iteration 827, the loss is 4353.395850666919, parameters k is -84.26338130429417 and b is 548.2354139967198\n",
      "Iteration 828, the loss is 4352.149802591061, parameters k is -84.25038723314793 and b is 548.1603151824905\n",
      "Iteration 829, the loss is 4350.904007071038, parameters k is -84.2373931620017 and b is 548.0852163682613\n",
      "Iteration 830, the loss is 4349.658464106851, parameters k is -84.22439909085546 and b is 548.010117554032\n",
      "Iteration 831, the loss is 4348.413173698484, parameters k is -84.21140501970922 and b is 547.9350187398028\n",
      "Iteration 832, the loss is 4347.16813584595, parameters k is -84.19841094856298 and b is 547.8599199255735\n",
      "Iteration 833, the loss is 4345.923350549249, parameters k is -84.18541687741674 and b is 547.7848211113443\n",
      "Iteration 834, the loss is 4344.678817808385, parameters k is -84.1724228062705 and b is 547.709722297115\n",
      "Iteration 835, the loss is 4343.434537623347, parameters k is -84.15942873512427 and b is 547.6346234828858\n",
      "Iteration 836, the loss is 4342.190509994133, parameters k is -84.14643466397803 and b is 547.5595246686565\n",
      "Iteration 837, the loss is 4340.946734920763, parameters k is -84.13344059283179 and b is 547.4844258544273\n",
      "Iteration 838, the loss is 4339.703212403216, parameters k is -84.12044652168555 and b is 547.409327040198\n",
      "Iteration 839, the loss is 4338.4599424414955, parameters k is -84.10745245053931 and b is 547.3342282259688\n",
      "Iteration 840, the loss is 4337.216925035612, parameters k is -84.09445837939307 and b is 547.2591294117395\n",
      "Iteration 841, the loss is 4335.974160185565, parameters k is -84.08146430824684 and b is 547.1840305975103\n",
      "Iteration 842, the loss is 4334.731647891336, parameters k is -84.0684702371006 and b is 547.108931783281\n",
      "Iteration 843, the loss is 4333.489388152947, parameters k is -84.05547616595436 and b is 547.0338329690518\n",
      "Iteration 844, the loss is 4332.247380970393, parameters k is -84.04248209480812 and b is 546.9587341548225\n",
      "Iteration 845, the loss is 4331.00562634366, parameters k is -84.02948802366188 and b is 546.8836353405933\n",
      "Iteration 846, the loss is 4329.764124272759, parameters k is -84.01649395251565 and b is 546.808536526364\n",
      "Iteration 847, the loss is 4328.522874757696, parameters k is -84.00349988136941 and b is 546.7334377121348\n",
      "Iteration 848, the loss is 4327.281877798461, parameters k is -83.99050581022317 and b is 546.6583388979055\n",
      "Iteration 849, the loss is 4326.04113339506, parameters k is -83.97751173907693 and b is 546.5832400836763\n",
      "Iteration 850, the loss is 4324.8006415474865, parameters k is -83.96451766793069 and b is 546.508141269447\n",
      "Iteration 851, the loss is 4323.589701640886, parameters k is -83.95152359678445 and b is 546.4290898860478\n",
      "Iteration 852, the loss is 4322.3789417543385, parameters k is -83.93852952563822 and b is 546.3500385026485\n",
      "Iteration 853, the loss is 4321.168361887862, parameters k is -83.92553545449198 and b is 546.2709871192493\n",
      "Iteration 854, the loss is 4319.957962041432, parameters k is -83.91254138334574 and b is 546.19193573585\n",
      "Iteration 855, the loss is 4318.747742215078, parameters k is -83.8995473121995 and b is 546.1128843524508\n",
      "Iteration 856, the loss is 4317.537702408772, parameters k is -83.88655324105326 and b is 546.0338329690516\n",
      "Iteration 857, the loss is 4316.327842622534, parameters k is -83.87355916990703 and b is 545.9547815856523\n",
      "Iteration 858, the loss is 4315.118162856344, parameters k is -83.86056509876079 and b is 545.8757302022531\n",
      "Iteration 859, the loss is 4313.90866311022, parameters k is -83.84757102761455 and b is 545.7966788188538\n",
      "Iteration 860, the loss is 4312.699343384163, parameters k is -83.83457695646831 and b is 545.7176274354546\n",
      "Iteration 861, the loss is 4311.490203678155, parameters k is -83.82158288532207 and b is 545.6385760520553\n",
      "Iteration 862, the loss is 4310.281243992205, parameters k is -83.80858881417583 and b is 545.5595246686561\n",
      "Iteration 863, the loss is 4309.072464326322, parameters k is -83.7955947430296 and b is 545.4804732852568\n",
      "Iteration 864, the loss is 4307.8638646804975, parameters k is -83.78260067188336 and b is 545.4014219018576\n",
      "Iteration 865, the loss is 4306.65544505473, parameters k is -83.76960660073712 and b is 545.3223705184583\n",
      "Iteration 866, the loss is 4305.447205449025, parameters k is -83.75661252959088 and b is 545.2433191350591\n",
      "Iteration 867, the loss is 4304.239145863371, parameters k is -83.74361845844464 and b is 545.1642677516598\n",
      "Iteration 868, the loss is 4303.031266297788, parameters k is -83.7306243872984 and b is 545.0852163682606\n",
      "Iteration 869, the loss is 4301.823566752259, parameters k is -83.71763031615217 and b is 545.0061649848614\n",
      "Iteration 870, the loss is 4300.61604722679, parameters k is -83.70463624500593 and b is 544.9271136014621\n",
      "Iteration 871, the loss is 4299.4087077213835, parameters k is -83.69164217385969 and b is 544.8480622180629\n",
      "Iteration 872, the loss is 4298.201548236034, parameters k is -83.67864810271345 and b is 544.7690108346636\n",
      "Iteration 873, the loss is 4296.994568770744, parameters k is -83.66565403156721 and b is 544.6899594512644\n",
      "Iteration 874, the loss is 4295.787769325508, parameters k is -83.65265996042098 and b is 544.6109080678651\n",
      "Iteration 875, the loss is 4294.581149900343, parameters k is -83.63966588927474 and b is 544.5318566844659\n",
      "Iteration 876, the loss is 4293.374710495233, parameters k is -83.6266718181285 and b is 544.4528053010666\n",
      "Iteration 877, the loss is 4292.168451110178, parameters k is -83.61367774698226 and b is 544.3737539176674\n",
      "Iteration 878, the loss is 4290.962371745184, parameters k is -83.60068367583602 and b is 544.2947025342681\n",
      "Iteration 879, the loss is 4289.7564724002505, parameters k is -83.58768960468979 and b is 544.2156511508689\n",
      "Iteration 880, the loss is 4288.550753075384, parameters k is -83.57469553354355 and b is 544.1365997674696\n",
      "Iteration 881, the loss is 4287.345213770562, parameters k is -83.56170146239731 and b is 544.0575483840704\n",
      "Iteration 882, the loss is 4286.139854485822, parameters k is -83.54870739125107 and b is 543.9784970006712\n",
      "Iteration 883, the loss is 4284.934675221126, parameters k is -83.53571332010483 and b is 543.8994456172719\n",
      "Iteration 884, the loss is 4283.729675976488, parameters k is -83.5227192489586 and b is 543.8203942338727\n",
      "Iteration 885, the loss is 4282.524856751913, parameters k is -83.50972517781236 and b is 543.7413428504734\n",
      "Iteration 886, the loss is 4281.320217547395, parameters k is -83.49673110666612 and b is 543.6622914670742\n",
      "Iteration 887, the loss is 4280.115758362943, parameters k is -83.48373703551988 and b is 543.5832400836749\n",
      "Iteration 888, the loss is 4278.911479198543, parameters k is -83.47074296437364 and b is 543.5041887002757\n",
      "Iteration 889, the loss is 4277.707380054209, parameters k is -83.4577488932274 and b is 543.4251373168764\n",
      "Iteration 890, the loss is 4276.503460929931, parameters k is -83.44475482208117 and b is 543.3460859334772\n",
      "Iteration 891, the loss is 4275.299721825712, parameters k is -83.43176075093493 and b is 543.2670345500779\n",
      "Iteration 892, the loss is 4274.096162741553, parameters k is -83.41876667978869 and b is 543.1879831666787\n",
      "Iteration 893, the loss is 4272.892783677454, parameters k is -83.40577260864245 and b is 543.1089317832794\n",
      "Iteration 894, the loss is 4271.689584633416, parameters k is -83.39277853749621 and b is 543.0298803998802\n",
      "Iteration 895, the loss is 4270.4865656094435, parameters k is -83.37978446634997 and b is 542.950829016481\n",
      "Iteration 896, the loss is 4269.2837266055185, parameters k is -83.36679039520374 and b is 542.8717776330817\n",
      "Iteration 897, the loss is 4268.0810676216615, parameters k is -83.3537963240575 and b is 542.7927262496825\n",
      "Iteration 898, the loss is 4266.8785886578635, parameters k is -83.34080225291126 and b is 542.7136748662832\n",
      "Iteration 899, the loss is 4265.67628971412, parameters k is -83.32780818176502 and b is 542.634623482884\n",
      "Iteration 900, the loss is 4264.4741707904395, parameters k is -83.31481411061878 and b is 542.5555720994847\n",
      "Iteration 901, the loss is 4263.272231886817, parameters k is -83.30182003947255 and b is 542.4765207160855\n",
      "Iteration 902, the loss is 4262.070473003254, parameters k is -83.28882596832631 and b is 542.3974693326862\n",
      "Iteration 903, the loss is 4260.868894139752, parameters k is -83.27583189718007 and b is 542.318417949287\n",
      "Iteration 904, the loss is 4259.667495296314, parameters k is -83.26283782603383 and b is 542.2393665658877\n",
      "Iteration 905, the loss is 4258.466276472926, parameters k is -83.24984375488759 and b is 542.1603151824885\n",
      "Iteration 906, the loss is 4257.265237669605, parameters k is -83.23684968374135 and b is 542.0812637990892\n",
      "Iteration 907, the loss is 4256.064378886342, parameters k is -83.22385561259512 and b is 542.00221241569\n",
      "Iteration 908, the loss is 4254.863700123132, parameters k is -83.21086154144888 and b is 541.9231610322907\n",
      "Iteration 909, the loss is 4253.663201379988, parameters k is -83.19786747030264 and b is 541.8441096488915\n",
      "Iteration 910, the loss is 4252.462882656904, parameters k is -83.1848733991564 and b is 541.7650582654923\n",
      "Iteration 911, the loss is 4251.262743953877, parameters k is -83.17187932801016 and b is 541.686006882093\n",
      "Iteration 912, the loss is 4250.062785270914, parameters k is -83.15888525686393 and b is 541.6069554986938\n",
      "Iteration 913, the loss is 4248.863006608006, parameters k is -83.14589118571769 and b is 541.5279041152945\n",
      "Iteration 914, the loss is 4247.6634079651585, parameters k is -83.13289711457145 and b is 541.4488527318953\n",
      "Iteration 915, the loss is 4246.463989342374, parameters k is -83.11990304342521 and b is 541.369801348496\n",
      "Iteration 916, the loss is 4245.264750739643, parameters k is -83.10690897227897 and b is 541.2907499650968\n",
      "Iteration 917, the loss is 4244.065692156979, parameters k is -83.09391490113273 and b is 541.2116985816975\n",
      "Iteration 918, the loss is 4242.866813594364, parameters k is -83.0809208299865 and b is 541.1326471982983\n",
      "Iteration 919, the loss is 4241.668115051816, parameters k is -83.06792675884026 and b is 541.053595814899\n",
      "Iteration 920, the loss is 4240.469596529329, parameters k is -83.05493268769402 and b is 540.9745444314998\n",
      "Iteration 921, the loss is 4239.271258026901, parameters k is -83.04193861654778 and b is 540.8954930481005\n",
      "Iteration 922, the loss is 4238.073099544527, parameters k is -83.02894454540154 and b is 540.8164416647013\n",
      "Iteration 923, the loss is 4236.875121082216, parameters k is -83.0159504742553 and b is 540.737390281302\n",
      "Iteration 924, the loss is 4235.6773226399655, parameters k is -83.00295640310907 and b is 540.6583388979028\n",
      "Iteration 925, the loss is 4234.479704217773, parameters k is -82.98996233196283 and b is 540.5792875145036\n",
      "Iteration 926, the loss is 4233.282265815642, parameters k is -82.97696826081659 and b is 540.5002361311043\n",
      "Iteration 927, the loss is 4232.08500743357, parameters k is -82.96397418967035 and b is 540.4211847477051\n",
      "Iteration 928, the loss is 4230.887929071555, parameters k is -82.95098011852411 and b is 540.3421333643058\n",
      "Iteration 929, the loss is 4229.691030729606, parameters k is -82.93798604737788 and b is 540.2630819809066\n",
      "Iteration 930, the loss is 4228.494312407711, parameters k is -82.92499197623164 and b is 540.1840305975073\n",
      "Iteration 931, the loss is 4227.297774105876, parameters k is -82.9119979050854 and b is 540.1049792141081\n",
      "Iteration 932, the loss is 4226.101415824103, parameters k is -82.89900383393916 and b is 540.0259278307088\n",
      "Iteration 933, the loss is 4224.905237562386, parameters k is -82.88600976279292 and b is 539.9468764473096\n",
      "Iteration 934, the loss is 4223.70923932073, parameters k is -82.87301569164669 and b is 539.8678250639103\n",
      "Iteration 935, the loss is 4222.513421099137, parameters k is -82.86002162050045 and b is 539.7887736805111\n",
      "Iteration 936, the loss is 4221.317782897596, parameters k is -82.84702754935421 and b is 539.7097222971119\n",
      "Iteration 937, the loss is 4220.122324716121, parameters k is -82.83403347820797 and b is 539.6306709137126\n",
      "Iteration 938, the loss is 4218.927046554703, parameters k is -82.82103940706173 and b is 539.5516195303134\n",
      "Iteration 939, the loss is 4217.731948413344, parameters k is -82.8080453359155 and b is 539.4725681469141\n",
      "Iteration 940, the loss is 4216.537030292047, parameters k is -82.79505126476926 and b is 539.3935167635149\n",
      "Iteration 941, the loss is 4215.342292190807, parameters k is -82.78205719362302 and b is 539.3144653801156\n",
      "Iteration 942, the loss is 4214.147734109629, parameters k is -82.76906312247678 and b is 539.2354139967164\n",
      "Iteration 943, the loss is 4212.953356048515, parameters k is -82.75606905133054 and b is 539.1563626133171\n",
      "Iteration 944, the loss is 4211.75915800745, parameters k is -82.7430749801843 and b is 539.0773112299179\n",
      "Iteration 945, the loss is 4210.565139986449, parameters k is -82.73008090903807 and b is 538.9982598465186\n",
      "Iteration 946, the loss is 4209.371301985514, parameters k is -82.71708683789183 and b is 538.9192084631194\n",
      "Iteration 947, the loss is 4208.177644004638, parameters k is -82.70409276674559 and b is 538.8401570797201\n",
      "Iteration 948, the loss is 4206.984166043809, parameters k is -82.69109869559935 and b is 538.7611056963209\n",
      "Iteration 949, the loss is 4205.790868103045, parameters k is -82.67810462445311 and b is 538.6820543129217\n",
      "Iteration 950, the loss is 4204.597750182347, parameters k is -82.66511055330687 and b is 538.6030029295224\n",
      "Iteration 951, the loss is 4203.404812281707, parameters k is -82.65211648216064 and b is 538.5239515461232\n",
      "Iteration 952, the loss is 4202.212054401122, parameters k is -82.6391224110144 and b is 538.4449001627239\n",
      "Iteration 953, the loss is 4201.019476540596, parameters k is -82.62612833986816 and b is 538.3658487793247\n",
      "Iteration 954, the loss is 4199.827078700134, parameters k is -82.61313426872192 and b is 538.2867973959254\n",
      "Iteration 955, the loss is 4198.634860879728, parameters k is -82.60014019757568 and b is 538.2077460125262\n",
      "Iteration 956, the loss is 4197.442823079388, parameters k is -82.58714612642945 and b is 538.1286946291269\n",
      "Iteration 957, the loss is 4196.2509652990975, parameters k is -82.57415205528321 and b is 538.0496432457277\n",
      "Iteration 958, the loss is 4195.059287538875, parameters k is -82.56115798413697 and b is 537.9705918623284\n",
      "Iteration 959, the loss is 4193.867789798708, parameters k is -82.54816391299073 and b is 537.8915404789292\n",
      "Iteration 960, the loss is 4192.676472078603, parameters k is -82.53516984184449 and b is 537.8124890955299\n",
      "Iteration 961, the loss is 4191.4853343785535, parameters k is -82.52217577069825 and b is 537.7334377121307\n",
      "Iteration 962, the loss is 4190.294376698568, parameters k is -82.50918169955202 and b is 537.6543863287314\n",
      "Iteration 963, the loss is 4189.103599038641, parameters k is -82.49618762840578 and b is 537.5753349453322\n",
      "Iteration 964, the loss is 4187.913001398774, parameters k is -82.48319355725954 and b is 537.496283561933\n",
      "Iteration 965, the loss is 4186.722583778961, parameters k is -82.4701994861133 and b is 537.4172321785337\n",
      "Iteration 966, the loss is 4185.532346179216, parameters k is -82.45720541496706 and b is 537.3381807951345\n",
      "Iteration 967, the loss is 4184.342288599526, parameters k is -82.44421134382083 and b is 537.2591294117352\n",
      "Iteration 968, the loss is 4183.152411039892, parameters k is -82.43121727267459 and b is 537.180078028336\n",
      "Iteration 969, the loss is 4181.962713500329, parameters k is -82.41822320152835 and b is 537.1010266449367\n",
      "Iteration 970, the loss is 4180.773195980817, parameters k is -82.40522913038211 and b is 537.0219752615375\n",
      "Iteration 971, the loss is 4179.583858481364, parameters k is -82.39223505923587 and b is 536.9429238781382\n",
      "Iteration 972, the loss is 4178.394701001976, parameters k is -82.37924098808963 and b is 536.863872494739\n",
      "Iteration 973, the loss is 4177.205723542642, parameters k is -82.3662469169434 and b is 536.7848211113397\n",
      "Iteration 974, the loss is 4176.016926103372, parameters k is -82.35325284579716 and b is 536.7057697279405\n",
      "Iteration 975, the loss is 4174.828308684157, parameters k is -82.34025877465092 and b is 536.6267183445412\n",
      "Iteration 976, the loss is 4173.639871285004, parameters k is -82.32726470350468 and b is 536.547666961142\n",
      "Iteration 977, the loss is 4172.451613905913, parameters k is -82.31427063235844 and b is 536.4686155777428\n",
      "Iteration 978, the loss is 4171.263536546875, parameters k is -82.3012765612122 and b is 536.3895641943435\n",
      "Iteration 979, the loss is 4170.075639207904, parameters k is -82.28828249006597 and b is 536.3105128109443\n",
      "Iteration 980, the loss is 4168.887921888988, parameters k is -82.27528841891973 and b is 536.231461427545\n",
      "Iteration 981, the loss is 4167.700384590131, parameters k is -82.26229434777349 and b is 536.1524100441458\n",
      "Iteration 982, the loss is 4166.5130273113355, parameters k is -82.24930027662725 and b is 536.0733586607465\n",
      "Iteration 983, the loss is 4165.325850052602, parameters k is -82.23630620548101 and b is 535.9943072773473\n",
      "Iteration 984, the loss is 4164.138852813927, parameters k is -82.22331213433478 and b is 535.915255893948\n",
      "Iteration 985, the loss is 4162.952035595308, parameters k is -82.21031806318854 and b is 535.8362045105488\n",
      "Iteration 986, the loss is 4161.765398396751, parameters k is -82.1973239920423 and b is 535.7571531271495\n",
      "Iteration 987, the loss is 4160.578941218255, parameters k is -82.18432992089606 and b is 535.6781017437503\n",
      "Iteration 988, the loss is 4159.392664059818, parameters k is -82.17133584974982 and b is 535.599050360351\n",
      "Iteration 989, the loss is 4158.206566921435, parameters k is -82.15834177860359 and b is 535.5199989769518\n",
      "Iteration 990, the loss is 4157.0206498031175, parameters k is -82.14534770745735 and b is 535.4409475935526\n",
      "Iteration 991, the loss is 4155.834912704859, parameters k is -82.13235363631111 and b is 535.3618962101533\n",
      "Iteration 992, the loss is 4154.6493556266605, parameters k is -82.11935956516487 and b is 535.2828448267541\n",
      "Iteration 993, the loss is 4153.463978568517, parameters k is -82.10636549401863 and b is 535.2037934433548\n",
      "Iteration 994, the loss is 4152.278781530442, parameters k is -82.0933714228724 and b is 535.1247420599556\n",
      "Iteration 995, the loss is 4151.093764512414, parameters k is -82.08037735172616 and b is 535.0456906765563\n",
      "Iteration 996, the loss is 4149.908927514456, parameters k is -82.06738328057992 and b is 534.9666392931571\n",
      "Iteration 997, the loss is 4148.724270536556, parameters k is -82.05438920943368 and b is 534.8875879097578\n",
      "Iteration 998, the loss is 4147.539793578713, parameters k is -82.04139513828744 and b is 534.8085365263586\n",
      "Iteration 999, the loss is 4146.35549664093, parameters k is -82.0284010671412 and b is 534.7294851429593\n",
      "Iteration 1000, the loss is 4145.171379723205, parameters k is -82.01540699599497 and b is 534.6504337595601\n",
      "Iteration 1001, the loss is 4143.987442825543, parameters k is -82.00241292484873 and b is 534.5713823761608\n",
      "Iteration 1002, the loss is 4142.803685947937, parameters k is -81.98941885370249 and b is 534.4923309927616\n",
      "Iteration 1003, the loss is 4141.620109090393, parameters k is -81.97642478255625 and b is 534.4132796093623\n",
      "Iteration 1004, the loss is 4140.436712252908, parameters k is -81.96343071141001 and b is 534.3342282259631\n",
      "Iteration 1005, the loss is 4139.253495435483, parameters k is -81.95043664026377 and b is 534.2551768425639\n",
      "Iteration 1006, the loss is 4138.070458638118, parameters k is -81.93744256911754 and b is 534.1761254591646\n",
      "Iteration 1007, the loss is 4136.887601860815, parameters k is -81.9244484979713 and b is 534.0970740757654\n",
      "Iteration 1008, the loss is 4135.704925103567, parameters k is -81.91145442682506 and b is 534.0180226923661\n",
      "Iteration 1009, the loss is 4134.522428366381, parameters k is -81.89846035567882 and b is 533.9389713089669\n",
      "Iteration 1010, the loss is 4133.340111649254, parameters k is -81.88546628453258 and b is 533.8599199255676\n",
      "Iteration 1011, the loss is 4132.157974952184, parameters k is -81.87247221338635 and b is 533.7808685421684\n",
      "Iteration 1012, the loss is 4130.976018275178, parameters k is -81.8594781422401 and b is 533.7018171587691\n",
      "Iteration 1013, the loss is 4129.794241618226, parameters k is -81.84648407109387 and b is 533.6227657753699\n",
      "Iteration 1014, the loss is 4128.612644981341, parameters k is -81.83348999994763 and b is 533.5437143919706\n",
      "Iteration 1015, the loss is 4127.431228364513, parameters k is -81.82049592880139 and b is 533.4646630085714\n",
      "Iteration 1016, the loss is 4126.249991767739, parameters k is -81.80750185765515 and b is 533.3856116251721\n",
      "Iteration 1017, the loss is 4125.06893519103, parameters k is -81.79450778650892 and b is 533.3065602417729\n",
      "Iteration 1018, the loss is 4123.88805863438, parameters k is -81.78151371536268 and b is 533.2275088583737\n",
      "Iteration 1019, the loss is 4122.707362097787, parameters k is -81.76851964421644 and b is 533.1484574749744\n",
      "Iteration 1020, the loss is 4121.526845581255, parameters k is -81.7555255730702 and b is 533.0694060915752\n",
      "Iteration 1021, the loss is 4120.346509084786, parameters k is -81.74253150192396 and b is 532.9903547081759\n",
      "Iteration 1022, the loss is 4119.166352608369, parameters k is -81.72953743077773 and b is 532.9113033247767\n",
      "Iteration 1023, the loss is 4117.986376152018, parameters k is -81.71654335963149 and b is 532.8322519413774\n",
      "Iteration 1024, the loss is 4116.806579715729, parameters k is -81.70354928848525 and b is 532.7532005579782\n",
      "Iteration 1025, the loss is 4115.6269632994945, parameters k is -81.69055521733901 and b is 532.6741491745789\n",
      "Iteration 1026, the loss is 4114.447526903322, parameters k is -81.67756114619277 and b is 532.5950977911797\n",
      "Iteration 1027, the loss is 4113.268270527204, parameters k is -81.66456707504653 and b is 532.5160464077804\n",
      "Iteration 1028, the loss is 4112.08919417115, parameters k is -81.6515730039003 and b is 532.4369950243812\n",
      "Iteration 1029, the loss is 4110.910297835154, parameters k is -81.63857893275406 and b is 532.357943640982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1030, the loss is 4109.731581519219, parameters k is -81.62558486160782 and b is 532.2788922575827\n",
      "Iteration 1031, the loss is 4108.553045223341, parameters k is -81.61259079046158 and b is 532.1998408741835\n",
      "Iteration 1032, the loss is 4107.374688947528, parameters k is -81.59959671931534 and b is 532.1207894907842\n",
      "Iteration 1033, the loss is 4106.1965126917685, parameters k is -81.5866026481691 and b is 532.041738107385\n",
      "Iteration 1034, the loss is 4105.018516456073, parameters k is -81.57360857702287 and b is 531.9626867239857\n",
      "Iteration 1035, the loss is 4103.840700240435, parameters k is -81.56061450587663 and b is 531.8836353405865\n",
      "Iteration 1036, the loss is 4102.6630640448575, parameters k is -81.54762043473039 and b is 531.8045839571872\n",
      "Iteration 1037, the loss is 4101.485607869338, parameters k is -81.53462636358415 and b is 531.725532573788\n",
      "Iteration 1038, the loss is 4100.308331713879, parameters k is -81.52163229243791 and b is 531.6464811903887\n",
      "Iteration 1039, the loss is 4099.131235578483, parameters k is -81.50863822129168 and b is 531.5674298069895\n",
      "Iteration 1040, the loss is 4097.95431946314, parameters k is -81.49564415014544 and b is 531.4883784235902\n",
      "Iteration 1041, the loss is 4096.777583367857, parameters k is -81.4826500789992 and b is 531.409327040191\n",
      "Iteration 1042, the loss is 4095.6010272926396, parameters k is -81.46965600785296 and b is 531.3302756567917\n",
      "Iteration 1043, the loss is 4094.4246512374793, parameters k is -81.45666193670672 and b is 531.2512242733925\n",
      "Iteration 1044, the loss is 4093.2484552023793, parameters k is -81.44366786556049 and b is 531.1721728899932\n",
      "Iteration 1045, the loss is 4092.072439187335, parameters k is -81.43067379441425 and b is 531.093121506594\n",
      "Iteration 1046, the loss is 4090.8966031923537, parameters k is -81.41767972326801 and b is 531.0140701231948\n",
      "Iteration 1047, the loss is 4089.72094721743, parameters k is -81.40468565212177 and b is 530.9350187397955\n",
      "Iteration 1048, the loss is 4088.545471262567, parameters k is -81.39169158097553 and b is 530.8559673563963\n",
      "Iteration 1049, the loss is 4087.3701753277633, parameters k is -81.3786975098293 and b is 530.776915972997\n",
      "Iteration 1050, the loss is 4086.195059413021, parameters k is -81.36570343868306 and b is 530.6978645895978\n",
      "Iteration 1051, the loss is 4085.020123518331, parameters k is -81.35270936753682 and b is 530.6188132061985\n",
      "Iteration 1052, the loss is 4083.845367643708, parameters k is -81.33971529639058 and b is 530.5397618227993\n",
      "Iteration 1053, the loss is 4082.695952113191, parameters k is -81.32672122524434 and b is 530.4567578702301\n",
      "Iteration 1054, the loss is 4081.5467065581784, parameters k is -81.3137271540981 and b is 530.373753917661\n",
      "Iteration 1055, the loss is 4080.397630978659, parameters k is -81.30073308295187 and b is 530.2907499650919\n",
      "Iteration 1056, the loss is 4079.248725374639, parameters k is -81.28773901180563 and b is 530.2077460125228\n",
      "Iteration 1057, the loss is 4078.0999897461206, parameters k is -81.27474494065939 and b is 530.1247420599536\n",
      "Iteration 1058, the loss is 4076.951424093109, parameters k is -81.26175086951315 and b is 530.0417381073845\n",
      "Iteration 1059, the loss is 4075.803028415592, parameters k is -81.24875679836691 and b is 529.9587341548154\n",
      "Iteration 1060, the loss is 4074.654802713577, parameters k is -81.23576272722067 and b is 529.8757302022462\n",
      "Iteration 1061, the loss is 4073.5067469870596, parameters k is -81.22276865607444 and b is 529.7927262496771\n",
      "Iteration 1062, the loss is 4072.3588612360454, parameters k is -81.2097745849282 and b is 529.709722297108\n",
      "Iteration 1063, the loss is 4071.21114546053, parameters k is -81.19678051378196 and b is 529.6267183445389\n",
      "Iteration 1064, the loss is 4070.063599660518, parameters k is -81.18378644263572 and b is 529.5437143919697\n",
      "Iteration 1065, the loss is 4068.9162238360022, parameters k is -81.17079237148948 and b is 529.4607104394006\n",
      "Iteration 1066, the loss is 4067.769017986985, parameters k is -81.15779830034325 and b is 529.3777064868315\n",
      "Iteration 1067, the loss is 4066.621982113475, parameters k is -81.144804229197 and b is 529.2947025342623\n",
      "Iteration 1068, the loss is 4065.4751162154566, parameters k is -81.13181015805077 and b is 529.2116985816932\n",
      "Iteration 1069, the loss is 4064.328420292949, parameters k is -81.11881608690453 and b is 529.1286946291241\n",
      "Iteration 1070, the loss is 4063.1818943459334, parameters k is -81.10582201575829 and b is 529.045690676555\n",
      "Iteration 1071, the loss is 4062.035538374422, parameters k is -81.09282794461205 and b is 528.9626867239858\n",
      "Iteration 1072, the loss is 4060.8893523784104, parameters k is -81.07983387346582 and b is 528.8796827714167\n",
      "Iteration 1073, the loss is 4059.743336357899, parameters k is -81.06683980231958 and b is 528.7966788188476\n",
      "Iteration 1074, the loss is 4058.597490312892, parameters k is -81.05384573117334 and b is 528.7136748662784\n",
      "Iteration 1075, the loss is 4057.4518142433762, parameters k is -81.0408516600271 and b is 528.6306709137093\n",
      "Iteration 1076, the loss is 4056.3063081493688, parameters k is -81.02785758888086 and b is 528.5476669611402\n",
      "Iteration 1077, the loss is 4055.160972030851, parameters k is -81.01486351773463 and b is 528.464663008571\n",
      "Iteration 1078, the loss is 4054.015805887847, parameters k is -81.00186944658839 and b is 528.3816590560019\n",
      "Iteration 1079, the loss is 4052.870809720339, parameters k is -80.98887537544215 and b is 528.2986551034328\n",
      "Iteration 1080, the loss is 4051.7259835283253, parameters k is -80.97588130429591 and b is 528.2156511508637\n",
      "Iteration 1081, the loss is 4050.5813273118156, parameters k is -80.96288723314967 and b is 528.1326471982945\n",
      "Iteration 1082, the loss is 4049.4368410708107, parameters k is -80.94989316200343 and b is 528.0496432457254\n",
      "Iteration 1083, the loss is 4048.292524805299, parameters k is -80.9368990908572 and b is 527.9666392931563\n",
      "Iteration 1084, the loss is 4047.1483785152923, parameters k is -80.92390501971096 and b is 527.8836353405871\n",
      "Iteration 1085, the loss is 4046.0044022007855, parameters k is -80.91091094856472 and b is 527.800631388018\n",
      "Iteration 1086, the loss is 4044.860595861781, parameters k is -80.89791687741848 and b is 527.7176274354489\n",
      "Iteration 1087, the loss is 4043.716959498266, parameters k is -80.88492280627224 and b is 527.6346234828798\n",
      "Iteration 1088, the loss is 4042.5734931102625, parameters k is -80.871928735126 and b is 527.5516195303106\n",
      "Iteration 1089, the loss is 4041.430196697758, parameters k is -80.85893466397977 and b is 527.4686155777415\n",
      "Iteration 1090, the loss is 4040.2870702607484, parameters k is -80.84594059283353 and b is 527.3856116251724\n",
      "Iteration 1091, the loss is 4039.144113799246, parameters k is -80.83294652168729 and b is 527.3026076726032\n",
      "Iteration 1092, the loss is 4038.001327313242, parameters k is -80.81995245054105 and b is 527.2196037200341\n",
      "Iteration 1093, the loss is 4036.8587108027373, parameters k is -80.80695837939481 and b is 527.136599767465\n",
      "Iteration 1094, the loss is 4035.7162642677285, parameters k is -80.79396430824858 and b is 527.0535958148959\n",
      "Iteration 1095, the loss is 4034.573987708225, parameters k is -80.78097023710234 and b is 526.9705918623267\n",
      "Iteration 1096, the loss is 4033.431881124221, parameters k is -80.7679761659561 and b is 526.8875879097576\n",
      "Iteration 1097, the loss is 4032.289944515718, parameters k is -80.75498209480986 and b is 526.8045839571885\n",
      "Iteration 1098, the loss is 4031.1481778827133, parameters k is -80.74198802366362 and b is 526.7215800046193\n",
      "Iteration 1099, the loss is 4030.0065812252114, parameters k is -80.72899395251738 and b is 526.6385760520502\n",
      "Iteration 1100, the loss is 4028.865154543209, parameters k is -80.71599988137115 and b is 526.5555720994811\n",
      "Iteration 1101, the loss is 4027.7238978367013, parameters k is -80.70300581022491 and b is 526.472568146912\n",
      "Iteration 1102, the loss is 4026.5828111057017, parameters k is -80.69001173907867 and b is 526.3895641943428\n",
      "Iteration 1103, the loss is 4025.441894350205, parameters k is -80.67701766793243 and b is 526.3065602417737\n",
      "Iteration 1104, the loss is 4024.3011475701974, parameters k is -80.6640235967862 and b is 526.2235562892046\n",
      "Iteration 1105, the loss is 4023.1605707656986, parameters k is -80.65102952563996 and b is 526.1405523366354\n",
      "Iteration 1106, the loss is 4022.0201639367, parameters k is -80.63803545449372 and b is 526.0575483840663\n",
      "Iteration 1107, the loss is 4020.879927083199, parameters k is -80.62504138334748 and b is 525.9745444314972\n",
      "Iteration 1108, the loss is 4019.7398602051994, parameters k is -80.61204731220124 and b is 525.891540478928\n",
      "Iteration 1109, the loss is 4018.599963302697, parameters k is -80.599053241055 and b is 525.8085365263589\n",
      "Iteration 1110, the loss is 4017.460236375691, parameters k is -80.58605916990876 and b is 525.7255325737898\n",
      "Iteration 1111, the loss is 4016.3206794241974, parameters k is -80.57306509876253 and b is 525.6425286212207\n",
      "Iteration 1112, the loss is 4015.181292448197, parameters k is -80.56007102761629 and b is 525.5595246686515\n",
      "Iteration 1113, the loss is 4014.0420754476972, parameters k is -80.54707695647005 and b is 525.4765207160824\n",
      "Iteration 1114, the loss is 4012.9030284227015, parameters k is -80.53408288532381 and b is 525.3935167635133\n",
      "Iteration 1115, the loss is 4011.764151373201, parameters k is -80.52108881417757 and b is 525.3105128109441\n",
      "Iteration 1116, the loss is 4010.625444299202, parameters k is -80.50809474303134 and b is 525.227508858375\n",
      "Iteration 1117, the loss is 4009.4869072007073, parameters k is -80.4951006718851 and b is 525.1445049058059\n",
      "Iteration 1118, the loss is 4008.3485400777104, parameters k is -80.48210660073886 and b is 525.0615009532368\n",
      "Iteration 1119, the loss is 4007.2103429302106, parameters k is -80.46911252959262 and b is 524.9784970006676\n",
      "Iteration 1120, the loss is 4006.0723157582165, parameters k is -80.45611845844638 and b is 524.8954930480985\n",
      "Iteration 1121, the loss is 4004.934458561721, parameters k is -80.44312438730014 and b is 524.8124890955294\n",
      "Iteration 1122, the loss is 4003.7967713407265, parameters k is -80.4301303161539 and b is 524.7294851429602\n",
      "Iteration 1123, the loss is 4002.659254095228, parameters k is -80.41713624500767 and b is 524.6464811903911\n",
      "Iteration 1124, the loss is 4001.5219068252354, parameters k is -80.40414217386143 and b is 524.563477237822\n",
      "Iteration 1125, the loss is 4000.3847295307405, parameters k is -80.39114810271519 and b is 524.4804732852529\n",
      "Iteration 1126, the loss is 3999.247722211744, parameters k is -80.37815403156895 and b is 524.3974693326837\n",
      "Iteration 1127, the loss is 3998.1108848682534, parameters k is -80.36515996042272 and b is 524.3144653801146\n",
      "Iteration 1128, the loss is 3996.9742175002566, parameters k is -80.35216588927648 and b is 524.2314614275455\n",
      "Iteration 1129, the loss is 3995.837720107766, parameters k is -80.33917181813024 and b is 524.1484574749763\n",
      "Iteration 1130, the loss is 3994.7013926907707, parameters k is -80.326177746984 and b is 524.0654535224072\n",
      "Iteration 1131, the loss is 3993.56523524928, parameters k is -80.31318367583776 and b is 523.9824495698381\n",
      "Iteration 1132, the loss is 3992.429247783283, parameters k is -80.30018960469152 and b is 523.899445617269\n",
      "Iteration 1133, the loss is 3991.2934302927933, parameters k is -80.28719553354529 and b is 523.8164416646998\n",
      "Iteration 1134, the loss is 3990.1577827777987, parameters k is -80.27420146239905 and b is 523.7334377121307\n",
      "Iteration 1135, the loss is 3989.022305238309, parameters k is -80.26120739125281 and b is 523.6504337595616\n",
      "Iteration 1136, the loss is 3987.88699767432, parameters k is -80.24821332010657 and b is 523.5674298069924\n",
      "Iteration 1137, the loss is 3986.7518600858275, parameters k is -80.23521924896033 and b is 523.4844258544233\n",
      "Iteration 1138, the loss is 3985.6168924728404, parameters k is -80.2222251778141 and b is 523.4014219018542\n",
      "Iteration 1139, the loss is 3984.4820948353445, parameters k is -80.20923110666786 and b is 523.318417949285\n",
      "Iteration 1140, the loss is 3983.347467173358, parameters k is -80.19623703552162 and b is 523.2354139967159\n",
      "Iteration 1141, the loss is 3982.2130094868676, parameters k is -80.18324296437538 and b is 523.1524100441468\n",
      "Iteration 1142, the loss is 3981.078721775877, parameters k is -80.17024889322914 and b is 523.0694060915777\n",
      "Iteration 1143, the loss is 3979.9446040403895, parameters k is -80.1572548220829 and b is 522.9864021390085\n",
      "Iteration 1144, the loss is 3978.8106562804, parameters k is -80.14426075093667 and b is 522.9033981864394\n",
      "Iteration 1145, the loss is 3977.6768784959077, parameters k is -80.13126667979043 and b is 522.8203942338703\n",
      "Iteration 1146, the loss is 3976.5432706869237, parameters k is -80.11827260864419 and b is 522.7373902813011\n",
      "Iteration 1147, the loss is 3975.409832853437, parameters k is -80.10527853749795 and b is 522.654386328732\n",
      "Iteration 1148, the loss is 3974.2765649954463, parameters k is -80.09228446635171 and b is 522.5713823761629\n",
      "Iteration 1149, the loss is 3973.1434671129637, parameters k is -80.07929039520548 and b is 522.4883784235938\n",
      "Iteration 1150, the loss is 3972.0105392059786, parameters k is -80.06629632405924 and b is 522.4053744710246\n",
      "Iteration 1151, the loss is 3970.8777812744847, parameters k is -80.053302252913 and b is 522.3223705184555\n",
      "Iteration 1152, the loss is 3969.745193318501, parameters k is -80.04030818176676 and b is 522.2393665658864\n",
      "Iteration 1153, the loss is 3968.6127753380156, parameters k is -80.02731411062052 and b is 522.1563626133172\n",
      "Iteration 1154, the loss is 3967.4805273330235, parameters k is -80.01432003947428 and b is 522.0733586607481\n",
      "Iteration 1155, the loss is 3966.348449303544, parameters k is -80.00132596832805 and b is 521.990354708179\n",
      "Iteration 1156, the loss is 3965.216541249563, parameters k is -79.98833189718181 and b is 521.9073507556099\n",
      "Iteration 1157, the loss is 3964.0848031710752, parameters k is -79.97533782603557 and b is 521.8243468030407\n",
      "Iteration 1158, the loss is 3962.9532350680875, parameters k is -79.96234375488933 and b is 521.7413428504716\n",
      "Iteration 1159, the loss is 3961.8218369406077, parameters k is -79.9493496837431 and b is 521.6583388979025\n",
      "Iteration 1160, the loss is 3960.690608788622, parameters k is -79.93635561259686 and b is 521.5753349453333\n",
      "Iteration 1161, the loss is 3959.55955061214, parameters k is -79.92336154145062 and b is 521.4923309927642\n",
      "Iteration 1162, the loss is 3958.428662411156, parameters k is -79.91036747030438 and b is 521.4093270401951\n",
      "Iteration 1163, the loss is 3957.2979441856746, parameters k is -79.89737339915814 and b is 521.326323087626\n",
      "Iteration 1164, the loss is 3956.167395935696, parameters k is -79.8843793280119 and b is 521.2433191350568\n",
      "Iteration 1165, the loss is 3955.03701766121, parameters k is -79.87138525686566 and b is 521.1603151824877\n",
      "Iteration 1166, the loss is 3953.9068093622304, parameters k is -79.85839118571943 and b is 521.0773112299186\n",
      "Iteration 1167, the loss is 3952.7767710387493, parameters k is -79.84539711457319 and b is 520.9943072773494\n",
      "Iteration 1168, the loss is 3951.6469026907644, parameters k is -79.83240304342695 and b is 520.9113033247803\n",
      "Iteration 1169, the loss is 3950.517204318288, parameters k is -79.81940897228071 and b is 520.8282993722112\n",
      "Iteration 1170, the loss is 3949.387675921303, parameters k is -79.80641490113447 and b is 520.745295419642\n",
      "Iteration 1171, the loss is 3948.2583174998244, parameters k is -79.79342082998824 and b is 520.6622914670729\n",
      "Iteration 1172, the loss is 3947.1291290538425, parameters k is -79.780426758842 and b is 520.5792875145038\n",
      "Iteration 1173, the loss is 3946.0001105833635, parameters k is -79.76743268769576 and b is 520.4962835619347\n",
      "Iteration 1174, the loss is 3944.8712620883866, parameters k is -79.75443861654952 and b is 520.4132796093655\n",
      "Iteration 1175, the loss is 3943.7425835689073, parameters k is -79.74144454540328 and b is 520.3302756567964\n",
      "Iteration 1176, the loss is 3942.614075024932, parameters k is -79.72845047425704 and b is 520.2472717042273\n",
      "Iteration 1177, the loss is 3941.4857364564537, parameters k is -79.7154564031108 and b is 520.1642677516581\n",
      "Iteration 1178, the loss is 3940.357567863469, parameters k is -79.70246233196457 and b is 520.081263799089\n",
      "Iteration 1179, the loss is 3939.2295692459975, parameters k is -79.68946826081833 and b is 519.9982598465199\n",
      "Iteration 1180, the loss is 3938.1017406040187, parameters k is -79.67647418967209 and b is 519.9152558939508\n",
      "Iteration 1181, the loss is 3936.974081937544, parameters k is -79.66348011852585 and b is 519.8322519413816\n",
      "Iteration 1182, the loss is 3935.846593246568, parameters k is -79.65048604737962 and b is 519.7492479888125\n",
      "Iteration 1183, the loss is 3934.7192745310913, parameters k is -79.63749197623338 and b is 519.6662440362434\n",
      "Iteration 1184, the loss is 3933.592125791112, parameters k is -79.62449790508714 and b is 519.5832400836742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1185, the loss is 3932.465147026641, parameters k is -79.6115038339409 and b is 519.5002361311051\n",
      "Iteration 1186, the loss is 3931.3383382376624, parameters k is -79.59850976279466 and b is 519.417232178536\n",
      "Iteration 1187, the loss is 3930.211699424193, parameters k is -79.58551569164842 and b is 519.3342282259669\n",
      "Iteration 1188, the loss is 3929.0852305862145, parameters k is -79.57252162050219 and b is 519.2512242733977\n",
      "Iteration 1189, the loss is 3927.9589317237355, parameters k is -79.55952754935595 and b is 519.1682203208286\n",
      "Iteration 1190, the loss is 3926.8328028367655, parameters k is -79.54653347820971 and b is 519.0852163682595\n",
      "Iteration 1191, the loss is 3925.706843925294, parameters k is -79.53353940706347 and b is 519.0022124156903\n",
      "Iteration 1192, the loss is 3924.5810549893195, parameters k is -79.52054533591723 and b is 518.9192084631212\n",
      "Iteration 1193, the loss is 3923.4554360288466, parameters k is -79.507551264771 and b is 518.8362045105521\n",
      "Iteration 1194, the loss is 3922.3299870438705, parameters k is -79.49455719362476 and b is 518.753200557983\n",
      "Iteration 1195, the loss is 3921.204708034401, parameters k is -79.48156312247852 and b is 518.6701966054138\n",
      "Iteration 1196, the loss is 3920.0795990004253, parameters k is -79.46856905133228 and b is 518.5871926528447\n",
      "Iteration 1197, the loss is 3918.9546599419587, parameters k is -79.45557498018604 and b is 518.5041887002756\n",
      "Iteration 1198, the loss is 3917.8298908589877, parameters k is -79.4425809090398 and b is 518.4211847477064\n",
      "Iteration 1199, the loss is 3916.705291751517, parameters k is -79.42958683789357 and b is 518.3381807951373\n",
      "Iteration 1200, the loss is 3915.5808626195467, parameters k is -79.41659276674733 and b is 518.2551768425682\n",
      "Iteration 1201, the loss is 3914.4298742615615, parameters k is -79.40359869560109 and b is 518.1761254591689\n",
      "Iteration 1202, the loss is 3913.305774480065, parameters k is -79.39060462445485 and b is 518.0931215065998\n",
      "Iteration 1203, the loss is 3912.155125517122, parameters k is -79.37761055330861 and b is 518.0140701232006\n",
      "Iteration 1204, the loss is 3911.0046565742327, parameters k is -79.36461648216238 and b is 517.9350187398013\n",
      "Iteration 1205, the loss is 3909.8810455181965, parameters k is -79.35162241101614 and b is 517.8520147872322\n",
      "Iteration 1206, the loss is 3908.7309159703445, parameters k is -79.3386283398699 and b is 517.7729634038329\n",
      "Iteration 1207, the loss is 3907.6076342647816, parameters k is -79.32563426872366 and b is 517.6899594512638\n",
      "Iteration 1208, the loss is 3906.431196934825, parameters k is -79.31264019757742 and b is 517.6148606370346\n",
      "Iteration 1209, the loss is 3905.3082339792154, parameters k is -79.29964612643118 and b is 517.5318566844654\n",
      "Iteration 1210, the loss is 3904.185440999107, parameters k is -79.28665205528495 and b is 517.4488527318963\n",
      "Iteration 1211, the loss is 3903.009553773882, parameters k is -79.27365798413871 and b is 517.373753917667\n",
      "Iteration 1212, the loss is 3901.8870795437297, parameters k is -79.26066391299247 and b is 517.2907499650979\n",
      "Iteration 1213, the loss is 3900.738168846132, parameters k is -79.24766984184623 and b is 517.2116985816987\n",
      "Iteration 1214, the loss is 3899.6160239664555, parameters k is -79.2346757707 and b is 517.1286946291295\n",
      "Iteration 1215, the loss is 3898.4674526638887, parameters k is -79.22168169955376 and b is 517.0496432457303\n",
      "Iteration 1216, the loss is 3897.319061381393, parameters k is -79.20868762840752 and b is 516.970591862331\n",
      "Iteration 1217, the loss is 3896.1708501189414, parameters k is -79.19569355726128 and b is 516.8915404789318\n",
      "Iteration 1218, the loss is 3895.0493533397, parameters k is -79.18269948611504 and b is 516.8085365263627\n",
      "Iteration 1219, the loss is 3893.9014814722937, parameters k is -79.1697054149688 and b is 516.7294851429634\n",
      "Iteration 1220, the loss is 3892.7537896249496, parameters k is -79.15671134382256 and b is 516.6504337595642\n",
      "Iteration 1221, the loss is 3891.6062777976617, parameters k is -79.14371727267633 and b is 516.5713823761649\n",
      "Iteration 1222, the loss is 3890.4854291188476, parameters k is -79.13072320153009 and b is 516.4883784235958\n",
      "Iteration 1223, the loss is 3889.3382566865944, parameters k is -79.11772913038385 and b is 516.4093270401966\n",
      "Iteration 1224, the loss is 3888.1912642744046, parameters k is -79.10473505923761 and b is 516.3302756567973\n",
      "Iteration 1225, the loss is 3887.044451882271, parameters k is -79.09174098809137 and b is 516.2512242733981\n",
      "Iteration 1226, the loss is 3885.897819510201, parameters k is -79.07874691694514 and b is 516.1721728899988\n",
      "Iteration 1227, the loss is 3884.777778306792, parameters k is -79.0657528457989 and b is 516.0891689374297\n",
      "Iteration 1228, the loss is 3883.631485329756, parameters k is -79.05275877465266 and b is 516.0101175540304\n",
      "Iteration 1229, the loss is 3882.485372372782, parameters k is -79.03976470350642 and b is 515.9310661706312\n",
      "Iteration 1230, the loss is 3881.339439435862, parameters k is -79.02677063236018 and b is 515.852014787232\n",
      "Iteration 1231, the loss is 3880.2200463328923, parameters k is -79.01377656121394 and b is 515.7690108346628\n",
      "Iteration 1232, the loss is 3879.0744527910074, parameters k is -79.0007824900677 and b is 515.6899594512636\n",
      "Iteration 1233, the loss is 3877.9290392691887, parameters k is -78.98778841892147 and b is 515.6109080678643\n",
      "Iteration 1234, the loss is 3876.783805767424, parameters k is -78.97479434777523 and b is 515.5318566844651\n",
      "Iteration 1235, the loss is 3875.6387522857276, parameters k is -78.96180027662899 and b is 515.4528053010658\n",
      "Iteration 1236, the loss is 3874.5201666581556, parameters k is -78.94880620548275 and b is 515.3698013484967\n",
      "Iteration 1237, the loss is 3873.3754525714944, parameters k is -78.93581213433652 and b is 515.2907499650975\n",
      "Iteration 1238, the loss is 3872.230918504891, parameters k is -78.92281806319028 and b is 515.2116985816982\n",
      "Iteration 1239, the loss is 3871.086564458348, parameters k is -78.90982399204404 and b is 515.132647198299\n",
      "Iteration 1240, the loss is 3869.9686269312083, parameters k is -78.8968299208978 and b is 515.0496432457298\n",
      "Iteration 1241, the loss is 3868.824612279698, parameters k is -78.88383584975156 and b is 514.9705918623306\n",
      "Iteration 1242, the loss is 3867.680777648244, parameters k is -78.87084177860532 and b is 514.8915404789313\n",
      "Iteration 1243, the loss is 3866.537123036864, parameters k is -78.85784770745909 and b is 514.8124890955321\n",
      "Iteration 1244, the loss is 3865.4198336101545, parameters k is -78.84485363631285 and b is 514.729485142963\n",
      "Iteration 1245, the loss is 3864.2765183938022, parameters k is -78.83185956516661 and b is 514.6504337595637\n",
      "Iteration 1246, the loss is 3863.133383197507, parameters k is -78.81886549402037 and b is 514.5713823761645\n",
      "Iteration 1247, the loss is 3861.990428021275, parameters k is -78.80587142287413 and b is 514.4923309927652\n",
      "Iteration 1248, the loss is 3860.847652865101, parameters k is -78.7928773517279 and b is 514.413279609366\n",
      "Iteration 1249, the loss is 3859.7311709137994, parameters k is -78.77988328058166 and b is 514.3302756567969\n",
      "Iteration 1250, the loss is 3858.5887351526644, parameters k is -78.76688920943542 and b is 514.2512242733976\n",
      "Iteration 1251, the loss is 3857.446479411589, parameters k is -78.75389513828918 and b is 514.1721728899984\n",
      "Iteration 1252, the loss is 3856.304403690567, parameters k is -78.74090106714294 and b is 514.0931215065991\n",
      "Iteration 1253, the loss is 3855.1885698396973, parameters k is -78.7279069959967 and b is 514.01011755403\n",
      "Iteration 1254, the loss is 3854.046833513716, parameters k is -78.71491292485047 and b is 513.9310661706307\n",
      "Iteration 1255, the loss is 3852.9052772077926, parameters k is -78.70191885370423 and b is 513.8520147872315\n",
      "Iteration 1256, the loss is 3851.763900921931, parameters k is -78.68892478255799 and b is 513.7729634038323\n",
      "Iteration 1257, the loss is 3850.6487151714928, parameters k is -78.67593071141175 and b is 513.6899594512631\n",
      "Iteration 1258, the loss is 3849.5076782806705, parameters k is -78.66293664026551 and b is 513.6109080678639\n",
      "Iteration 1259, the loss is 3848.3668214099043, parameters k is -78.64994256911928 and b is 513.5318566844646\n",
      "Iteration 1260, the loss is 3847.2261445591917, parameters k is -78.63694849797304 and b is 513.4528053010654\n",
      "Iteration 1261, the loss is 3846.085647728544, parameters k is -78.6239544268268 and b is 513.3737539176661\n",
      "Iteration 1262, the loss is 3844.9712694535197, parameters k is -78.61096035568056 and b is 513.290749965097\n",
      "Iteration 1263, the loss is 3843.831112017906, parameters k is -78.59796628453432 and b is 513.2116985816978\n",
      "Iteration 1264, the loss is 3842.6911346023494, parameters k is -78.58497221338808 and b is 513.1326471982985\n",
      "Iteration 1265, the loss is 3841.551337206865, parameters k is -78.57197814224185 and b is 513.0535958148993\n",
      "Iteration 1266, the loss is 3840.437607032263, parameters k is -78.55898407109561 and b is 512.9705918623301\n",
      "Iteration 1267, the loss is 3839.298149031812, parameters k is -78.54598999994937 and b is 512.8915404789309\n",
      "Iteration 1268, the loss is 3838.1588710514156, parameters k is -78.53299592880313 and b is 512.8124890955316\n",
      "Iteration 1269, the loss is 3837.0197730910763, parameters k is -78.5200018576569 and b is 512.7334377121324\n",
      "Iteration 1270, the loss is 3835.880855150799, parameters k is -78.50700778651066 and b is 512.6543863287332\n",
      "Iteration 1271, the loss is 3834.7679324516175, parameters k is -78.49401371536442 and b is 512.571382376164\n",
      "Iteration 1272, the loss is 3833.6293539063718, parameters k is -78.48101964421818 and b is 512.4923309927648\n",
      "Iteration 1273, the loss is 3832.4909553811917, parameters k is -78.46802557307194 and b is 512.4132796093655\n",
      "Iteration 1274, the loss is 3831.352736876067, parameters k is -78.4550315019257 and b is 512.3342282259663\n",
      "Iteration 1275, the loss is 3830.2404622773115, parameters k is -78.44203743077946 and b is 512.2512242733972\n",
      "Iteration 1276, the loss is 3829.1025831672255, parameters k is -78.42904335963323 and b is 512.1721728899979\n",
      "Iteration 1277, the loss is 3827.9648840772056, parameters k is -78.41604928848699 and b is 512.0931215065987\n",
      "Iteration 1278, the loss is 3826.8273650072356, parameters k is -78.40305521734075 and b is 512.0140701231994\n",
      "Iteration 1279, the loss is 3825.7157385089113, parameters k is -78.39006114619451 and b is 511.93106617063023\n",
      "Iteration 1280, the loss is 3824.5785588339822, parameters k is -78.37706707504827 and b is 511.85201478723104\n",
      "Iteration 1281, the loss is 3823.4415591791076, parameters k is -78.36407300390204 and b is 511.77296340383185\n",
      "Iteration 1282, the loss is 3822.3047395442964, parameters k is -78.3510789327558 and b is 511.69391202043266\n",
      "Iteration 1283, the loss is 3821.168099929546, parameters k is -78.33808486160956 and b is 511.6148606370335\n",
      "Iteration 1284, the loss is 3820.0572809066293, parameters k is -78.32509079046332 and b is 511.5318566844643\n",
      "Iteration 1285, the loss is 3818.920980686916, parameters k is -78.31209671931708 and b is 511.4528053010651\n",
      "Iteration 1286, the loss is 3817.7848604872606, parameters k is -78.29910264817084 and b is 511.3737539176659\n",
      "Iteration 1287, the loss is 3816.648920307667, parameters k is -78.2861085770246 and b is 511.2947025342667\n",
      "Iteration 1288, the loss is 3815.538749385177, parameters k is -78.27311450587837 and b is 511.21169858169753\n",
      "Iteration 1289, the loss is 3814.4031486006147, parameters k is -78.26012043473213 and b is 511.13264719829834\n",
      "Iteration 1290, the loss is 3813.267727836117, parameters k is -78.24712636358589 and b is 511.05359581489915\n",
      "Iteration 1291, the loss is 3812.1324870916824, parameters k is -78.23413229243965 and b is 510.97454443149996\n",
      "Iteration 1292, the loss is 3810.9974263672975, parameters k is -78.22113822129342 and b is 510.8954930481008\n",
      "Iteration 1293, the loss is 3809.8880629202195, parameters k is -78.20814415014718 and b is 510.8124890955316\n",
      "Iteration 1294, the loss is 3808.7533415908742, parameters k is -78.19515007900094 and b is 510.7334377121324\n",
      "Iteration 1295, the loss is 3807.618800281595, parameters k is -78.1821560078547 and b is 510.6543863287332\n",
      "Iteration 1296, the loss is 3806.509925559965, parameters k is -78.16916193670846 and b is 510.571382376164\n",
      "Iteration 1297, the loss is 3805.40122081384, parameters k is -78.15616786556222 and b is 510.48837842359484\n",
      "Iteration 1298, the loss is 3804.292686043218, parameters k is -78.14317379441599 and b is 510.40537447102565\n",
      "Iteration 1299, the loss is 3803.1588028789242, parameters k is -78.13017972326975 and b is 510.32632308762646\n",
      "Iteration 1300, the loss is 3802.050597458778, parameters k is -78.11718565212351 and b is 510.2433191350573\n",
      "Iteration 1301, the loss is 3800.942562014128, parameters k is -78.10419158097727 and b is 510.1603151824881\n",
      "Iteration 1302, the loss is 3799.834696544984, parameters k is -78.09119750983103 and b is 510.0773112299189\n",
      "Iteration 1303, the loss is 3798.7270010513357, parameters k is -78.0782034386848 and b is 509.9943072773497\n",
      "Iteration 1304, the loss is 3797.6194755331903, parameters k is -78.06520936753856 and b is 509.91130332478053\n",
      "Iteration 1305, the loss is 3796.4865692638414, parameters k is -78.05221529639232 and b is 509.83225194138134\n",
      "Iteration 1306, the loss is 3795.379373096171, parameters k is -78.03922122524608 and b is 509.74924798881216\n",
      "Iteration 1307, the loss is 3794.272346904003, parameters k is -78.02622715409984 and b is 509.666244036243\n",
      "Iteration 1308, the loss is 3793.1654906873364, parameters k is -78.0132330829536 and b is 509.5832400836738\n",
      "Iteration 1309, the loss is 3792.058804446165, parameters k is -78.00023901180737 and b is 509.5002361311046\n",
      "Iteration 1310, the loss is 3790.9522881805005, parameters k is -77.98724494066113 and b is 509.4172321785354\n",
      "Iteration 1311, the loss is 3789.8459418903335, parameters k is -77.97425086951489 and b is 509.3342282259662\n",
      "Iteration 1312, the loss is 3788.714171890908, parameters k is -77.96125679836865 and b is 509.25517684256704\n",
      "Iteration 1313, the loss is 3787.608154951214, parameters k is -77.94826272722241 and b is 509.17217288999785\n",
      "Iteration 1314, the loss is 3786.50230798702, parameters k is -77.93526865607618 and b is 509.08916893742867\n",
      "Iteration 1315, the loss is 3785.3966309983357, parameters k is -77.92227458492994 and b is 509.0061649848595\n",
      "Iteration 1316, the loss is 3784.291123985147, parameters k is -77.9092805137837 and b is 508.9231610322903\n",
      "Iteration 1317, the loss is 3783.1857869474584, parameters k is -77.89628644263746 and b is 508.8401570797211\n",
      "Iteration 1318, the loss is 3782.054993842973, parameters k is -77.88329237149122 and b is 508.7611056963219\n",
      "Iteration 1319, the loss is 3780.9499861557633, parameters k is -77.87029830034498 and b is 508.67810174375273\n",
      "Iteration 1320, the loss is 3779.8451484440493, parameters k is -77.85730422919875 and b is 508.59509779118355\n",
      "Iteration 1321, the loss is 3778.7404807078374, parameters k is -77.84431015805251 and b is 508.51209383861436\n",
      "Iteration 1322, the loss is 3777.6359829471285, parameters k is -77.83131608690627 and b is 508.4290898860452\n",
      "Iteration 1323, the loss is 3776.5316551619194, parameters k is -77.81832201576003 and b is 508.346085933476\n",
      "Iteration 1324, the loss is 3775.427497352207, parameters k is -77.8053279446138 and b is 508.2630819809068\n",
      "Iteration 1325, the loss is 3774.297840517646, parameters k is -77.79233387346756 and b is 508.1840305975076\n",
      "Iteration 1326, the loss is 3773.194012058412, parameters k is -77.77933980232132 and b is 508.10102664493843\n",
      "Iteration 1327, the loss is 3772.09035357468, parameters k is -77.76634573117508 and b is 508.01802269236924\n",
      "Iteration 1328, the loss is 3770.986865066445, parameters k is -77.75335166002884 and b is 507.93501873980006\n",
      "Iteration 1329, the loss is 3769.8835465337224, parameters k is -77.7403575888826 and b is 507.85201478723087\n",
      "Iteration 1330, the loss is 3768.7803979764894, parameters k is -77.72736351773636 and b is 507.7690108346617\n",
      "Iteration 1331, the loss is 3767.6517180368664, parameters k is -77.71436944659013 and b is 507.6899594512625\n",
      "Iteration 1332, the loss is 3766.5488988301145, parameters k is -77.70137537544389 and b is 507.6069554986933\n",
      "Iteration 1333, the loss is 3765.4462495988596, parameters k is -77.68838130429765 and b is 507.5239515461241\n",
      "Iteration 1334, the loss is 3764.343770343106, parameters k is -77.67538723315141 and b is 507.44094759355494\n",
      "Iteration 1335, the loss is 3763.215748548474, parameters k is -77.66239316200517 and b is 507.36189621015575\n",
      "Iteration 1336, the loss is 3762.1135986432014, parameters k is -77.64939909085894 and b is 507.27889225758656\n",
      "Iteration 1337, the loss is 3760.9859162436096, parameters k is -77.6364050197127 and b is 507.1998408741874\n",
      "Iteration 1338, the loss is 3759.8840956888116, parameters k is -77.62341094856646 and b is 507.1168369216182\n",
      "Iteration 1339, the loss is 3758.756752684255, parameters k is -77.61041687742022 and b is 507.037785538219\n",
      "Iteration 1340, the loss is 3757.6295896997576, parameters k is -77.59742280627398 and b is 506.9587341548198\n",
      "Iteration 1341, the loss is 3756.5282578704127, parameters k is -77.58442873512774 and b is 506.8757302022506\n",
      "Iteration 1342, the loss is 3755.401434280953, parameters k is -77.5714346639815 and b is 506.79667881885143\n",
      "Iteration 1343, the loss is 3754.3004318020803, parameters k is -77.55844059283527 and b is 506.71367486628225\n",
      "Iteration 1344, the loss is 3753.173947607661, parameters k is -77.54544652168903 and b is 506.63462348288306\n",
      "Iteration 1345, the loss is 3752.0476434332977, parameters k is -77.53245245054279 and b is 506.55557209948387\n",
      "Iteration 1346, the loss is 3750.9471296798847, parameters k is -77.51945837939655 and b is 506.4725681469147\n",
      "Iteration 1347, the loss is 3749.821164900556, parameters k is -77.50646430825032 and b is 506.3935167635155\n",
      "Iteration 1348, the loss is 3748.7209804976183, parameters k is -77.49347023710408 and b is 506.3105128109463\n",
      "Iteration 1349, the loss is 3747.595355113329, parameters k is -77.48047616595784 and b is 506.2314614275471\n",
      "Iteration 1350, the loss is 3746.495500060872, parameters k is -77.4674820948116 and b is 506.14845747497793\n",
      "Iteration 1351, the loss is 3745.3702140716186, parameters k is -77.45448802366536 and b is 506.06940609157874\n",
      "Iteration 1352, the loss is 3744.245108102423, parameters k is -77.44149395251912 and b is 505.99035470817955\n",
      "Iteration 1353, the loss is 3743.1457417754164, parameters k is -77.42849988137289 and b is 505.90735075561037\n",
      "Iteration 1354, the loss is 3742.02097520126, parameters k is -77.41550581022665 and b is 505.8282993722112\n",
      "Iteration 1355, the loss is 3740.921938224732, parameters k is -77.40251173908041 and b is 505.745295419642\n",
      "Iteration 1356, the loss is 3739.7975110456096, parameters k is -77.38951766793417 and b is 505.6662440362428\n",
      "Iteration 1357, the loss is 3738.6732638865487, parameters k is -77.37652359678793 and b is 505.5871926528436\n",
      "Iteration 1358, the loss is 3737.574715635474, parameters k is -77.3635295256417 and b is 505.5041887002744\n",
      "Iteration 1359, the loss is 3736.450807871452, parameters k is -77.35053545449546 and b is 505.42513731687524\n",
      "Iteration 1360, the loss is 3735.3525889708526, parameters k is -77.33754138334922 and b is 505.34213336430605\n",
      "Iteration 1361, the loss is 3734.2290206018597, parameters k is -77.32454731220298 and b is 505.26308198090686\n",
      "Iteration 1362, the loss is 3733.1311310517412, parameters k is -77.31155324105674 and b is 505.1800780283377\n",
      "Iteration 1363, the loss is 3732.0079020777916, parameters k is -77.2985591699105 and b is 505.1010266449385\n",
      "Iteration 1364, the loss is 3730.884853123897, parameters k is -77.28556509876427 and b is 505.0219752615393\n",
      "Iteration 1365, the loss is 3729.7874522992347, parameters k is -77.27257102761803 and b is 504.9389713089701\n",
      "Iteration 1366, the loss is 3728.664742740374, parameters k is -77.25957695647179 and b is 504.8599199255709\n",
      "Iteration 1367, the loss is 3727.56767126619, parameters k is -77.24658288532555 and b is 504.77691597300173\n",
      "Iteration 1368, the loss is 3726.4453011023697, parameters k is -77.23358881417931 and b is 504.69786458960255\n",
      "Iteration 1369, the loss is 3725.348558978657, parameters k is -77.22059474303308 and b is 504.61486063703336\n",
      "Iteration 1370, the loss is 3724.226528209877, parameters k is -77.20760067188684 and b is 504.53580925363417\n",
      "Iteration 1371, the loss is 3723.1046774611495, parameters k is -77.1946066007406 and b is 504.456757870235\n",
      "Iteration 1372, the loss is 3722.0084240628944, parameters k is -77.18161252959436 and b is 504.3737539176658\n",
      "Iteration 1373, the loss is 3720.8869127092103, parameters k is -77.16861845844812 and b is 504.2947025342666\n",
      "Iteration 1374, the loss is 3719.7909886614266, parameters k is -77.15562438730188 and b is 504.2116985816974\n",
      "Iteration 1375, the loss is 3718.6698167027726, parameters k is -77.14263031615565 and b is 504.13264719829823\n",
      "Iteration 1376, the loss is 3717.54882476418, parameters k is -77.12963624500941 and b is 504.05359581489904\n",
      "Iteration 1377, the loss is 3716.453389441862, parameters k is -77.11664217386317 and b is 503.97059186232985\n",
      "Iteration 1378, the loss is 3715.332736898301, parameters k is -77.10364810271693 and b is 503.89154047893066\n",
      "Iteration 1379, the loss is 3714.237630926459, parameters k is -77.0906540315707 and b is 503.8085365263615\n",
      "Iteration 1380, the loss is 3713.1173177779433, parameters k is -77.07765996042446 and b is 503.7294851429623\n",
      "Iteration 1381, the loss is 3712.0225411565707, parameters k is -77.06466588927822 and b is 503.6464811903931\n",
      "Iteration 1382, the loss is 3710.9025674030845, parameters k is -77.05167181813198 and b is 503.5674298069939\n",
      "Iteration 1383, the loss is 3709.7827736696618, parameters k is -77.03867774698574 and b is 503.4883784235947\n",
      "Iteration 1384, the loss is 3708.6884857737496, parameters k is -77.0256836758395 and b is 503.40537447102554\n",
      "Iteration 1385, the loss is 3707.5690314353596, parameters k is -77.01268960469326 and b is 503.32632308762635\n",
      "Iteration 1386, the loss is 3706.4750728899216, parameters k is -76.99969553354703 and b is 503.24331913505716\n",
      "Iteration 1387, the loss is 3705.3559579465723, parameters k is -76.98670146240079 and b is 503.164267751658\n",
      "Iteration 1388, the loss is 3704.2370230232827, parameters k is -76.97370739125455 and b is 503.0852163682588\n",
      "Iteration 1389, the loss is 3703.143553203296, parameters k is -76.96071332010831 and b is 503.0022124156896\n",
      "Iteration 1390, the loss is 3702.0249576750443, parameters k is -76.94771924896207 and b is 502.9231610322904\n",
      "Iteration 1391, the loss is 3700.931817205537, parameters k is -76.93472517781584 and b is 502.8401570797212\n",
      "Iteration 1392, the loss is 3699.8135610723157, parameters k is -76.9217311066696 and b is 502.76110569632203\n",
      "Iteration 1393, the loss is 3698.7207499532888, parameters k is -76.90873703552336 and b is 502.67810174375285\n",
      "Iteration 1394, the loss is 3697.6028332151104, parameters k is -76.89574296437712 and b is 502.59905036035366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1395, the loss is 3696.4850964969905, parameters k is -76.88274889323088 and b is 502.51999897695447\n",
      "Iteration 1396, the loss is 3695.3927741034136, parameters k is -76.86975482208464 and b is 502.4369950243853\n",
      "Iteration 1397, the loss is 3694.275376780326, parameters k is -76.8567607509384 and b is 502.3579436409861\n",
      "Iteration 1398, the loss is 3693.18338373723, parameters k is -76.84376667979217 and b is 502.2749396884169\n",
      "Iteration 1399, the loss is 3692.0663258091818, parameters k is -76.83077260864593 and b is 502.1958883050177\n",
      "Iteration 1400, the loss is 3690.949447901191, parameters k is -76.81777853749969 and b is 502.1168369216185\n",
      "Iteration 1401, the loss is 3689.8579435835463, parameters k is -76.80478446635345 and b is 502.03383296904934\n",
      "Iteration 1402, the loss is 3688.7414050705956, parameters k is -76.79179039520722 and b is 501.95478158565015\n",
      "Iteration 1403, the loss is 3687.65023010343, parameters k is -76.77879632406098 and b is 501.87177763308097\n",
      "Iteration 1404, the loss is 3686.534030985511, parameters k is -76.76580225291474 and b is 501.7927262496818\n",
      "Iteration 1405, the loss is 3685.443185368819, parameters k is -76.7528081817685 and b is 501.7097222971126\n",
      "Iteration 1406, the loss is 3684.3273256459393, parameters k is -76.73981411062226 and b is 501.6306709137134\n",
      "Iteration 1407, the loss is 3683.2116459431277, parameters k is -76.72682003947602 and b is 501.5516195303142\n",
      "Iteration 1408, the loss is 3682.1212890518864, parameters k is -76.71382596832979 and b is 501.468615577745\n",
      "Iteration 1409, the loss is 3681.0059487441017, parameters k is -76.70083189718355 and b is 501.38956419434584\n",
      "Iteration 1410, the loss is 3679.9159212033487, parameters k is -76.68783782603731 and b is 501.30656024177665\n",
      "Iteration 1411, the loss is 3678.800920290597, parameters k is -76.67484375489107 and b is 501.22750885837746\n",
      "Iteration 1412, the loss is 3677.711222100319, parameters k is -76.66184968374483 and b is 501.1445049058083\n",
      "Iteration 1413, the loss is 3676.5965605826077, parameters k is -76.6488556125986 and b is 501.0654535224091\n",
      "Iteration 1414, the loss is 3675.4820790849603, parameters k is -76.63586154145236 and b is 500.9864021390099\n",
      "Iteration 1415, the loss is 3674.417992878498, parameters k is -76.62286747030612 and b is 500.89944561727077\n",
      "Iteration 1416, the loss is 3673.3038301307993, parameters k is -76.60987339915988 and b is 500.8203942338716\n",
      "Iteration 1417, the loss is 3672.2149606169696, parameters k is -76.59687932801364 and b is 500.7373902813024\n",
      "Iteration 1418, the loss is 3671.126261078646, parameters k is -76.5838852568674 and b is 500.6543863287332\n",
      "Iteration 1419, the loss is 3670.012597100962, parameters k is -76.57089118572117 and b is 500.575334945334\n",
      "Iteration 1420, the loss is 3668.9242269131123, parameters k is -76.55789711457493 and b is 500.49233099276483\n",
      "Iteration 1421, the loss is 3667.836026700764, parameters k is -76.54490304342869 and b is 500.40932704019565\n",
      "Iteration 1422, the loss is 3666.747996463916, parameters k is -76.53190897228245 and b is 500.32632308762646\n",
      "Iteration 1423, the loss is 3665.6601362025717, parameters k is -76.51891490113621 and b is 500.2433191350573\n",
      "Iteration 1424, the loss is 3664.572445916719, parameters k is -76.50592082998998 and b is 500.1603151824881\n",
      "Iteration 1425, the loss is 3663.4849256063703, parameters k is -76.49292675884374 and b is 500.0773112299189\n",
      "Iteration 1426, the loss is 3662.3975752715237, parameters k is -76.4799326876975 and b is 499.9943072773497\n",
      "Iteration 1427, the loss is 3661.285206938742, parameters k is -76.46693861655126 and b is 499.91525589395053\n",
      "Iteration 1428, the loss is 3660.198185954365, parameters k is -76.45394454540502 and b is 499.83225194138134\n",
      "Iteration 1429, the loss is 3659.1113349454954, parameters k is -76.44095047425878 and b is 499.74924798881216\n",
      "Iteration 1430, the loss is 3658.02465391213, parameters k is -76.42795640311255 and b is 499.666244036243\n",
      "Iteration 1431, the loss is 3656.9381428542606, parameters k is -76.41496233196631 and b is 499.5832400836738\n",
      "Iteration 1432, the loss is 3655.8518017718898, parameters k is -76.40196826082007 and b is 499.5002361311046\n",
      "Iteration 1433, the loss is 3654.7656306650197, parameters k is -76.38897418967383 and b is 499.4172321785354\n",
      "Iteration 1434, the loss is 3653.679629533652, parameters k is -76.3759801185276 and b is 499.3342282259662\n",
      "Iteration 1435, the loss is 3652.56855684577, parameters k is -76.36298604738136 and b is 499.25517684256704\n",
      "Iteration 1436, the loss is 3651.4828850648764, parameters k is -76.34999197623512 and b is 499.17217288999785\n",
      "Iteration 1437, the loss is 3650.39738325948, parameters k is -76.33699790508888 and b is 499.08916893742867\n",
      "Iteration 1438, the loss is 3649.3120514295906, parameters k is -76.32400383394264 and b is 499.0061649848595\n",
      "Iteration 1439, the loss is 3648.226889575202, parameters k is -76.3110097627964 and b is 498.9231610322903\n",
      "Iteration 1440, the loss is 3647.1418976963155, parameters k is -76.29801569165016 and b is 498.8401570797211\n",
      "Iteration 1441, the loss is 3646.0570757929268, parameters k is -76.28502162050393 and b is 498.7571531271519\n",
      "Iteration 1442, the loss is 3644.947139374954, parameters k is -76.27202754935769 and b is 498.67810174375273\n",
      "Iteration 1443, the loss is 3643.8626468220423, parameters k is -76.25903347821145 and b is 498.59509779118355\n",
      "Iteration 1444, the loss is 3642.778324244628, parameters k is -76.24603940706521 and b is 498.51209383861436\n",
      "Iteration 1445, the loss is 3641.694171642717, parameters k is -76.23304533591897 and b is 498.4290898860452\n",
      "Iteration 1446, the loss is 3640.61018901631, parameters k is -76.22005126477274 and b is 498.346085933476\n",
      "Iteration 1447, the loss is 3639.5263763654007, parameters k is -76.2070571936265 and b is 498.2630819809068\n",
      "Iteration 1448, the loss is 3638.442733689989, parameters k is -76.19406312248026 and b is 498.1800780283376\n",
      "Iteration 1449, the loss is 3637.3592609900747, parameters k is -76.18106905133402 and b is 498.09707407576843\n",
      "Iteration 1450, the loss is 3636.2506202170084, parameters k is -76.16807498018778 and b is 498.01802269236924\n",
      "Iteration 1451, the loss is 3635.1674768675803, parameters k is -76.15508090904154 and b is 497.93501873980006\n",
      "Iteration 1452, the loss is 3634.084503493641, parameters k is -76.1420868378953 and b is 497.85201478723087\n",
      "Iteration 1453, the loss is 3633.001700095211, parameters k is -76.12909276674907 and b is 497.7690108346617\n",
      "Iteration 1454, the loss is 3631.919066672279, parameters k is -76.11609869560283 and b is 497.6860068820925\n",
      "Iteration 1455, the loss is 3630.836603224841, parameters k is -76.10310462445659 and b is 497.6030029295233\n",
      "Iteration 1456, the loss is 3629.754309752915, parameters k is -76.09011055331035 and b is 497.5199989769541\n",
      "Iteration 1457, the loss is 3628.646805249766, parameters k is -76.07711648216412 and b is 497.44094759355494\n",
      "Iteration 1458, the loss is 3627.5648411283114, parameters k is -76.06412241101788 and b is 497.35794364098575\n",
      "Iteration 1459, the loss is 3626.4830469823573, parameters k is -76.05112833987164 and b is 497.27493968841657\n",
      "Iteration 1460, the loss is 3625.401422811905, parameters k is -76.0381342687254 and b is 497.1919357358474\n",
      "Iteration 1461, the loss is 3624.3199686169564, parameters k is -76.02514019757916 and b is 497.1089317832782\n",
      "Iteration 1462, the loss is 3623.2386843974964, parameters k is -76.01214612643292 and b is 497.025927830709\n",
      "Iteration 1463, the loss is 3622.1575701535476, parameters k is -75.99915205528669 and b is 496.9429238781398\n",
      "Iteration 1464, the loss is 3621.0766258850904, parameters k is -75.98615798414045 and b is 496.85991992557064\n",
      "Iteration 1465, the loss is 3619.970417026846, parameters k is -75.97316391299421 and b is 496.78086854217145\n",
      "Iteration 1466, the loss is 3618.8898021088676, parameters k is -75.96016984184797 and b is 496.69786458960226\n",
      "Iteration 1467, the loss is 3617.809357166397, parameters k is -75.94717577070173 and b is 496.6148606370331\n",
      "Iteration 1468, the loss is 3616.7290821994156, parameters k is -75.9341816995555 and b is 496.5318566844639\n",
      "Iteration 1469, the loss is 3615.648977207946, parameters k is -75.92118762840926 and b is 496.4488527318947\n",
      "Iteration 1470, the loss is 3614.5690421919703, parameters k is -75.90819355726302 and b is 496.3658487793255\n",
      "Iteration 1471, the loss is 3613.4892771514933, parameters k is -75.89519948611678 and b is 496.28284482675633\n",
      "Iteration 1472, the loss is 3612.409682086523, parameters k is -75.88220541497054 and b is 496.19984087418715\n",
      "Iteration 1473, the loss is 3611.304768873169, parameters k is -75.8692113438243 and b is 496.12078949078796\n",
      "Iteration 1474, the loss is 3610.22550315868, parameters k is -75.85621727267807 and b is 496.03778553821877\n",
      "Iteration 1475, the loss is 3609.1464074196833, parameters k is -75.84322320153183 and b is 495.9547815856496\n",
      "Iteration 1476, the loss is 3608.0674816561823, parameters k is -75.83022913038559 and b is 495.8717776330804\n",
      "Iteration 1477, the loss is 3606.988725868192, parameters k is -75.81723505923935 and b is 495.7887736805112\n",
      "Iteration 1478, the loss is 3605.9101400556965, parameters k is -75.80424098809311 and b is 495.705769727942\n",
      "Iteration 1479, the loss is 3604.8317242187, parameters k is -75.79124691694687 and b is 495.62276577537284\n",
      "Iteration 1480, the loss is 3603.727947275273, parameters k is -75.77825284580064 and b is 495.54371439197365\n",
      "Iteration 1481, the loss is 3602.6498607887524, parameters k is -75.7652587746544 and b is 495.46071043940447\n",
      "Iteration 1482, the loss is 3601.571944277736, parameters k is -75.75226470350816 and b is 495.3777064868353\n",
      "Iteration 1483, the loss is 3600.4941977422163, parameters k is -75.73927063236192 and b is 495.2947025342661\n",
      "Iteration 1484, the loss is 3599.416621182205, parameters k is -75.72627656121568 and b is 495.2116985816969\n",
      "Iteration 1485, the loss is 3598.3392145976877, parameters k is -75.71328249006945 and b is 495.1286946291277\n",
      "Iteration 1486, the loss is 3597.261977988672, parameters k is -75.70028841892321 and b is 495.04569067655854\n",
      "Iteration 1487, the loss is 3596.184911355158, parameters k is -75.68729434777697 and b is 494.96268672398935\n",
      "Iteration 1488, the loss is 3595.0824300566223, parameters k is -75.67430027663073 and b is 494.88363534059016\n",
      "Iteration 1489, the loss is 3594.005692773583, parameters k is -75.6613062054845 and b is 494.800631388021\n",
      "Iteration 1490, the loss is 3592.9291254660484, parameters k is -75.64831213433825 and b is 494.7176274354518\n",
      "Iteration 1491, the loss is 3591.85272813401, parameters k is -75.63531806319202 and b is 494.6346234828826\n",
      "Iteration 1492, the loss is 3590.776500777473, parameters k is -75.62232399204578 and b is 494.5516195303134\n",
      "Iteration 1493, the loss is 3589.700443396437, parameters k is -75.60932992089954 and b is 494.46861557774423\n",
      "Iteration 1494, the loss is 3588.6245559909016, parameters k is -75.5963358497533 and b is 494.38561162517504\n",
      "Iteration 1495, the loss is 3587.5488385608637, parameters k is -75.58334177860706 and b is 494.30260767260586\n",
      "Iteration 1496, the loss is 3586.4476529072285, parameters k is -75.57034770746083 and b is 494.22355628920667\n",
      "Iteration 1497, the loss is 3585.3722648276726, parameters k is -75.55735363631459 and b is 494.1405523366375\n",
      "Iteration 1498, the loss is 3584.2970467236128, parameters k is -75.54435956516835 and b is 494.0575483840683\n",
      "Iteration 1499, the loss is 3583.2219985950596, parameters k is -75.53136549402211 and b is 493.9745444314991\n",
      "Iteration 1500, the loss is 3582.1471204419972, parameters k is -75.51837142287587 and b is 493.8915404789299\n",
      "Iteration 1501, the loss is 3581.072412264438, parameters k is -75.50537735172963 and b is 493.80853652636074\n",
      "Iteration 1502, the loss is 3579.997874062382, parameters k is -75.4923832805834 and b is 493.72553257379155\n",
      "Iteration 1503, the loss is 3578.897824678664, parameters k is -75.47938920943716 and b is 493.64648119039236\n",
      "Iteration 1504, the loss is 3577.823615827086, parameters k is -75.46639513829092 and b is 493.5634772378232\n",
      "Iteration 1505, the loss is 3576.7495769510087, parameters k is -75.45340106714468 and b is 493.480473285254\n",
      "Iteration 1506, the loss is 3575.675708050424, parameters k is -75.44040699599844 and b is 493.3974693326848\n",
      "Iteration 1507, the loss is 3574.6020091253486, parameters k is -75.4274129248522 and b is 493.3144653801156\n",
      "Iteration 1508, the loss is 3573.528480175768, parameters k is -75.41441885370597 and b is 493.23146142754644\n",
      "Iteration 1509, the loss is 3572.455121201694, parameters k is -75.40142478255973 and b is 493.14845747497725\n",
      "Iteration 1510, the loss is 3571.3819322031104, parameters k is -75.38843071141349 and b is 493.06545352240806\n",
      "Iteration 1511, the loss is 3570.2831784642985, parameters k is -75.37543664026725 and b is 492.9864021390089\n",
      "Iteration 1512, the loss is 3569.210318816196, parameters k is -75.36244256912101 and b is 492.9033981864397\n",
      "Iteration 1513, the loss is 3568.1376291435977, parameters k is -75.34944849797478 and b is 492.8203942338705\n",
      "Iteration 1514, the loss is 3567.065109446498, parameters k is -75.33645442682854 and b is 492.7373902813013\n",
      "Iteration 1515, the loss is 3565.992759724897, parameters k is -75.3234603556823 and b is 492.65438632873213\n",
      "Iteration 1516, the loss is 3564.9205799787983, parameters k is -75.31046628453606 and b is 492.57138237616294\n",
      "Iteration 1517, the loss is 3563.8485702081985, parameters k is -75.29747221338982 and b is 492.48837842359376\n",
      "Iteration 1518, the loss is 3562.7767304130994, parameters k is -75.28447814224359 and b is 492.4053744710246\n",
      "Iteration 1519, the loss is 3561.6792723191847, parameters k is -75.27148407109735 and b is 492.3263230876254\n",
      "Iteration 1520, the loss is 3560.6077618745617, parameters k is -75.25848999995111 and b is 492.2433191350562\n",
      "Iteration 1521, the loss is 3559.53642140544, parameters k is -75.24549592880487 and b is 492.160315182487\n",
      "Iteration 1522, the loss is 3558.4652509118177, parameters k is -75.23250185765863 and b is 492.0773112299178\n",
      "Iteration 1523, the loss is 3557.394250393699, parameters k is -75.2195077865124 and b is 491.99430727734864\n",
      "Iteration 1524, the loss is 3556.323419851076, parameters k is -75.20651371536616 and b is 491.91130332477945\n",
      "Iteration 1525, the loss is 3555.252759283958, parameters k is -75.19351964421992 and b is 491.82829937221027\n",
      "Iteration 1526, the loss is 3554.1564374599598, parameters k is -75.18052557307368 and b is 491.7492479888111\n",
      "Iteration 1527, the loss is 3553.0861062433155, parameters k is -75.16753150192744 and b is 491.6662440362419\n",
      "Iteration 1528, the loss is 3552.0159450021756, parameters k is -75.1545374307812 and b is 491.5832400836727\n",
      "Iteration 1529, the loss is 3550.9459537365306, parameters k is -75.14154335963497 and b is 491.5002361311035\n",
      "Iteration 1530, the loss is 3549.87613244639, parameters k is -75.12854928848873 and b is 491.41723217853433\n",
      "Iteration 1531, the loss is 3548.806481131748, parameters k is -75.11555521734249 and b is 491.33422822596515\n",
      "Iteration 1532, the loss is 3547.736999792608, parameters k is -75.10256114619625 and b is 491.25122427339596\n",
      "Iteration 1533, the loss is 3546.66768842896, parameters k is -75.08956707505001 and b is 491.1682203208268\n",
      "Iteration 1534, the loss is 3545.5726622498655, parameters k is -75.07657300390377 and b is 491.0891689374276\n",
      "Iteration 1535, the loss is 3544.5036802367035, parameters k is -75.06357893275754 and b is 491.0061649848584\n",
      "Iteration 1536, the loss is 3543.43486819904, parameters k is -75.0505848616113 and b is 490.9231610322892\n",
      "Iteration 1537, the loss is 3542.3662261368777, parameters k is -75.03759079046506 and b is 490.84015707972003\n",
      "Iteration 1538, the loss is 3541.2718581027693, parameters k is -75.02459671931882 and b is 490.76110569632084\n",
      "Iteration 1539, the loss is 3540.177670088721, parameters k is -75.01160264817258 and b is 490.68205431292165\n",
      "Iteration 1540, the loss is 3539.0836620947325, parameters k is -74.99860857702635 and b is 490.60300292952246\n",
      "Iteration 1541, the loss is 3537.9898341208013, parameters k is -74.98561450588011 and b is 490.52395154612327\n",
      "Iteration 1542, the loss is 3536.896186166934, parameters k is -74.97262043473387 and b is 490.4449001627241\n",
      "Iteration 1543, the loss is 3535.802718233123, parameters k is -74.95962636358763 and b is 490.3658487793249\n",
      "Iteration 1544, the loss is 3534.709430319371, parameters k is -74.9466322924414 and b is 490.2867973959257\n",
      "Iteration 1545, the loss is 3533.616322425682, parameters k is -74.93363822129515 and b is 490.2077460125265\n",
      "Iteration 1546, the loss is 3532.5233945520436, parameters k is -74.92064415014892 and b is 490.1286946291273\n",
      "Iteration 1547, the loss is 3531.4306466984767, parameters k is -74.90765007900268 and b is 490.04964324572813\n",
      "Iteration 1548, the loss is 3530.3380788649642, parameters k is -74.89465600785644 and b is 489.97059186232894\n",
      "Iteration 1549, the loss is 3529.2456910515134, parameters k is -74.8816619367102 and b is 489.89154047892976\n",
      "Iteration 1550, the loss is 3528.1534832581215, parameters k is -74.86866786556396 and b is 489.81248909553057\n",
      "Iteration 1551, the loss is 3527.0614554847884, parameters k is -74.85567379441773 and b is 489.7334377121314\n",
      "Iteration 1552, the loss is 3525.9696077315134, parameters k is -74.84267972327149 and b is 489.6543863287322\n",
      "Iteration 1553, the loss is 3524.8779399982973, parameters k is -74.82968565212525 and b is 489.575334945333\n",
      "Iteration 1554, the loss is 3523.7864522851405, parameters k is -74.81669158097901 and b is 489.4962835619338\n",
      "Iteration 1555, the loss is 3522.695144592047, parameters k is -74.80369750983277 and b is 489.4172321785346\n",
      "Iteration 1556, the loss is 3521.604016919012, parameters k is -74.79070343868653 and b is 489.33818079513543\n",
      "Iteration 1557, the loss is 3520.5130692660346, parameters k is -74.7777093675403 and b is 489.25912941173624\n",
      "Iteration 1558, the loss is 3519.422301633121, parameters k is -74.76471529639406 and b is 489.18007802833705\n",
      "Iteration 1559, the loss is 3518.331714020262, parameters k is -74.75172122524782 and b is 489.10102664493786\n",
      "Iteration 1560, the loss is 3517.241306427464, parameters k is -74.73872715410158 and b is 489.02197526153867\n",
      "Iteration 1561, the loss is 3516.1510788547266, parameters k is -74.72573308295534 and b is 488.9429238781395\n",
      "Iteration 1562, the loss is 3515.0610313020484, parameters k is -74.7127390118091 and b is 488.8638724947403\n",
      "Iteration 1563, the loss is 3513.971163769425, parameters k is -74.69974494066287 and b is 488.7848211113411\n",
      "Iteration 1564, the loss is 3512.8814762568677, parameters k is -74.68675086951663 and b is 488.7057697279419\n",
      "Iteration 1565, the loss is 3511.791968764372, parameters k is -74.67375679837039 and b is 488.6267183445427\n",
      "Iteration 1566, the loss is 3510.70264129193, parameters k is -74.66076272722415 and b is 488.54766696114353\n",
      "Iteration 1567, the loss is 3509.6134938395544, parameters k is -74.64776865607791 and b is 488.46861557774434\n",
      "Iteration 1568, the loss is 3508.5245264072328, parameters k is -74.63477458493168 and b is 488.38956419434516\n",
      "Iteration 1569, the loss is 3507.435738994972, parameters k is -74.62178051378544 and b is 488.31051281094597\n",
      "Iteration 1570, the loss is 3506.3471316027685, parameters k is -74.6087864426392 and b is 488.2314614275468\n",
      "Iteration 1571, the loss is 3505.2587042306286, parameters k is -74.59579237149296 and b is 488.1524100441476\n",
      "Iteration 1572, the loss is 3504.1704568785435, parameters k is -74.58279830034672 and b is 488.0733586607484\n",
      "Iteration 1573, the loss is 3503.08238954652, parameters k is -74.56980422920049 and b is 487.9943072773492\n",
      "Iteration 1574, the loss is 3501.9945022345582, parameters k is -74.55681015805425 and b is 487.91525589395\n",
      "Iteration 1575, the loss is 3500.9067949426576, parameters k is -74.54381608690801 and b is 487.83620451055083\n",
      "Iteration 1576, the loss is 3499.8192676708113, parameters k is -74.53082201576177 and b is 487.75715312715164\n",
      "Iteration 1577, the loss is 3498.731920419028, parameters k is -74.51782794461553 and b is 487.67810174375245\n",
      "Iteration 1578, the loss is 3497.6447531872973, parameters k is -74.5048338734693 and b is 487.59905036035326\n",
      "Iteration 1579, the loss is 3496.5577659756314, parameters k is -74.49183980232306 and b is 487.51999897695407\n",
      "Iteration 1580, the loss is 3495.4709587840302, parameters k is -74.47884573117682 and b is 487.4409475935549\n",
      "Iteration 1581, the loss is 3494.384331612491, parameters k is -74.46585166003058 and b is 487.3618962101557\n",
      "Iteration 1582, the loss is 3493.2978844609966, parameters k is -74.45285758888434 and b is 487.2828448267565\n",
      "Iteration 1583, the loss is 3492.2116173295717, parameters k is -74.4398635177381 and b is 487.2037934433573\n",
      "Iteration 1584, the loss is 3491.125530218202, parameters k is -74.42686944659187 and b is 487.1247420599581\n",
      "Iteration 1585, the loss is 3490.039623126892, parameters k is -74.41387537544563 and b is 487.04569067655893\n",
      "Iteration 1586, the loss is 3488.953896055649, parameters k is -74.40088130429939 and b is 486.96663929315974\n",
      "Iteration 1587, the loss is 3487.8683490044577, parameters k is -74.38788723315315 and b is 486.88758790976055\n",
      "Iteration 1588, the loss is 3486.78298197333, parameters k is -74.37489316200691 and b is 486.80853652636137\n",
      "Iteration 1589, the loss is 3485.6977949622587, parameters k is -74.36189909086067 and b is 486.7294851429622\n",
      "Iteration 1590, the loss is 3484.61278797125, parameters k is -74.34890501971444 and b is 486.650433759563\n",
      "Iteration 1591, the loss is 3483.5279610003013, parameters k is -74.3359109485682 and b is 486.5713823761638\n",
      "Iteration 1592, the loss is 3482.443314049411, parameters k is -74.32291687742196 and b is 486.4923309927646\n",
      "Iteration 1593, the loss is 3481.358847118581, parameters k is -74.30992280627572 and b is 486.4132796093654\n",
      "Iteration 1594, the loss is 3480.2745602078107, parameters k is -74.29692873512948 and b is 486.3342282259662\n",
      "Iteration 1595, the loss is 3479.190453317092, parameters k is -74.28393466398325 and b is 486.25517684256704\n",
      "Iteration 1596, the loss is 3478.106526446439, parameters k is -74.27094059283701 and b is 486.17612545916785\n",
      "Iteration 1597, the loss is 3477.0227795958494, parameters k is -74.25794652169077 and b is 486.09707407576866\n",
      "Iteration 1598, the loss is 3475.9392127653164, parameters k is -74.24495245054453 and b is 486.01802269236947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1599, the loss is 3474.8558259548417, parameters k is -74.23195837939829 and b is 485.9389713089703\n",
      "Iteration 1600, the loss is 3473.772619164429, parameters k is -74.21896430825205 and b is 485.8599199255711\n",
      "Iteration 1601, the loss is 3472.6895923940738, parameters k is -74.20597023710582 and b is 485.7808685421719\n",
      "Iteration 1602, the loss is 3471.606745643782, parameters k is -74.19297616595958 and b is 485.7018171587727\n",
      "Iteration 1603, the loss is 3470.5240789135446, parameters k is -74.17998209481334 and b is 485.6227657753735\n",
      "Iteration 1604, the loss is 3469.441592203366, parameters k is -74.1669880236671 and b is 485.54371439197433\n",
      "Iteration 1605, the loss is 3468.3592855132565, parameters k is -74.15399395252086 and b is 485.46466300857514\n",
      "Iteration 1606, the loss is 3467.277158843195, parameters k is -74.14099988137463 and b is 485.38561162517595\n",
      "Iteration 1607, the loss is 3466.1952121932004, parameters k is -74.12800581022839 and b is 485.30656024177676\n",
      "Iteration 1608, the loss is 3465.113445563261, parameters k is -74.11501173908215 and b is 485.2275088583776\n",
      "Iteration 1609, the loss is 3464.031858953379, parameters k is -74.10201766793591 and b is 485.1484574749784\n",
      "Iteration 1610, the loss is 3462.9504523635665, parameters k is -74.08902359678967 and b is 485.0694060915792\n",
      "Iteration 1611, the loss is 3461.8692257938033, parameters k is -74.07602952564343 and b is 484.99035470818\n",
      "Iteration 1612, the loss is 3460.7881792441044, parameters k is -74.0630354544972 and b is 484.9113033247808\n",
      "Iteration 1613, the loss is 3459.707312714467, parameters k is -74.05004138335096 and b is 484.8322519413816\n",
      "Iteration 1614, the loss is 3458.62662620489, parameters k is -74.03704731220472 and b is 484.75320055798244\n",
      "Iteration 1615, the loss is 3457.546119715367, parameters k is -74.02405324105848 and b is 484.67414917458325\n",
      "Iteration 1616, the loss is 3456.465793245904, parameters k is -74.01105916991224 and b is 484.59509779118406\n",
      "Iteration 1617, the loss is 3455.38564679651, parameters k is -73.998065098766 and b is 484.51604640778487\n",
      "Iteration 1618, the loss is 3454.305680367165, parameters k is -73.98507102761977 and b is 484.4369950243857\n",
      "Iteration 1619, the loss is 3453.2258939578755, parameters k is -73.97207695647353 and b is 484.3579436409865\n",
      "Iteration 1620, the loss is 3452.1462875686566, parameters k is -73.95908288532729 and b is 484.2788922575873\n",
      "Iteration 1621, the loss is 3451.0668611994947, parameters k is -73.94608881418105 and b is 484.1998408741881\n",
      "Iteration 1622, the loss is 3449.987614850391, parameters k is -73.93309474303481 and b is 484.1207894907889\n",
      "Iteration 1623, the loss is 3448.908548521351, parameters k is -73.92010067188858 and b is 484.04173810738973\n",
      "Iteration 1624, the loss is 3447.8296622123653, parameters k is -73.90710660074234 and b is 483.96268672399054\n",
      "Iteration 1625, the loss is 3446.7509559234422, parameters k is -73.8941125295961 and b is 483.88363534059135\n",
      "Iteration 1626, the loss is 3445.6724296545794, parameters k is -73.88111845844986 and b is 483.80458395719216\n",
      "Iteration 1627, the loss is 3444.594083405773, parameters k is -73.86812438730362 and b is 483.725532573793\n",
      "Iteration 1628, the loss is 3443.5159171770265, parameters k is -73.85513031615739 and b is 483.6464811903938\n",
      "Iteration 1629, the loss is 3442.437930968335, parameters k is -73.84213624501115 and b is 483.5674298069946\n",
      "Iteration 1630, the loss is 3441.360124779708, parameters k is -73.82914217386491 and b is 483.4883784235954\n",
      "Iteration 1631, the loss is 3440.282498611145, parameters k is -73.81614810271867 and b is 483.4093270401962\n",
      "Iteration 1632, the loss is 3439.2050524626407, parameters k is -73.80315403157243 and b is 483.330275656797\n",
      "Iteration 1633, the loss is 3438.1277863341884, parameters k is -73.7901599604262 and b is 483.25122427339784\n",
      "Iteration 1634, the loss is 3437.050700225803, parameters k is -73.77716588927996 and b is 483.17217288999865\n",
      "Iteration 1635, the loss is 3435.973794137472, parameters k is -73.76417181813372 and b is 483.09312150659946\n",
      "Iteration 1636, the loss is 3434.8970680692037, parameters k is -73.75117774698748 and b is 483.01407012320027\n",
      "Iteration 1637, the loss is 3433.820522020995, parameters k is -73.73818367584124 and b is 482.9350187398011\n",
      "Iteration 1638, the loss is 3432.7441559928457, parameters k is -73.725189604695 and b is 482.8559673564019\n",
      "Iteration 1639, the loss is 3431.6679699847546, parameters k is -73.71219553354877 and b is 482.7769159730027\n",
      "Iteration 1640, the loss is 3430.591963996723, parameters k is -73.69920146240253 and b is 482.6978645896035\n",
      "Iteration 1641, the loss is 3429.516138028751, parameters k is -73.68620739125629 and b is 482.6188132062043\n",
      "Iteration 1642, the loss is 3428.440492080837, parameters k is -73.67321332011005 and b is 482.53976182280513\n",
      "Iteration 1643, the loss is 3427.365026152987, parameters k is -73.66021924896381 and b is 482.46071043940594\n",
      "Iteration 1644, the loss is 3426.2897402451977, parameters k is -73.64722517781757 and b is 482.38165905600675\n",
      "Iteration 1645, the loss is 3425.214634357461, parameters k is -73.63423110667134 and b is 482.30260767260756\n",
      "Iteration 1646, the loss is 3424.1397084897903, parameters k is -73.6212370355251 and b is 482.2235562892084\n",
      "Iteration 1647, the loss is 3423.0649626421723, parameters k is -73.60824296437886 and b is 482.1445049058092\n",
      "Iteration 1648, the loss is 3421.9903968146255, parameters k is -73.59524889323262 and b is 482.06545352241\n",
      "Iteration 1649, the loss is 3420.9160110071293, parameters k is -73.58225482208638 and b is 481.9864021390108\n",
      "Iteration 1650, the loss is 3419.84180521969, parameters k is -73.56926075094015 and b is 481.9073507556116\n",
      "Iteration 1651, the loss is 3418.7677794523147, parameters k is -73.55626667979391 and b is 481.8282993722124\n",
      "Iteration 1652, the loss is 3417.6939337050007, parameters k is -73.54327260864767 and b is 481.74924798881324\n",
      "Iteration 1653, the loss is 3416.620267977747, parameters k is -73.53027853750143 and b is 481.67019660541405\n",
      "Iteration 1654, the loss is 3415.546782270551, parameters k is -73.51728446635519 and b is 481.59114522201486\n",
      "Iteration 1655, the loss is 3414.4734765834132, parameters k is -73.50429039520895 and b is 481.51209383861567\n",
      "Iteration 1656, the loss is 3413.4003509163394, parameters k is -73.49129632406272 and b is 481.4330424552165\n",
      "Iteration 1657, the loss is 3412.3274052693164, parameters k is -73.47830225291648 and b is 481.3539910718173\n",
      "Iteration 1658, the loss is 3411.254639642357, parameters k is -73.46530818177024 and b is 481.2749396884181\n",
      "Iteration 1659, the loss is 3410.1820540354597, parameters k is -73.452314110624 and b is 481.1958883050189\n",
      "Iteration 1660, the loss is 3409.109648448619, parameters k is -73.43932003947776 and b is 481.1168369216197\n",
      "Iteration 1661, the loss is 3408.0374228818414, parameters k is -73.42632596833153 and b is 481.03778553822053\n",
      "Iteration 1662, the loss is 3406.96537733512, parameters k is -73.41333189718529 and b is 480.95873415482134\n",
      "Iteration 1663, the loss is 3405.8935118084623, parameters k is -73.40033782603905 and b is 480.87968277142215\n",
      "Iteration 1664, the loss is 3404.8218263018607, parameters k is -73.38734375489281 and b is 480.80063138802296\n",
      "Iteration 1665, the loss is 3403.7735948372606, parameters k is -73.37434968374657 and b is 480.7176274354538\n",
      "Iteration 1666, the loss is 3402.725533348171, parameters k is -73.36135561260033 and b is 480.6346234828846\n",
      "Iteration 1667, the loss is 3401.654346611581, parameters k is -73.3483615414541 and b is 480.5555720994854\n",
      "Iteration 1668, the loss is 3400.6066144729652, parameters k is -73.33536747030786 and b is 480.4725681469162\n",
      "Iteration 1669, the loss is 3399.535767131415, parameters k is -73.32237339916162 and b is 480.393516763517\n",
      "Iteration 1670, the loss is 3398.488364343271, parameters k is -73.30937932801538 and b is 480.31051281094784\n",
      "Iteration 1671, the loss is 3397.441131530635, parameters k is -73.29638525686914 and b is 480.22750885837866\n",
      "Iteration 1672, the loss is 3396.3707829590894, parameters k is -73.2833911857229 and b is 480.14845747497947\n",
      "Iteration 1673, the loss is 3395.323879496931, parameters k is -73.27039711457667 and b is 480.0654535224103\n",
      "Iteration 1674, the loss is 3394.2771460102676, parameters k is -73.25740304343043 and b is 479.9824495698411\n",
      "Iteration 1675, the loss is 3393.2072962087427, parameters k is -73.24440897228419 and b is 479.9033981864419\n",
      "Iteration 1676, the loss is 3392.1608920725566, parameters k is -73.23141490113795 and b is 479.8203942338727\n",
      "Iteration 1677, the loss is 3391.1146579118717, parameters k is -73.21842082999171 and b is 479.73739028130353\n",
      "Iteration 1678, the loss is 3390.045306880362, parameters k is -73.20542675884548 and b is 479.65833889790434\n",
      "Iteration 1679, the loss is 3388.999402070155, parameters k is -73.19243268769924 and b is 479.57533494533516\n",
      "Iteration 1680, the loss is 3387.953667235446, parameters k is -73.179438616553 and b is 479.492330992766\n",
      "Iteration 1681, the loss is 3386.8848149739506, parameters k is -73.16644454540676 and b is 479.4132796093668\n",
      "Iteration 1682, the loss is 3385.8394094897176, parameters k is -73.15345047426052 and b is 479.3302756567976\n",
      "Iteration 1683, the loss is 3384.7708966232567, parameters k is -73.14045640311429 and b is 479.2512242733984\n",
      "Iteration 1684, the loss is 3383.7258204895015, parameters k is -73.12746233196805 and b is 479.1682203208292\n",
      "Iteration 1685, the loss is 3382.680914331252, parameters k is -73.11446826082181 and b is 479.08521636826003\n",
      "Iteration 1686, the loss is 3381.6129002348034, parameters k is -73.10147418967557 and b is 479.00616498486085\n",
      "Iteration 1687, the loss is 3380.5683234270264, parameters k is -73.08848011852933 and b is 478.92316103229166\n",
      "Iteration 1688, the loss is 3379.523916594755, parameters k is -73.0754860473831 and b is 478.8401570797225\n",
      "Iteration 1689, the loss is 3378.456401268319, parameters k is -73.06249197623686 and b is 478.7611056963233\n",
      "Iteration 1690, the loss is 3377.4123237865247, parameters k is -73.04949790509062 and b is 478.6781017437541\n",
      "Iteration 1691, the loss is 3376.3684162802224, parameters k is -73.03650383394438 and b is 478.5950977911849\n",
      "Iteration 1692, the loss is 3375.301399723799, parameters k is -73.02350976279814 and b is 478.5160464077857\n",
      "Iteration 1693, the loss is 3374.257821567984, parameters k is -73.0105156916519 and b is 478.43304245521654\n",
      "Iteration 1694, the loss is 3373.214413387665, parameters k is -72.99752162050567 and b is 478.35003850264735\n",
      "Iteration 1695, the loss is 3372.147895601255, parameters k is -72.98452754935943 and b is 478.27098711924816\n",
      "Iteration 1696, the loss is 3371.1048167714143, parameters k is -72.97153347821319 and b is 478.187983166679\n",
      "Iteration 1697, the loss is 3370.0386383800437, parameters k is -72.95853940706695 and b is 478.1089317832798\n",
      "Iteration 1698, the loss is 3368.9958889006725, parameters k is -72.94554533592071 and b is 478.0259278307106\n",
      "Iteration 1699, the loss is 3367.9533093968103, parameters k is -72.93255126477447 and b is 477.9429238781414\n",
      "Iteration 1700, the loss is 3366.8876297754523, parameters k is -72.91955719362824 and b is 477.8638724947422\n",
      "Iteration 1701, the loss is 3365.8453796220633, parameters k is -72.906563122482 and b is 477.78086854217304\n",
      "Iteration 1702, the loss is 3364.8032994441783, parameters k is -72.89356905133576 and b is 477.69786458960385\n",
      "Iteration 1703, the loss is 3363.7381185928357, parameters k is -72.88057498018952 and b is 477.61881320620466\n",
      "Iteration 1704, the loss is 3362.6963677654226, parameters k is -72.86758090904328 and b is 477.5358092536355\n",
      "Iteration 1705, the loss is 3361.65478691351, parameters k is -72.85458683789705 and b is 477.4528053010663\n",
      "Iteration 1706, the loss is 3360.5901048321825, parameters k is -72.84159276675081 and b is 477.3737539176671\n",
      "Iteration 1707, the loss is 3359.5488533307475, parameters k is -72.82859869560457 and b is 477.2907499650979\n",
      "Iteration 1708, the loss is 3358.4845106444563, parameters k is -72.81560462445833 and b is 477.2116985816987\n",
      "Iteration 1709, the loss is 3357.443588493499, parameters k is -72.80261055331209 and b is 477.12869462912954\n",
      "Iteration 1710, the loss is 3356.4028363180432, parameters k is -72.78961648216585 and b is 477.04569067656035\n",
      "Iteration 1711, the loss is 3355.338992401761, parameters k is -72.77662241101962 and b is 476.96663929316117\n",
      "Iteration 1712, the loss is 3354.298569576783, parameters k is -72.76362833987338 and b is 476.883635340592\n",
      "Iteration 1713, the loss is 3353.2583167273056, parameters k is -72.75063426872714 and b is 476.8006313880228\n",
      "Iteration 1714, the loss is 3352.194971581037, parameters k is -72.7376401975809 and b is 476.7215800046236\n",
      "Iteration 1715, the loss is 3351.155048082036, parameters k is -72.72464612643466 and b is 476.6385760520544\n",
      "Iteration 1716, the loss is 3350.11529455854, parameters k is -72.71165205528843 and b is 476.55557209948523\n",
      "Iteration 1717, the loss is 3349.0524481822854, parameters k is -72.69865798414219 and b is 476.47652071608604\n",
      "Iteration 1718, the loss is 3348.013024009262, parameters k is -72.68566391299595 and b is 476.39351676351686\n",
      "Iteration 1719, the loss is 3346.973769811736, parameters k is -72.67266984184971 and b is 476.31051281094767\n",
      "Iteration 1720, the loss is 3345.9114222054977, parameters k is -72.65967577070347 and b is 476.2314614275485\n",
      "Iteration 1721, the loss is 3344.872497358455, parameters k is -72.64668169955723 and b is 476.1484574749793\n",
      "Iteration 1722, the loss is 3343.8104891472435, parameters k is -72.633687628411 and b is 476.0694060915801\n",
      "Iteration 1723, the loss is 3342.771893650681, parameters k is -72.62069355726476 and b is 475.9864021390109\n",
      "Iteration 1724, the loss is 3341.733468129612, parameters k is -72.60769948611852 and b is 475.90339818644173\n",
      "Iteration 1725, the loss is 3340.6719586884196, parameters k is -72.59470541497228 and b is 475.82434680304254\n",
      "Iteration 1726, the loss is 3339.6338625178278, parameters k is -72.58171134382604 and b is 475.74134285047336\n",
      "Iteration 1727, the loss is 3338.5959363227394, parameters k is -72.5687172726798 and b is 475.6583388979042\n",
      "Iteration 1728, the loss is 3337.534925651561, parameters k is -72.55572320153357 and b is 475.579287514505\n",
      "Iteration 1729, the loss is 3336.4973288069464, parameters k is -72.54272913038733 and b is 475.4962835619358\n",
      "Iteration 1730, the loss is 3335.4599019378334, parameters k is -72.52973505924109 and b is 475.4132796093666\n",
      "Iteration 1731, the loss is 3334.3993900366704, parameters k is -72.51674098809485 and b is 475.3342282259674\n",
      "Iteration 1732, the loss is 3333.362292518031, parameters k is -72.50374691694861 and b is 475.25122427339824\n",
      "Iteration 1733, the loss is 3332.3253649748995, parameters k is -72.49075284580238 and b is 475.16822032082905\n",
      "Iteration 1734, the loss is 3331.2653518437482, parameters k is -72.47775877465614 and b is 475.08916893742986\n",
      "Iteration 1735, the loss is 3330.228753651084, parameters k is -72.4647647035099 and b is 475.0061649848607\n",
      "Iteration 1736, the loss is 3329.1690799149787, parameters k is -72.45177063236366 and b is 474.9271136014615\n",
      "Iteration 1737, the loss is 3328.1328110727954, parameters k is -72.43877656121742 and b is 474.8441096488923\n",
      "Iteration 1738, the loss is 3327.096712206117, parameters k is -72.42578249007119 and b is 474.7611056963231\n",
      "Iteration 1739, the loss is 3326.037537240014, parameters k is -72.41278841892495 and b is 474.6820543129239\n",
      "Iteration 1740, the loss is 3325.0017677238116, parameters k is -72.39979434777871 and b is 474.59905036035474\n",
      "Iteration 1741, the loss is 3323.9661681831085, parameters k is -72.38680027663247 and b is 474.51604640778555\n",
      "Iteration 1742, the loss is 3322.9074919870213, parameters k is -72.37380620548623 and b is 474.43699502438636\n",
      "Iteration 1743, the loss is 3321.8722217967975, parameters k is -72.36081213434 and b is 474.3539910718172\n",
      "Iteration 1744, the loss is 3320.837121582071, parameters k is -72.34781806319376 and b is 474.270987119248\n",
      "Iteration 1745, the loss is 3319.778944155993, parameters k is -72.33482399204752 and b is 474.1919357358488\n",
      "Iteration 1746, the loss is 3318.744173291744, parameters k is -72.32182992090128 and b is 474.1089317832796\n",
      "Iteration 1747, the loss is 3317.709572402996, parameters k is -72.30883584975504 and b is 474.02592783071043\n",
      "Iteration 1748, the loss is 3316.6518937469395, parameters k is -72.2958417786088 and b is 473.94687644731124\n",
      "Iteration 1749, the loss is 3315.617622208668, parameters k is -72.28284770746257 and b is 473.86387249474205\n",
      "Iteration 1750, the loss is 3314.560282947643, parameters k is -72.26985363631633 and b is 473.78482111134286\n",
      "Iteration 1751, the loss is 3313.526340759851, parameters k is -72.25685956517009 and b is 473.7018171587737\n",
      "Iteration 1752, the loss is 3312.4925685475587, parameters k is -72.24386549402385 and b is 473.6188132062045\n",
      "Iteration 1753, the loss is 3311.435728056545, parameters k is -72.23087142287761 and b is 473.5397618228053\n",
      "Iteration 1754, the loss is 3310.4022851947284, parameters k is -72.21787735173137 and b is 473.4567578702361\n",
      "Iteration 1755, the loss is 3309.3690123084184, parameters k is -72.20488328058514 and b is 473.37375391766693\n",
      "Iteration 1756, the loss is 3308.3126705874192, parameters k is -72.1918892094389 and b is 473.29470253426774\n",
      "Iteration 1757, the loss is 3307.279727051582, parameters k is -72.17889513829266 and b is 473.21169858169856\n",
      "Iteration 1758, the loss is 3306.2469534912434, parameters k is -72.16590106714642 and b is 473.12869462912937\n",
      "Iteration 1759, the loss is 3305.1911105402605, parameters k is -72.15290699600018 and b is 473.0496432457302\n",
      "Iteration 1760, the loss is 3304.1586663304024, parameters k is -72.13991292485395 and b is 472.966639293161\n",
      "Iteration 1761, the loss is 3303.126392096037, parameters k is -72.1269188537077 and b is 472.8836353405918\n",
      "Iteration 1762, the loss is 3302.0710479150657, parameters k is -72.11392478256147 and b is 472.8045839571926\n",
      "Iteration 1763, the loss is 3301.0391030311835, parameters k is -72.10093071141523 and b is 472.72158000462343\n",
      "Iteration 1764, the loss is 3299.984098245249, parameters k is -72.08793664026899 and b is 472.64252862122424\n",
      "Iteration 1765, the loss is 3298.952482711844, parameters k is -72.07494256912275 and b is 472.55952466865506\n",
      "Iteration 1766, the loss is 3297.92103715394, parameters k is -72.06194849797652 and b is 472.4765207160859\n",
      "Iteration 1767, the loss is 3296.8665311380137, parameters k is -72.04895442683028 and b is 472.3974693326867\n",
      "Iteration 1768, the loss is 3295.835414930589, parameters k is -72.03596035568404 and b is 472.3144653801175\n",
      "Iteration 1769, the loss is 3294.804468698662, parameters k is -72.0229662845378 and b is 472.2314614275483\n",
      "Iteration 1770, the loss is 3293.7504614527547, parameters k is -72.00997221339156 and b is 472.1524100441491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1771, the loss is 3292.7198445713047, parameters k is -71.99697814224533 and b is 472.06940609157994\n",
      "Iteration 1772, the loss is 3291.689397665356, parameters k is -71.98398407109909 and b is 471.98640213901075\n",
      "Iteration 1773, the loss is 3290.635889189458, parameters k is -71.97098999995285 and b is 471.90735075561156\n",
      "Iteration 1774, the loss is 3289.605771633984, parameters k is -71.95799592880661 and b is 471.8243468030424\n",
      "Iteration 1775, the loss is 3288.5758240540113, parameters k is -71.94500185766037 and b is 471.7413428504732\n",
      "Iteration 1776, the loss is 3287.522814348134, parameters k is -71.93200778651413 and b is 471.662291467074\n",
      "Iteration 1777, the loss is 3286.4931961186376, parameters k is -71.9190137153679 and b is 471.5792875145048\n",
      "Iteration 1778, the loss is 3285.4405258077913, parameters k is -71.90601964422166 and b is 471.5002361311056\n",
      "Iteration 1779, the loss is 3284.4112369287727, parameters k is -71.89302557307542 and b is 471.41723217853644\n",
      "Iteration 1780, the loss is 3283.382118025254, parameters k is -71.88003150192918 and b is 471.33422822596725\n",
      "Iteration 1781, the loss is 3282.3299464844235, parameters k is -71.86703743078294 and b is 471.25517684256806\n",
      "Iteration 1782, the loss is 3281.301156931384, parameters k is -71.8540433596367 and b is 471.1721728899989\n",
      "Iteration 1783, the loss is 3280.2725373538397, parameters k is -71.84104928849047 and b is 471.0891689374297\n",
      "Iteration 1784, the loss is 3279.2208645830256, parameters k is -71.82805521734423 and b is 471.0101175540305\n",
      "Iteration 1785, the loss is 3278.1925743559623, parameters k is -71.81506114619799 and b is 470.9271136014613\n",
      "Iteration 1786, the loss is 3277.1644541044, parameters k is -71.80206707505175 and b is 470.84410964889213\n",
      "Iteration 1787, the loss is 3276.1132801035965, parameters k is -71.78907300390551 and b is 470.76505826549294\n",
      "Iteration 1788, the loss is 3275.085489202509, parameters k is -71.77607893275928 and b is 470.68205431292375\n",
      "Iteration 1789, the loss is 3274.0578682769246, parameters k is -71.76308486161304 and b is 470.59905036035457\n",
      "Iteration 1790, the loss is 3273.0071930461277, parameters k is -71.7500907904668 and b is 470.5199989769554\n",
      "Iteration 1791, the loss is 3271.9799014710225, parameters k is -71.73709671932056 and b is 470.4369950243862\n",
      "Iteration 1792, the loss is 3270.92956563527, parameters k is -71.72410264817432 and b is 470.357943640987\n",
      "Iteration 1793, the loss is 3269.9026034106437, parameters k is -71.71110857702809 and b is 470.2749396884178\n",
      "Iteration 1794, the loss is 3268.875811161512, parameters k is -71.69811450588185 and b is 470.19193573584863\n",
      "Iteration 1795, the loss is 3267.8259740957674, parameters k is -71.68512043473561 and b is 470.11288435244944\n",
      "Iteration 1796, the loss is 3266.799511197115, parameters k is -71.67212636358937 and b is 470.02988039988026\n",
      "Iteration 1797, the loss is 3265.773218273962, parameters k is -71.65913229244313 and b is 469.94687644731107\n",
      "Iteration 1798, the loss is 3264.7238799782376, parameters k is -71.6461382212969 and b is 469.8678250639119\n",
      "Iteration 1799, the loss is 3263.697916405558, parameters k is -71.63314415015066 and b is 469.7848211113427\n",
      "Iteration 1800, the loss is 3262.672122808384, parameters k is -71.62015007900442 and b is 469.7018171587735\n",
      "Iteration 1801, the loss is 3261.6232832826695, parameters k is -71.60715600785818 and b is 469.6227657753743\n",
      "Iteration 1802, the loss is 3260.597819035971, parameters k is -71.59416193671194 and b is 469.53976182280513\n",
      "Iteration 1803, the loss is 3259.5725247647756, parameters k is -71.5811678655657 and b is 469.45675787023595\n",
      "Iteration 1804, the loss is 3258.5241840090725, parameters k is -71.56817379441947 and b is 469.37770648683676\n",
      "Iteration 1805, the loss is 3257.499219088351, parameters k is -71.55517972327323 and b is 469.29470253426757\n",
      "Iteration 1806, the loss is 3256.4512177276915, parameters k is -71.54218565212699 and b is 469.2156511508684\n",
      "Iteration 1807, the loss is 3255.4265821574427, parameters k is -71.52919158098075 and b is 469.1326471982992\n",
      "Iteration 1808, the loss is 3254.4021165627028, parameters k is -71.51619750983451 and b is 469.04964324573\n",
      "Iteration 1809, the loss is 3253.354613972052, parameters k is -71.50320343868827 and b is 468.9705918623308\n",
      "Iteration 1810, the loss is 3252.330477727788, parameters k is -71.49020936754204 and b is 468.88758790976163\n",
      "Iteration 1811, the loss is 3251.30651145902, parameters k is -71.4772152963958 and b is 468.80458395719245\n",
      "Iteration 1812, the loss is 3250.2595076383814, parameters k is -71.46422122524956 and b is 468.72553257379326\n",
      "Iteration 1813, the loss is 3249.235870720092, parameters k is -71.45122715410332 and b is 468.6425286212241\n",
      "Iteration 1814, the loss is 3248.2124037773037, parameters k is -71.43823308295708 and b is 468.5595246686549\n",
      "Iteration 1815, the loss is 3247.165898726682, parameters k is -71.42523901181085 and b is 468.4804732852557\n",
      "Iteration 1816, the loss is 3246.142761134375, parameters k is -71.4122449406646 and b is 468.3974693326865\n",
      "Iteration 1817, the loss is 3245.0965954787844, parameters k is -71.39925086951837 and b is 468.3184179492873\n",
      "Iteration 1818, the loss is 3244.0737872369527, parameters k is -71.38625679837213 and b is 468.23541399671814\n",
      "Iteration 1819, the loss is 3243.051148970616, parameters k is -71.37326272722589 and b is 468.15241004414895\n",
      "Iteration 1820, the loss is 3242.005482085043, parameters k is -71.36026865607965 and b is 468.07335866074976\n",
      "Iteration 1821, the loss is 3240.9831731691875, parameters k is -71.34727458493342 and b is 467.9903547081806\n",
      "Iteration 1822, the loss is 3239.9610342288324, parameters k is -71.33428051378718 and b is 467.9073507556114\n",
      "Iteration 1823, the loss is 3238.915866113273, parameters k is -71.32128644264094 and b is 467.8282993722122\n",
      "Iteration 1824, the loss is 3237.8940565233897, parameters k is -71.3082923714947 and b is 467.745295419643\n",
      "Iteration 1825, the loss is 3236.8724169090133, parameters k is -71.29529830034846 and b is 467.6622914670738\n",
      "Iteration 1826, the loss is 3235.827747563464, parameters k is -71.28230422920223 and b is 467.58324008367464\n",
      "Iteration 1827, the loss is 3234.806437299566, parameters k is -71.26931015805599 and b is 467.50023613110545\n",
      "Iteration 1828, the loss is 3233.7852970111658, parameters k is -71.25631608690975 and b is 467.41723217853627\n",
      "Iteration 1829, the loss is 3232.7411264356374, parameters k is -71.24332201576351 and b is 467.3381807951371\n",
      "Iteration 1830, the loss is 3231.720315497708, parameters k is -71.23032794461727 and b is 467.2551768425679\n",
      "Iteration 1831, the loss is 3230.676484317209, parameters k is -71.21733387347103 and b is 467.1761254591687\n",
      "Iteration 1832, the loss is 3229.656002729764, parameters k is -71.2043398023248 and b is 467.0931215065995\n",
      "Iteration 1833, the loss is 3228.6356911178223, parameters k is -71.19134573117856 and b is 467.01011755403033\n",
      "Iteration 1834, the loss is 3227.592358707333, parameters k is -71.17835166003232 and b is 466.93106617063114\n",
      "Iteration 1835, the loss is 3226.5723764458676, parameters k is -71.16535758888608 and b is 466.84806221806195\n",
      "Iteration 1836, the loss is 3225.552564159894, parameters k is -71.15236351773984 and b is 466.76505826549277\n",
      "Iteration 1837, the loss is 3224.509730519427, parameters k is -71.1393694465936 and b is 466.6860068820936\n",
      "Iteration 1838, the loss is 3223.4902475839344, parameters k is -71.12637537544737 and b is 466.6030029295244\n",
      "Iteration 1839, the loss is 3222.4709346239474, parameters k is -71.11338130430113 and b is 466.5199989769552\n",
      "Iteration 1840, the loss is 3221.428599753484, parameters k is -71.10038723315489 and b is 466.440947593556\n",
      "Iteration 1841, the loss is 3220.409616143974, parameters k is -71.08739316200865 and b is 466.35794364098683\n",
      "Iteration 1842, the loss is 3219.3908025099604, parameters k is -71.07439909086241 and b is 466.27493968841765\n",
      "Iteration 1843, the loss is 3218.3489664095205, parameters k is -71.06140501971618 and b is 466.19588830501846\n",
      "Iteration 1844, the loss is 3217.330482125987, parameters k is -71.04841094856994 and b is 466.11288435244927\n",
      "Iteration 1845, the loss is 3216.2889854205764, parameters k is -71.0354168774237 and b is 466.0338329690501\n",
      "Iteration 1846, the loss is 3215.2708304875177, parameters k is -71.02242280627746 and b is 465.9508290164809\n",
      "Iteration 1847, the loss is 3214.2528455299553, parameters k is -71.00942873513122 and b is 465.8678250639117\n",
      "Iteration 1848, the loss is 3213.211847594563, parameters k is -70.99643466398499 and b is 465.7887736805125\n",
      "Iteration 1849, the loss is 3212.1941919874826, parameters k is -70.98344059283875 and b is 465.70576972794333\n",
      "Iteration 1850, the loss is 3211.1767063559, parameters k is -70.97044652169251 and b is 465.62276577537415\n",
      "Iteration 1851, the loss is 3210.1362071905255, parameters k is -70.95745245054627 and b is 465.54371439197496\n",
      "Iteration 1852, the loss is 3209.1190509094167, parameters k is -70.94445837940003 and b is 465.4607104394058\n",
      "Iteration 1853, the loss is 3208.102064603812, parameters k is -70.9314643082538 and b is 465.3777064868366\n",
      "Iteration 1854, the loss is 3207.0620642084477, parameters k is -70.91847023710756 and b is 465.2986551034374\n",
      "Iteration 1855, the loss is 3206.0454072533203, parameters k is -70.90547616596132 and b is 465.2156511508682\n",
      "Iteration 1856, the loss is 3205.0289202736935, parameters k is -70.89248209481508 and b is 465.132647198299\n",
      "Iteration 1857, the loss is 3203.98941864834, parameters k is -70.87948802366884 and b is 465.05359581489984\n",
      "Iteration 1858, the loss is 3202.9732610191945, parameters k is -70.8664939525226 and b is 464.97059186233065\n",
      "Iteration 1859, the loss is 3201.9340987888786, parameters k is -70.85349988137637 and b is 464.89154047893146\n",
      "Iteration 1860, the loss is 3200.9182705102044, parameters k is -70.84050581023013 and b is 464.8085365263623\n",
      "Iteration 1861, the loss is 3199.902612207037, parameters k is -70.82751173908389 and b is 464.7255325737931\n",
      "Iteration 1862, the loss is 3198.863948746733, parameters k is -70.81451766793765 and b is 464.6464811903939\n",
      "Iteration 1863, the loss is 3197.848619794038, parameters k is -70.80152359679141 and b is 464.5634772378247\n",
      "Iteration 1864, the loss is 3196.833460816845, parameters k is -70.78852952564517 and b is 464.4804732852555\n",
      "Iteration 1865, the loss is 3195.7952961265555, parameters k is -70.77553545449894 and b is 464.40142190185634\n",
      "Iteration 1866, the loss is 3194.7804664998375, parameters k is -70.7625413833527 and b is 464.31841794928715\n",
      "Iteration 1867, the loss is 3193.7658068486203, parameters k is -70.74954731220646 and b is 464.23541399671797\n",
      "Iteration 1868, the loss is 3192.7281409283423, parameters k is -70.73655324106022 and b is 464.1563626133188\n",
      "Iteration 1869, the loss is 3191.713810627607, parameters k is -70.72355916991398 and b is 464.0733586607496\n",
      "Iteration 1870, the loss is 3190.6996503023665, parameters k is -70.71056509876774 and b is 463.9903547081804\n",
      "Iteration 1871, the loss is 3189.6624831521053, parameters k is -70.6975710276215 and b is 463.9113033247812\n",
      "Iteration 1872, the loss is 3188.648652177345, parameters k is -70.68457695647527 and b is 463.82829937221203\n",
      "Iteration 1873, the loss is 3187.6118244221166, parameters k is -70.67158288532903 and b is 463.74924798881284\n",
      "Iteration 1874, the loss is 3186.598322797834, parameters k is -70.65858881418279 and b is 463.66624403624365\n",
      "Iteration 1875, the loss is 3185.5849911490495, parameters k is -70.64559474303655 and b is 463.58324008367447\n",
      "Iteration 1876, the loss is 3184.548662163838, parameters k is -70.63260067189032 and b is 463.5041887002753\n",
      "Iteration 1877, the loss is 3183.535659865531, parameters k is -70.61960660074408 and b is 463.4211847477061\n",
      "Iteration 1878, the loss is 3182.5228275427226, parameters k is -70.60661252959784 and b is 463.3381807951369\n",
      "Iteration 1879, the loss is 3181.4869973275227, parameters k is -70.5936184584516 and b is 463.2591294117377\n",
      "Iteration 1880, the loss is 3180.4744943551927, parameters k is -70.58062438730536 and b is 463.17612545916853\n",
      "Iteration 1881, the loss is 3179.4621613583654, parameters k is -70.56763031615912 and b is 463.09312150659935\n",
      "Iteration 1882, the loss is 3178.426829913181, parameters k is -70.55463624501289 and b is 463.01407012320016\n",
      "Iteration 1883, the loss is 3177.414826266825, parameters k is -70.54164217386665 and b is 462.93106617063097\n",
      "Iteration 1884, the loss is 3176.402992595977, parameters k is -70.52864810272041 and b is 462.8480622180618\n",
      "Iteration 1885, the loss is 3175.3681599208044, parameters k is -70.51565403157417 and b is 462.7690108346626\n",
      "Iteration 1886, the loss is 3174.356655600427, parameters k is -70.50265996042793 and b is 462.6860068820934\n",
      "Iteration 1887, the loss is 3173.322162320292, parameters k is -70.4896658892817 and b is 462.6069554986942\n",
      "Iteration 1888, the loss is 3172.310987350399, parameters k is -70.47667181813546 and b is 462.52395154612503\n",
      "Iteration 1889, the loss is 3171.299982356, parameters k is -70.46367774698922 and b is 462.44094759355585\n",
      "Iteration 1890, the loss is 3170.2659878458803, parameters k is -70.45068367584298 and b is 462.36189621015666\n",
      "Iteration 1891, the loss is 3169.2553122019563, parameters k is -70.43768960469674 and b is 462.2788922575875\n",
      "Iteration 1892, the loss is 3168.2448065335393, parameters k is -70.4246955335505 and b is 462.1958883050183\n",
      "Iteration 1893, the loss is 3167.211310793431, parameters k is -70.41170146240427 and b is 462.1168369216191\n",
      "Iteration 1894, the loss is 3166.201134475487, parameters k is -70.39870739125803 and b is 462.0338329690499\n",
      "Iteration 1895, the loss is 3165.191128133049, parameters k is -70.38571332011179 and b is 461.9508290164807\n",
      "Iteration 1896, the loss is 3164.158131162946, parameters k is -70.37271924896555 and b is 461.87177763308154\n",
      "Iteration 1897, the loss is 3163.1484541709856, parameters k is -70.35972517781931 and b is 461.78877368051235\n",
      "Iteration 1898, the loss is 3162.1389471545235, parameters k is -70.34673110667308 and b is 461.70576972794316\n",
      "Iteration 1899, the loss is 3161.1064489544383, parameters k is -70.33373703552684 and b is 461.626718344544\n",
      "Iteration 1900, the loss is 3160.097271288457, parameters k is -70.3207429643806 and b is 461.5437143919748\n",
      "Iteration 1901, the loss is 3159.065112483407, parameters k is -70.30774889323436 and b is 461.4646630085756\n",
      "Iteration 1902, the loss is 3158.056264167898, parameters k is -70.29475482208812 and b is 461.3816590560064\n",
      "Iteration 1903, the loss is 3157.047585827887, parameters k is -70.28176075094188 and b is 461.2986551034372\n",
      "Iteration 1904, the loss is 3156.0159257928567, parameters k is -70.26876667979565 and b is 461.21960372003804\n",
      "Iteration 1905, the loss is 3155.007576803324, parameters k is -70.25577260864941 and b is 461.13659976746885\n",
      "Iteration 1906, the loss is 3153.999397789292, parameters k is -70.24277853750317 and b is 461.05359581489967\n",
      "Iteration 1907, the loss is 3152.96823652427, parameters k is -70.22978446635693 and b is 460.9745444315005\n",
      "Iteration 1908, the loss is 3151.9603868607173, parameters k is -70.2167903952107 and b is 460.8915404789313\n",
      "Iteration 1909, the loss is 3150.952707172665, parameters k is -70.20379632406446 and b is 460.8085365263621\n",
      "Iteration 1910, the loss is 3149.9220446776553, parameters k is -70.19080225291822 and b is 460.7294851429629\n",
      "Iteration 1911, the loss is 3148.9146943400806, parameters k is -70.17780818177198 and b is 460.64648119039373\n",
      "Iteration 1912, the loss is 3147.90751397801, parameters k is -70.16481411062574 and b is 460.56347723782454\n",
      "Iteration 1913, the loss is 3146.877350253014, parameters k is -70.1518200394795 and b is 460.48442585442535\n",
      "Iteration 1914, the loss is 3145.870499241413, parameters k is -70.13882596833326 and b is 460.40142190185617\n",
      "Iteration 1915, the loss is 3144.8406749114597, parameters k is -70.12583189718703 and b is 460.322370518457\n",
      "Iteration 1916, the loss is 3143.8341532503377, parameters k is -70.11283782604079 and b is 460.2393665658878\n",
      "Iteration 1917, the loss is 3142.827801564718, parameters k is -70.09984375489455 and b is 460.1563626133186\n",
      "Iteration 1918, the loss is 3141.7984760047693, parameters k is -70.08684968374831 and b is 460.0773112299194\n",
      "Iteration 1919, the loss is 3140.792453669628, parameters k is -70.07385561260207 and b is 459.99430727735023\n",
      "Iteration 1920, the loss is 3139.786601309983, parameters k is -70.06086154145584 and b is 459.91130332478104\n",
      "Iteration 1921, the loss is 3138.757774520058, parameters k is -70.0478674703096 and b is 459.83225194138186\n",
      "Iteration 1922, the loss is 3137.7522515108903, parameters k is -70.03487339916336 and b is 459.74924798881267\n",
      "Iteration 1923, the loss is 3136.746898477223, parameters k is -70.02187932801712 and b is 459.6662440362435\n",
      "Iteration 1924, the loss is 3135.7185704573058, parameters k is -70.00888525687088 and b is 459.5871926528443\n",
      "Iteration 1925, the loss is 3134.713546774116, parameters k is -69.99589118572464 and b is 459.5041887002751\n",
      "Iteration 1926, the loss is 3133.6855581492355, parameters k is -69.9828971145784 and b is 459.4251373168759\n",
      "Iteration 1927, the loss is 3132.680863816527, parameters k is -69.96990304343217 and b is 459.34213336430673\n",
      "Iteration 1928, the loss is 3131.6763394593127, parameters k is -69.95690897228593 and b is 459.25912941173755\n",
      "Iteration 1929, the loss is 3130.648849604447, parameters k is -69.94391490113969 and b is 459.18007802833836\n",
      "Iteration 1930, the loss is 3129.6446545977105, parameters k is -69.93092082999345 and b is 459.09707407576917\n",
      "Iteration 1931, the loss is 3128.640629566479, parameters k is -69.91792675884722 and b is 459.0140701232\n",
      "Iteration 1932, the loss is 3127.613638481628, parameters k is -69.90493268770098 and b is 458.9350187398008\n",
      "Iteration 1933, the loss is 3126.6099428008715, parameters k is -69.89193861655474 and b is 458.8520147872316\n",
      "Iteration 1934, the loss is 3125.6064170956124, parameters k is -69.8789445454085 and b is 458.7690108346624\n",
      "Iteration 1935, the loss is 3124.579924780772, parameters k is -69.86595047426226 and b is 458.68995945126323\n",
      "Iteration 1936, the loss is 3123.5767284259928, parameters k is -69.85295640311602 and b is 458.60695549869405\n",
      "Iteration 1937, the loss is 3122.573702046718, parameters k is -69.83996233196979 and b is 458.52395154612486\n",
      "Iteration 1938, the loss is 3121.5477085018865, parameters k is -69.82696826082355 and b is 458.4449001627257\n",
      "Iteration 1939, the loss is 3120.5450114730884, parameters k is -69.81397418967731 and b is 458.3618962101565\n",
      "Iteration 1940, the loss is 3119.519357323299, parameters k is -69.80098011853107 and b is 458.2828448267573\n",
      "Iteration 1941, the loss is 3118.5169896449715, parameters k is -69.78798604738483 and b is 458.1998408741881\n",
      "Iteration 1942, the loss is 3117.5147919421465, parameters k is -69.7749919762386 and b is 458.1168369216189\n",
      "Iteration 1943, the loss is 3116.489636562374, parameters k is -69.76199790509236 and b is 458.03778553821974\n",
      "Iteration 1944, the loss is 3115.4877682100264, parameters k is -69.74900383394612 and b is 457.95478158565055\n",
      "Iteration 1945, the loss is 3114.486069833178, parameters k is -69.73600976279988 and b is 457.87177763308136\n",
      "Iteration 1946, the loss is 3113.461413223417, parameters k is -69.72301569165364 and b is 457.7927262496822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1947, the loss is 3112.4600441970447, parameters k is -69.7100216205074 and b is 457.709722297113\n",
      "Iteration 1948, the loss is 3111.458845146178, parameters k is -69.69702754936117 and b is 457.6267183445438\n",
      "Iteration 1949, the loss is 3110.434687306434, parameters k is -69.68403347821493 and b is 457.5476669611446\n",
      "Iteration 1950, the loss is 3109.4338176060373, parameters k is -69.67103940706869 and b is 457.4646630085754\n",
      "Iteration 1951, the loss is 3108.4331178811412, parameters k is -69.65804533592245 and b is 457.38165905600624\n",
      "Iteration 1952, the loss is 3107.409458811409, parameters k is -69.64505126477621 and b is 457.30260767260705\n",
      "Iteration 1953, the loss is 3106.409088436997, parameters k is -69.63205719362998 and b is 457.21960372003787\n",
      "Iteration 1954, the loss is 3105.385768762297, parameters k is -69.61906312248374 and b is 457.1405523366387\n",
      "Iteration 1955, the loss is 3104.385727738358, parameters k is -69.6060690513375 and b is 457.0575483840695\n",
      "Iteration 1956, the loss is 3103.3858566899207, parameters k is -69.59307498019126 and b is 456.9745444315003\n",
      "Iteration 1957, the loss is 3102.363035785236, parameters k is -69.58008090904502 and b is 456.8954930481011\n",
      "Iteration 1958, the loss is 3101.3634940872794, parameters k is -69.56708683789878 and b is 456.81248909553193\n",
      "Iteration 1959, the loss is 3100.364122364817, parameters k is -69.55409276675255 and b is 456.72948514296274\n",
      "Iteration 1960, the loss is 3099.341800230145, parameters k is -69.54109869560631 and b is 456.65043375956355\n",
      "Iteration 1961, the loss is 3098.342757858162, parameters k is -69.52810462446007 and b is 456.56742980699437\n",
      "Iteration 1962, the loss is 3097.3438854616793, parameters k is -69.51511055331383 and b is 456.4844258544252\n",
      "Iteration 1963, the loss is 3096.3220620970246, parameters k is -69.5021164821676 and b is 456.405374471026\n",
      "Iteration 1964, the loss is 3095.3235190510227, parameters k is -69.48912241102136 and b is 456.3223705184568\n",
      "Iteration 1965, the loss is 3094.325145980515, parameters k is -69.47612833987512 and b is 456.2393665658876\n",
      "Iteration 1966, the loss is 3093.3038213858713, parameters k is -69.46313426872888 and b is 456.16031518248843\n",
      "Iteration 1967, the loss is 3092.3057776658434, parameters k is -69.45014019758264 and b is 456.07731122991925\n",
      "Iteration 1968, the loss is 3091.284792466236, parameters k is -69.4371461264364 and b is 455.99825984652006\n",
      "Iteration 1969, the loss is 3090.287078096685, parameters k is -69.42415205529016 and b is 455.91525589395087\n",
      "Iteration 1970, the loss is 3089.289533702632, parameters k is -69.41115798414393 and b is 455.8322519413817\n",
      "Iteration 1971, the loss is 3088.269047273036, parameters k is -69.39816391299769 and b is 455.7532005579825\n",
      "Iteration 1972, the loss is 3087.271832229464, parameters k is -69.38516984185145 and b is 455.6701966054133\n",
      "Iteration 1973, the loss is 3086.274787161396, parameters k is -69.37217577070521 and b is 455.5871926528441\n",
      "Iteration 1974, the loss is 3085.254799501812, parameters k is -69.35918169955897 and b is 455.50814126944493\n",
      "Iteration 1975, the loss is 3084.258083784217, parameters k is -69.34618762841274 and b is 455.42513731687575\n",
      "Iteration 1976, the loss is 3083.261538042123, parameters k is -69.3331935572665 and b is 455.34213336430656\n",
      "Iteration 1977, the loss is 3082.2420491525577, parameters k is -69.32019948612026 and b is 455.2630819809074\n",
      "Iteration 1978, the loss is 3081.245832760938, parameters k is -69.30720541497402 and b is 455.1800780283382\n",
      "Iteration 1979, the loss is 3080.2497863448166, parameters k is -69.29421134382778 and b is 455.097074075769\n",
      "Iteration 1980, the loss is 3079.230796225266, parameters k is -69.28121727268154 and b is 455.0180226923698\n",
      "Iteration 1981, the loss is 3078.2350791596295, parameters k is -69.2682232015353 and b is 454.9350187398006\n",
      "Iteration 1982, the loss is 3077.216428435105, parameters k is -69.25522913038907 and b is 454.85596735640144\n",
      "Iteration 1983, the loss is 3076.2210407199414, parameters k is -69.24223505924283 and b is 454.77296340383225\n",
      "Iteration 1984, the loss is 3075.2258229802806, parameters k is -69.22924098809659 and b is 454.68995945126306\n",
      "Iteration 1985, the loss is 3074.2076710257747, parameters k is -69.21624691695035 and b is 454.6109080678639\n",
      "Iteration 1986, the loss is 3073.2127826365895, parameters k is -69.20325284580412 and b is 454.5279041152947\n",
      "Iteration 1987, the loss is 3072.2180642229055, parameters k is -69.19025877465788 and b is 454.4449001627255\n",
      "Iteration 1988, the loss is 3071.2004110384155, parameters k is -69.17726470351164 and b is 454.3658487793263\n",
      "Iteration 1989, the loss is 3070.206021975208, parameters k is -69.1642706323654 and b is 454.2828448267571\n",
      "Iteration 1990, the loss is 3069.211802887499, parameters k is -69.15127656121916 and b is 454.19984087418794\n",
      "Iteration 1991, the loss is 3068.1946484730206, parameters k is -69.13828249007292 and b is 454.12078949078875\n",
      "Iteration 1992, the loss is 3067.2007587357934, parameters k is -69.12528841892669 and b is 454.03778553821957\n",
      "Iteration 1993, the loss is 3066.2070389740634, parameters k is -69.11229434778045 and b is 453.9547815856504\n",
      "Iteration 1994, the loss is 3065.16730871697, parameters k is -69.09930027663421 and b is 453.87968277142113\n",
      "Iteration 1995, the loss is 3064.17390770519, parameters k is -69.08630620548797 and b is 453.79667881885194\n",
      "Iteration 1996, the loss is 3063.1806766689183, parameters k is -69.07331213434173 and b is 453.71367486628276\n",
      "Iteration 1997, the loss is 3062.164540439545, parameters k is -69.0603180631955 and b is 453.63462348288357\n",
      "Iteration 1998, the loss is 3061.1716387537513, parameters k is -69.04732399204926 and b is 453.5516195303144\n",
      "Iteration 1999, the loss is 3060.1558419194134, parameters k is -69.03432992090302 and b is 453.4725681469152\n",
      "Iteration 2000, the loss is 3059.1632695840945, parameters k is -69.02133584975678 and b is 453.389564194346\n",
      "Iteration 2001, the loss is 3058.147812144798, parameters k is -69.00834177861054 and b is 453.3105128109468\n",
      "Iteration 2002, the loss is 3057.132534725561, parameters k is -68.9953477074643 and b is 453.23146142754763\n",
      "Iteration 2003, the loss is 3056.140451115696, parameters k is -68.98235363631807 and b is 453.14845747497844\n",
      "Iteration 2004, the loss is 3055.1255130914956, parameters k is -68.96935956517183 and b is 453.06940609157925\n",
      "Iteration 2005, the loss is 3054.1337588321076, parameters k is -68.95636549402559 and b is 452.98640213901007\n",
      "Iteration 2006, the loss is 3053.119160202941, parameters k is -68.94337142287935 and b is 452.9073507556109\n",
      "Iteration 2007, the loss is 3052.1277352940297, parameters k is -68.93037735173311 and b is 452.8243468030417\n",
      "Iteration 2008, the loss is 3051.1134760598998, parameters k is -68.91738328058688 and b is 452.7452954196425\n",
      "Iteration 2009, the loss is 3050.099396845833, parameters k is -68.90438920944064 and b is 452.6662440362433\n",
      "Iteration 2010, the loss is 3049.1084606623745, parameters k is -68.8913951382944 and b is 452.5832400836741\n",
      "Iteration 2011, the loss is 3048.094720843344, parameters k is -68.87840106714816 and b is 452.50418870027494\n",
      "Iteration 2012, the loss is 3047.1041140103666, parameters k is -68.86540699600192 and b is 452.42118474770575\n",
      "Iteration 2013, the loss is 3046.0907135863667, parameters k is -68.85241292485568 and b is 452.34213336430656\n",
      "Iteration 2014, the loss is 3045.0774931824317, parameters k is -68.83941885370945 and b is 452.2630819809074\n",
      "Iteration 2015, the loss is 3044.0873750749056, parameters k is -68.82642478256321 and b is 452.1800780283382\n",
      "Iteration 2016, the loss is 3043.074494066007, parameters k is -68.81343071141697 and b is 452.101026644939\n",
      "Iteration 2017, the loss is 3042.0847053089597, parameters k is -68.80043664027073 and b is 452.0180226923698\n",
      "Iteration 2018, the loss is 3041.0721636950943, parameters k is -68.7874425691245 and b is 451.9389713089706\n",
      "Iteration 2019, the loss is 3040.0827042885257, parameters k is -68.77444849797826 and b is 451.85596735640144\n",
      "Iteration 2020, the loss is 3039.070502069697, parameters k is -68.76145442683202 and b is 451.77691597300225\n",
      "Iteration 2021, the loss is 3038.058479870933, parameters k is -68.74846035568578 and b is 451.69786458960306\n",
      "Iteration 2022, the loss is 3037.069509189812, parameters k is -68.73546628453954 and b is 451.61486063703387\n",
      "Iteration 2023, the loss is 3036.057826386082, parameters k is -68.7224722133933 and b is 451.5358092536347\n",
      "Iteration 2024, the loss is 3035.06918505544, parameters k is -68.70947814224706 and b is 451.4528053010655\n",
      "Iteration 2025, the loss is 3034.0578416467547, parameters k is -68.69648407110083 and b is 451.3737539176663\n",
      "Iteration 2026, the loss is 3033.0466782581166, parameters k is -68.68348999995459 and b is 451.2947025342671\n",
      "Iteration 2027, the loss is 3032.058525652934, parameters k is -68.67049592880835 and b is 451.21169858169793\n",
      "Iteration 2028, the loss is 3031.0477016593322, parameters k is -68.65750185766211 and b is 451.13264719829874\n",
      "Iteration 2029, the loss is 3030.059878404622, parameters k is -68.64450778651587 and b is 451.04964324572956\n",
      "Iteration 2030, the loss is 3029.049393806065, parameters k is -68.63151371536964 and b is 450.97059186233037\n",
      "Iteration 2031, the loss is 3028.084741821709, parameters k is -68.6185196442234 and b is 450.88363534059124\n",
      "Iteration 2032, the loss is 3027.074575973098, parameters k is -68.60552557307716 and b is 450.80458395719205\n",
      "Iteration 2033, the loss is 3026.087422019868, parameters k is -68.59253150193092 and b is 450.72158000462287\n",
      "Iteration 2034, the loss is 3025.1004380421373, parameters k is -68.57953743078468 and b is 450.6385760520537\n",
      "Iteration 2035, the loss is 3024.113624039907, parameters k is -68.56654335963844 and b is 450.5555720994845\n",
      "Iteration 2036, the loss is 3023.1269800131754, parameters k is -68.5535492884922 and b is 450.4725681469153\n",
      "Iteration 2037, the loss is 3022.1405059619474, parameters k is -68.54055521734597 and b is 450.3895641943461\n",
      "Iteration 2038, the loss is 3021.1542018862174, parameters k is -68.52756114619973 and b is 450.30656024177694\n",
      "Iteration 2039, the loss is 3020.1680677859877, parameters k is -68.51456707505349 and b is 450.22355628920775\n",
      "Iteration 2040, the loss is 3019.1821036612523, parameters k is -68.50157300390725 and b is 450.14055233663856\n",
      "Iteration 2041, the loss is 3018.196309512029, parameters k is -68.48857893276102 and b is 450.0575483840694\n",
      "Iteration 2042, the loss is 3017.2106853382957, parameters k is -68.47558486161478 and b is 449.9745444315002\n",
      "Iteration 2043, the loss is 3016.225231140074, parameters k is -68.46259079046854 and b is 449.891540478931\n",
      "Iteration 2044, the loss is 3015.239946917344, parameters k is -68.4495967193223 and b is 449.8085365263618\n",
      "Iteration 2045, the loss is 3014.2548326701185, parameters k is -68.43660264817606 and b is 449.72553257379263\n",
      "Iteration 2046, the loss is 3013.2698883983903, parameters k is -68.42360857702982 and b is 449.64252862122345\n",
      "Iteration 2047, the loss is 3012.262133819521, parameters k is -68.41061450588359 and b is 449.56347723782426\n",
      "Iteration 2048, the loss is 3011.2775188982673, parameters k is -68.39762043473735 and b is 449.4804732852551\n",
      "Iteration 2049, the loss is 3010.293073952521, parameters k is -68.38462636359111 and b is 449.3974693326859\n",
      "Iteration 2050, the loss is 3009.3087989822698, parameters k is -68.37163229244487 and b is 449.3144653801167\n",
      "Iteration 2051, the loss is 3008.3246939875194, parameters k is -68.35863822129863 and b is 449.2314614275475\n",
      "Iteration 2052, the loss is 3007.3407589682774, parameters k is -68.3456441501524 and b is 449.14845747497833\n",
      "Iteration 2053, the loss is 3006.356993924524, parameters k is -68.33265007900616 and b is 449.06545352240914\n",
      "Iteration 2054, the loss is 3005.373398856278, parameters k is -68.31965600785992 and b is 448.98244956983996\n",
      "Iteration 2055, the loss is 3004.389973763527, parameters k is -68.30666193671368 and b is 448.89944561727077\n",
      "Iteration 2056, the loss is 3003.4067186462803, parameters k is -68.29366786556744 and b is 448.8164416647016\n",
      "Iteration 2057, the loss is 3002.4236335045343, parameters k is -68.2806737944212 and b is 448.7334377121324\n",
      "Iteration 2058, the loss is 3001.4176526954916, parameters k is -68.26767972327497 and b is 448.6543863287332\n",
      "Iteration 2059, the loss is 3000.434896904223, parameters k is -68.25468565212873 and b is 448.571382376164\n",
      "Iteration 2060, the loss is 2999.4523110884475, parameters k is -68.24169158098249 and b is 448.48837842359484\n",
      "Iteration 2061, the loss is 2998.44682904942, parameters k is -68.22869750983625 and b is 448.40932704019565\n",
      "Iteration 2062, the loss is 2997.464572584132, parameters k is -68.21570343869001 and b is 448.32632308762646\n",
      "Iteration 2063, the loss is 2996.459429940142, parameters k is -68.20270936754378 and b is 448.2472717042273\n",
      "Iteration 2064, the loss is 2995.4775028253275, parameters k is -68.18971529639754 and b is 448.1642677516581\n",
      "Iteration 2065, the loss is 2994.495745686011, parameters k is -68.1767212252513 and b is 448.0812637990889\n",
      "Iteration 2066, the loss is 2993.491101812032, parameters k is -68.16372715410506 and b is 448.0022124156897\n",
      "Iteration 2067, the loss is 2992.509674023191, parameters k is -68.15073308295882 and b is 447.9192084631205\n",
      "Iteration 2068, the loss is 2991.528416209857, parameters k is -68.13773901181258 and b is 447.83620451055134\n",
      "Iteration 2069, the loss is 2990.524271105889, parameters k is -68.12474494066635 and b is 447.75715312715215\n",
      "Iteration 2070, the loss is 2989.543342643028, parameters k is -68.11175086952011 and b is 447.67414917458296\n",
      "Iteration 2071, the loss is 2988.562584155671, parameters k is -68.09875679837387 and b is 447.5911452220138\n",
      "Iteration 2072, the loss is 2987.5589378217205, parameters k is -68.08576272722763 and b is 447.5120938386146\n",
      "Iteration 2073, the loss is 2986.5785086848377, parameters k is -68.0727686560814 and b is 447.4290898860454\n",
      "Iteration 2074, the loss is 2985.5982495234557, parameters k is -68.05977458493516 and b is 447.3460859334762\n",
      "Iteration 2075, the loss is 2984.595101959511, parameters k is -68.04678051378892 and b is 447.267034550077\n",
      "Iteration 2076, the loss is 2983.61517214861, parameters k is -68.03378644264268 and b is 447.18403059750784\n",
      "Iteration 2077, the loss is 2982.612363979708, parameters k is -68.02079237149644 and b is 447.10497921410865\n",
      "Iteration 2078, the loss is 2981.63276351928, parameters k is -68.0077983003502 and b is 447.02197526153947\n",
      "Iteration 2079, the loss is 2980.6533330343545, parameters k is -67.99480422920396 and b is 446.9389713089703\n",
      "Iteration 2080, the loss is 2979.651023635467, parameters k is -67.98181015805773 and b is 446.8599199255711\n",
      "Iteration 2081, the loss is 2978.671922501014, parameters k is -67.96881608691149 and b is 446.7769159730019\n",
      "Iteration 2082, the loss is 2977.6929913420613, parameters k is -67.95582201576525 and b is 446.6939120204327\n",
      "Iteration 2083, the loss is 2976.6911807131924, parameters k is -67.94282794461901 and b is 446.61486063703353\n",
      "Iteration 2084, the loss is 2975.712578904721, parameters k is -67.92983387347277 and b is 446.53185668446434\n",
      "Iteration 2085, the loss is 2974.7341470717456, parameters k is -67.91683980232654 and b is 446.44885273189516\n",
      "Iteration 2086, the loss is 2973.73283521288, parameters k is -67.9038457311803 and b is 446.36980134849597\n",
      "Iteration 2087, the loss is 2972.754732730387, parameters k is -67.89085166003406 and b is 446.2867973959268\n",
      "Iteration 2088, the loss is 2971.7768002233947, parameters k is -67.87785758888782 and b is 446.2037934433576\n",
      "Iteration 2089, the loss is 2970.7759871345456, parameters k is -67.86486351774158 and b is 446.1247420599584\n",
      "Iteration 2090, the loss is 2969.798383978028, parameters k is -67.85186944659534 and b is 446.0417381073892\n",
      "Iteration 2091, the loss is 2968.7979102842196, parameters k is -67.8388753754491 and b is 445.96268672399003\n",
      "Iteration 2092, the loss is 2967.8206364781727, parameters k is -67.82588130430287 and b is 445.87968277142085\n",
      "Iteration 2093, the loss is 2966.843532647639, parameters k is -67.81288723315663 and b is 445.79667881885166\n",
      "Iteration 2094, the loss is 2965.8435577238406, parameters k is -67.79989316201039 and b is 445.71762743545247\n",
      "Iteration 2095, the loss is 2964.8667832437754, parameters k is -67.78689909086415 and b is 445.6346234828833\n",
      "Iteration 2096, the loss is 2963.8901787392138, parameters k is -67.77390501971792 and b is 445.5516195303141\n",
      "Iteration 2097, the loss is 2962.8907025854287, parameters k is -67.76091094857168 and b is 445.4725681469149\n",
      "Iteration 2098, the loss is 2961.9144274313426, parameters k is -67.74791687742544 and b is 445.3895641943457\n",
      "Iteration 2099, the loss is 2960.938322252759, parameters k is -67.7349228062792 and b is 445.30656024177654\n",
      "Iteration 2100, the loss is 2959.9393448689866, parameters k is -67.72192873513296 and b is 445.22750885837735\n",
      "Iteration 2101, the loss is 2958.9635690408822, parameters k is -67.70893466398672 and b is 445.14450490580816\n",
      "Iteration 2102, the loss is 2957.9879631882745, parameters k is -67.69594059284049 and b is 445.061500953239\n",
      "Iteration 2103, the loss is 2956.989484574509, parameters k is -67.68294652169425 and b is 444.9824495698398\n",
      "Iteration 2104, the loss is 2956.0142080723826, parameters k is -67.66995245054801 and b is 444.8994456172706\n",
      "Iteration 2105, the loss is 2955.016068853663, parameters k is -67.65695837940177 and b is 444.8203942338714\n",
      "Iteration 2106, the loss is 2954.04112170201, parameters k is -67.64396430825553 and b is 444.7373902813022\n",
      "Iteration 2107, the loss is 2953.0663445258538, parameters k is -67.6309702371093 and b is 444.65438632873304\n",
      "Iteration 2108, the loss is 2952.0687040771504, parameters k is -67.61797616596306 and b is 444.57533494533385\n",
      "Iteration 2109, the loss is 2951.094256251472, parameters k is -67.60498209481682 and b is 444.49233099276466\n",
      "Iteration 2110, the loss is 2950.119978401296, parameters k is -67.59198802367058 and b is 444.4093270401955\n",
      "Iteration 2111, the loss is 2949.1228367226026, parameters k is -67.57899395252434 and b is 444.3302756567963\n",
      "Iteration 2112, the loss is 2948.1488882229073, parameters k is -67.5659998813781 and b is 444.2472717042271\n",
      "Iteration 2113, the loss is 2947.175109698709, parameters k is -67.55300581023187 and b is 444.1642677516579\n",
      "Iteration 2114, the loss is 2946.1784667900233, parameters k is -67.54001173908563 and b is 444.0852163682587\n",
      "Iteration 2115, the loss is 2945.2050176163034, parameters k is -67.52701766793939 and b is 444.00221241568954\n",
      "Iteration 2116, the loss is 2944.2317384180824, parameters k is -67.51402359679315 and b is 443.91920846312036\n",
      "Iteration 2117, the loss is 2943.235594279417, parameters k is -67.50102952564691 and b is 443.84015707972117\n",
      "Iteration 2118, the loss is 2942.2626444316775, parameters k is -67.48803545450068 and b is 443.757153127152\n",
      "Iteration 2119, the loss is 2941.266839688047, parameters k is -67.47504138335444 and b is 443.6781017437528\n",
      "Iteration 2120, the loss is 2940.2942191907814, parameters k is -67.4620473122082 and b is 443.5950977911836\n",
      "Iteration 2121, the loss is 2939.321768669014, parameters k is -67.44905324106196 and b is 443.5120938386144\n",
      "Iteration 2122, the loss is 2938.3264626953924, parameters k is -67.43605916991572 and b is 443.43304245521523\n",
      "Iteration 2123, the loss is 2937.3543415241056, parameters k is -67.42306509876948 and b is 443.35003850264604\n",
      "Iteration 2124, the loss is 2936.3823903283187, parameters k is -67.41007102762325 and b is 443.26703455007686\n",
      "Iteration 2125, the loss is 2935.387583124716, parameters k is -67.39707695647701 and b is 443.18798316667767\n",
      "Iteration 2126, the loss is 2934.4159612794065, parameters k is -67.38408288533077 and b is 443.1049792141085\n",
      "Iteration 2127, the loss is 2933.4445094095936, parameters k is -67.37108881418453 and b is 443.0219752615393\n",
      "Iteration 2128, the loss is 2932.4502009760035, parameters k is -67.3580947430383 and b is 442.9429238781401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2129, the loss is 2931.479078456667, parameters k is -67.34510067189206 and b is 442.8599199255709\n",
      "Iteration 2130, the loss is 2930.5081259128383, parameters k is -67.33210660074582 and b is 442.77691597300173\n",
      "Iteration 2131, the loss is 2929.514316249256, parameters k is -67.31911252959958 and b is 442.69786458960255\n",
      "Iteration 2132, the loss is 2928.5436930559063, parameters k is -67.30611845845334 and b is 442.61486063703336\n",
      "Iteration 2133, the loss is 2927.550222787361, parameters k is -67.2931243873071 and b is 442.53580925363417\n",
      "Iteration 2134, the loss is 2926.5799289444844, parameters k is -67.28013031616086 and b is 442.452805301065\n",
      "Iteration 2135, the loss is 2925.609805077107, parameters k is -67.26713624501463 and b is 442.3698013484958\n",
      "Iteration 2136, the loss is 2924.6168335785774, parameters k is -67.25414217386839 and b is 442.2907499650966\n",
      "Iteration 2137, the loss is 2923.6470390616787, parameters k is -67.24114810272215 and b is 442.2077460125274\n",
      "Iteration 2138, the loss is 2922.67741452028, parameters k is -67.22815403157591 and b is 442.12474205995824\n",
      "Iteration 2139, the loss is 2921.6849417917624, parameters k is -67.21515996042967 and b is 442.04569067655905\n",
      "Iteration 2140, the loss is 2920.7156466008423, parameters k is -67.20216588928344 and b is 441.96268672398986\n",
      "Iteration 2141, the loss is 2919.7465213854202, parameters k is -67.1891718181372 and b is 441.8796827714207\n",
      "Iteration 2142, the loss is 2918.754547426917, parameters k is -67.17617774699096 and b is 441.8006313880215\n",
      "Iteration 2143, the loss is 2917.785751561973, parameters k is -67.16318367584472 and b is 441.7176274354523\n",
      "Iteration 2144, the loss is 2916.7941169985065, parameters k is -67.15018960469848 and b is 441.6385760520531\n",
      "Iteration 2145, the loss is 2915.8256504840356, parameters k is -67.13719553355224 and b is 441.5555720994839\n",
      "Iteration 2146, the loss is 2914.8573539450704, parameters k is -67.124201462406 and b is 441.47256814691474\n",
      "Iteration 2147, the loss is 2913.86621815162, parameters k is -67.11120739125977 and b is 441.39351676351555\n",
      "Iteration 2148, the loss is 2912.89825096313, parameters k is -67.09821332011353 and b is 441.31051281094636\n",
      "Iteration 2149, the loss is 2911.930453750138, parameters k is -67.08521924896729 and b is 441.2275088583772\n",
      "Iteration 2150, the loss is 2910.939816726703, parameters k is -67.07222517782105 and b is 441.148457474978\n",
      "Iteration 2151, the loss is 2909.97234886419, parameters k is -67.05923110667482 and b is 441.0654535224088\n",
      "Iteration 2152, the loss is 2909.0050509771745, parameters k is -67.04623703552858 and b is 440.9824495698396\n",
      "Iteration 2153, the loss is 2908.014912723753, parameters k is -67.03324296438234 and b is 440.9033981864404\n",
      "Iteration 2154, the loss is 2907.0479441872167, parameters k is -67.0202488932361 and b is 440.82039423387124\n",
      "Iteration 2155, the loss is 2906.0811456261804, parameters k is -67.00725482208986 and b is 440.73739028130205\n",
      "Iteration 2156, the loss is 2905.091506142766, parameters k is -66.99426075094362 and b is 440.65833889790287\n",
      "Iteration 2157, the loss is 2904.1250369322106, parameters k is -66.98126667979739 and b is 440.5753349453337\n",
      "Iteration 2158, the loss is 2903.1357368438366, parameters k is -66.96827260865115 and b is 440.4962835619345\n",
      "Iteration 2159, the loss is 2902.169596983755, parameters k is -66.95527853750491 and b is 440.4132796093653\n",
      "Iteration 2160, the loss is 2901.2036270991716, parameters k is -66.94228446635867 and b is 440.3302756567961\n",
      "Iteration 2161, the loss is 2900.214825780815, parameters k is -66.92929039521243 and b is 440.25122427339693\n",
      "Iteration 2162, the loss is 2899.249185246711, parameters k is -66.9162963240662 and b is 440.16822032082774\n",
      "Iteration 2163, the loss is 2898.2837146881084, parameters k is -66.90330225291996 and b is 440.08521636825856\n",
      "Iteration 2164, the loss is 2897.2954121397624, parameters k is -66.89030818177372 and b is 440.00616498485937\n",
      "Iteration 2165, the loss is 2896.330270931633, parameters k is -66.87731411062748 and b is 439.9231610322902\n",
      "Iteration 2166, the loss is 2895.365299699011, parameters k is -66.86432003948124 and b is 439.840157079721\n",
      "Iteration 2167, the loss is 2894.3774959206767, parameters k is -66.851325968335 and b is 439.7611056963218\n",
      "Iteration 2168, the loss is 2893.412854038529, parameters k is -66.83833189718877 and b is 439.6781017437526\n",
      "Iteration 2169, the loss is 2892.448382131878, parameters k is -66.82533782604253 and b is 439.59509779118343\n",
      "Iteration 2170, the loss is 2891.4610771235575, parameters k is -66.81234375489629 and b is 439.51604640778424\n",
      "Iteration 2171, the loss is 2890.4969345673885, parameters k is -66.79934968375005 and b is 439.43304245521506\n",
      "Iteration 2172, the loss is 2889.5099689541025, parameters k is -66.78635561260381 and b is 439.35399107181587\n",
      "Iteration 2173, the loss is 2888.5461557484086, parameters k is -66.77336154145758 and b is 439.2709871192467\n",
      "Iteration 2174, the loss is 2887.582512518215, parameters k is -66.76036747031134 and b is 439.1879831666775\n",
      "Iteration 2175, the loss is 2886.596045674947, parameters k is -66.7473733991651 and b is 439.1089317832783\n",
      "Iteration 2176, the loss is 2885.6327317952314, parameters k is -66.73437932801886 and b is 439.0259278307091\n",
      "Iteration 2177, the loss is 2884.6695878910145, parameters k is -66.72138525687262 and b is 438.94292387813994\n",
      "Iteration 2178, the loss is 2883.6836198177593, parameters k is -66.70839118572638 and b is 438.86387249474075\n",
      "Iteration 2179, the loss is 2882.7208052640185, parameters k is -66.69539711458015 and b is 438.78086854217156\n",
      "Iteration 2180, the loss is 2881.7581606857784, parameters k is -66.68240304343391 and b is 438.6978645896024\n",
      "Iteration 2181, the loss is 2880.7726913825395, parameters k is -66.66940897228767 and b is 438.6188132062032\n",
      "Iteration 2182, the loss is 2879.810376154776, parameters k is -66.65641490114143 and b is 438.535809253634\n",
      "Iteration 2183, the loss is 2878.8482309025185, parameters k is -66.6434208299952 and b is 438.4528053010648\n",
      "Iteration 2184, the loss is 2877.863260369283, parameters k is -66.63042675884896 and b is 438.3737539176656\n",
      "Iteration 2185, the loss is 2876.901444467501, parameters k is -66.61743268770272 and b is 438.29074996509644\n",
      "Iteration 2186, the loss is 2875.9168133293074, parameters k is -66.60443861655648 and b is 438.21169858169725\n",
      "Iteration 2187, the loss is 2874.955326778001, parameters k is -66.59144454541024 and b is 438.12869462912806\n",
      "Iteration 2188, the loss is 2873.994010202196, parameters k is -66.578450474264 and b is 438.0456906765589\n",
      "Iteration 2189, the loss is 2873.009877834017, parameters k is -66.56545640311776 and b is 437.9666392931597\n",
      "Iteration 2190, the loss is 2872.0488906086866, parameters k is -66.55246233197153 and b is 437.8836353405905\n",
      "Iteration 2191, the loss is 2871.088073358857, parameters k is -66.53946826082529 and b is 437.8006313880213\n",
      "Iteration 2192, the loss is 2870.104439760691, parameters k is -66.52647418967905 and b is 437.7215800046221\n",
      "Iteration 2193, the loss is 2869.1439518613415, parameters k is -66.51348011853281 and b is 437.63857605205294\n",
      "Iteration 2194, the loss is 2868.183633937487, parameters k is -66.50048604738657 and b is 437.55557209948375\n",
      "Iteration 2195, the loss is 2867.2004991093336, parameters k is -66.48749197624034 and b is 437.47652071608456\n",
      "Iteration 2196, the loss is 2866.2405105359653, parameters k is -66.4744979050941 and b is 437.3935167635154\n",
      "Iteration 2197, the loss is 2865.2806919380882, parameters k is -66.46150383394786 and b is 437.3105128109462\n",
      "Iteration 2198, the loss is 2864.2980558799486, parameters k is -66.44850976280162 and b is 437.231461427547\n",
      "Iteration 2199, the loss is 2863.338566632552, parameters k is -66.43551569165538 and b is 437.1484574749778\n",
      "Iteration 2200, the loss is 2862.3562699694485, parameters k is -66.42252162050914 and b is 437.0694060915786\n",
      "Iteration 2201, the loss is 2861.39711007253, parameters k is -66.4095275493629 and b is 436.98640213900944\n",
      "Iteration 2202, the loss is 2860.4381201511123, parameters k is -66.39653347821667 and b is 436.90339818644026\n",
      "Iteration 2203, the loss is 2859.456322258023, parameters k is -66.38353940707043 and b is 436.82434680304107\n",
      "Iteration 2204, the loss is 2858.497661687082, parameters k is -66.37054533592419 and b is 436.7413428504719\n",
      "Iteration 2205, the loss is 2857.5391710916388, parameters k is -66.35755126477795 and b is 436.6583388979027\n",
      "Iteration 2206, the loss is 2856.5349247110307, parameters k is -66.34455719363172 and b is 436.58324008367345\n",
      "Iteration 2207, the loss is 2855.5767528655415, parameters k is -66.33156312248548 and b is 436.50023613110426\n",
      "Iteration 2208, the loss is 2854.6187509955566, parameters k is -66.31856905133924 and b is 436.4172321785351\n",
      "Iteration 2209, the loss is 2853.6379712875764, parameters k is -66.305574980193 and b is 436.3381807951359\n",
      "Iteration 2210, the loss is 2852.680298768066, parameters k is -66.29258090904676 and b is 436.2551768425667\n",
      "Iteration 2211, the loss is 2851.699858455125, parameters k is -66.27958683790052 and b is 436.1761254591675\n",
      "Iteration 2212, the loss is 2850.719598162236, parameters k is -66.26659276675429 and b is 436.0970740757683\n",
      "Iteration 2213, the loss is 2849.7395178894108, parameters k is -66.25359869560805 and b is 436.01802269236913\n",
      "Iteration 2214, the loss is 2848.78249347033, parameters k is -66.24060462446181 and b is 435.93501873979994\n",
      "Iteration 2215, the loss is 2847.802752592543, parameters k is -66.22761055331557 and b is 435.85596735640075\n",
      "Iteration 2216, the loss is 2846.823191734813, parameters k is -66.21461648216933 and b is 435.77691597300156\n",
      "Iteration 2217, the loss is 2845.8666560411853, parameters k is -66.2016224110231 and b is 435.6939120204324\n",
      "Iteration 2218, the loss is 2844.887434578497, parameters k is -66.18862833987686 and b is 435.6148606370332\n",
      "Iteration 2219, the loss is 2843.9083931358605, parameters k is -66.17563426873062 and b is 435.535809253634\n",
      "Iteration 2220, the loss is 2842.9295317132846, parameters k is -66.16264019758438 and b is 435.4567578702348\n",
      "Iteration 2221, the loss is 2841.9736441200916, parameters k is -66.14964612643814 and b is 435.3737539176656\n",
      "Iteration 2222, the loss is 2840.9951220925554, parameters k is -66.1366520552919 and b is 435.29470253426643\n",
      "Iteration 2223, the loss is 2840.016780085077, parameters k is -66.12365798414567 and b is 435.21565115086725\n",
      "Iteration 2224, the loss is 2839.0613812173374, parameters k is -66.11066391299943 and b is 435.13264719829806\n",
      "Iteration 2225, the loss is 2838.083378604897, parameters k is -66.09766984185319 and b is 435.05359581489887\n",
      "Iteration 2226, the loss is 2837.1055560125133, parameters k is -66.08467577070695 and b is 434.9745444314997\n",
      "Iteration 2227, the loss is 2836.12791344019, parameters k is -66.07168169956071 and b is 434.8954930481005\n",
      "Iteration 2228, the loss is 2835.173162672885, parameters k is -66.05868762841448 and b is 434.8124890955313\n",
      "Iteration 2229, the loss is 2834.1958594955977, parameters k is -66.04569355726824 and b is 434.7334377121321\n",
      "Iteration 2230, the loss is 2833.218736338369, parameters k is -66.032699486122 and b is 434.6543863287329\n",
      "Iteration 2231, the loss is 2832.2644742965203, parameters k is -66.01970541497576 and b is 434.57138237616374\n",
      "Iteration 2232, the loss is 2831.2876905343314, parameters k is -66.00671134382952 and b is 434.49233099276455\n",
      "Iteration 2233, the loss is 2830.3110867921987, parameters k is -65.99371727268328 and b is 434.41327960936536\n",
      "Iteration 2234, the loss is 2829.3573134757967, parameters k is -65.98072320153705 and b is 434.3302756567962\n",
      "Iteration 2235, the loss is 2828.381049128706, parameters k is -65.96772913039081 and b is 434.251224273397\n",
      "Iteration 2236, the loss is 2827.4049648016667, parameters k is -65.95473505924457 and b is 434.1721728899978\n",
      "Iteration 2237, the loss is 2826.429060494696, parameters k is -65.94174098809833 and b is 434.0931215065986\n",
      "Iteration 2238, the loss is 2825.475935278728, parameters k is -65.9287469169521 and b is 434.0101175540294\n",
      "Iteration 2239, the loss is 2824.500370366789, parameters k is -65.91575284580586 and b is 433.93106617063023\n",
      "Iteration 2240, the loss is 2823.5249854749127, parameters k is -65.90275877465962 and b is 433.85201478723104\n",
      "Iteration 2241, the loss is 2822.5723489843963, parameters k is -65.88976470351338 and b is 433.76901083466186\n",
      "Iteration 2242, the loss is 2821.5973034875537, parameters k is -65.87677063236714 and b is 433.68995945126267\n",
      "Iteration 2243, the loss is 2820.622438010772, parameters k is -65.8637765612209 and b is 433.6109080678635\n",
      "Iteration 2244, the loss is 2819.647752554048, parameters k is -65.85078249007466 and b is 433.5318566844643\n",
      "Iteration 2245, the loss is 2818.695764163962, parameters k is -65.83778841892843 and b is 433.4488527318951\n",
      "Iteration 2246, the loss is 2817.721418102277, parameters k is -65.82479434778219 and b is 433.3698013484959\n",
      "Iteration 2247, the loss is 2816.74725206065, parameters k is -65.81180027663595 and b is 433.2907499650967\n",
      "Iteration 2248, the loss is 2815.795752396019, parameters k is -65.79880620548971 and b is 433.20774601252754\n",
      "Iteration 2249, the loss is 2814.821925749431, parameters k is -65.78581213434347 and b is 433.12869462912835\n",
      "Iteration 2250, the loss is 2813.8482791229007, parameters k is -65.77281806319724 and b is 433.04964324572916\n",
      "Iteration 2251, the loss is 2812.874812516429, parameters k is -65.759823992051 and b is 432.97059186232997\n",
      "Iteration 2252, the loss is 2811.9239609522315, parameters k is -65.74682992090476 and b is 432.8875879097608\n",
      "Iteration 2253, the loss is 2810.950833740793, parameters k is -65.73383584975852 and b is 432.8085365263616\n",
      "Iteration 2254, the loss is 2809.9778865494222, parameters k is -65.72084177861228 and b is 432.7294851429624\n",
      "Iteration 2255, the loss is 2809.0275237106753, parameters k is -65.70784770746604 and b is 432.6464811903932\n",
      "Iteration 2256, the loss is 2808.0549159143334, parameters k is -65.6948536363198 and b is 432.567429806994\n",
      "Iteration 2257, the loss is 2807.082488138056, parameters k is -65.68185956517357 and b is 432.48837842359484\n",
      "Iteration 2258, the loss is 2806.1102403818327, parameters k is -65.66886549402733 and b is 432.40932704019565\n",
      "Iteration 2259, the loss is 2805.1605256435228, parameters k is -65.65587142288109 and b is 432.32632308762646\n",
      "Iteration 2260, the loss is 2804.1886172823415, parameters k is -65.64287735173485 and b is 432.2472717042273\n",
      "Iteration 2261, the loss is 2803.2168889412196, parameters k is -65.62988328058861 and b is 432.1682203208281\n",
      "Iteration 2262, the loss is 2802.267662928362, parameters k is -65.61688920944238 and b is 432.0852163682589\n",
      "Iteration 2263, the loss is 2801.2962739822688, parameters k is -65.60389513829614 and b is 432.0061649848597\n",
      "Iteration 2264, the loss is 2800.325065056246, parameters k is -65.5909010671499 and b is 431.9271136014605\n",
      "Iteration 2265, the loss is 2799.376327768835, parameters k is -65.57790699600366 and b is 431.84410964889133\n",
      "Iteration 2266, the loss is 2798.4054582378512, parameters k is -65.56491292485742 and b is 431.76505826549214\n",
      "Iteration 2267, the loss is 2797.43476872692, parameters k is -65.55191885371119 and b is 431.68600688209295\n",
      "Iteration 2268, the loss is 2796.4642592360433, parameters k is -65.53892478256495 and b is 431.60695549869376\n",
      "Iteration 2269, the loss is 2795.516170049069, parameters k is -65.52593071141871 and b is 431.5239515461246\n",
      "Iteration 2270, the loss is 2794.5459999532354, parameters k is -65.51293664027247 and b is 431.4449001627254\n",
      "Iteration 2271, the loss is 2793.5760098774626, parameters k is -65.49994256912623 and b is 431.3658487793262\n",
      "Iteration 2272, the loss is 2792.6284094159428, parameters k is -65.48694849798 and b is 431.282844826757\n",
      "Iteration 2273, the loss is 2791.6587587352037, parameters k is -65.47395442683376 and b is 431.2037934433578\n",
      "Iteration 2274, the loss is 2790.6892880745263, parameters k is -65.46096035568752 and b is 431.12474205995863\n",
      "Iteration 2275, the loss is 2789.7199974339014, parameters k is -65.44796628454128 and b is 431.04569067655945\n",
      "Iteration 2276, the loss is 2788.773045072813, parameters k is -65.43497221339504 and b is 430.96268672399026\n",
      "Iteration 2277, the loss is 2787.804093827229, parameters k is -65.4219781422488 and b is 430.88363534059107\n",
      "Iteration 2278, the loss is 2786.835322601705, parameters k is -65.40898407110257 and b is 430.8045839571919\n",
      "Iteration 2279, the loss is 2785.8888589660723, parameters k is -65.39598999995633 and b is 430.7215800046227\n",
      "Iteration 2280, the loss is 2784.920427135584, parameters k is -65.38299592881009 and b is 430.6425286212235\n",
      "Iteration 2281, the loss is 2783.9521753251574, parameters k is -65.37000185766385 and b is 430.5634772378243\n",
      "Iteration 2282, the loss is 2782.98410353479, parameters k is -65.35700778651761 and b is 430.4844258544251\n",
      "Iteration 2283, the loss is 2782.0382879995877, parameters k is -65.34401371537137 and b is 430.40142190185594\n",
      "Iteration 2284, the loss is 2781.070555604253, parameters k is -65.33101964422514 and b is 430.32237051845675\n",
      "Iteration 2285, the loss is 2780.103003228979, parameters k is -65.3180255730789 and b is 430.24331913505756\n",
      "Iteration 2286, the loss is 2779.157676419232, parameters k is -65.30503150193266 and b is 430.1603151824884\n",
      "Iteration 2287, the loss is 2778.1904634389953, parameters k is -65.29203743078642 and b is 430.0812637990892\n",
      "Iteration 2288, the loss is 2777.223430478824, parameters k is -65.27904335964018 and b is 430.00221241569\n",
      "Iteration 2289, the loss is 2776.2565775387015, parameters k is -65.26604928849395 and b is 429.9231610322908\n",
      "Iteration 2290, the loss is 2775.3118988293845, parameters k is -65.25305521734771 and b is 429.8401570797216\n",
      "Iteration 2291, the loss is 2774.345385284306, parameters k is -65.24006114620147 and b is 429.76110569632243\n",
      "Iteration 2292, the loss is 2773.3790517592874, parameters k is -65.22706707505523 and b is 429.68205431292324\n",
      "Iteration 2293, the loss is 2772.4348617754226, parameters k is -65.214073003909 and b is 429.59905036035406\n",
      "Iteration 2294, the loss is 2771.4688676454366, parameters k is -65.20107893276275 and b is 429.51999897695487\n",
      "Iteration 2295, the loss is 2770.5030535355136, parameters k is -65.18808486161652 and b is 429.4409475935557\n",
      "Iteration 2296, the loss is 2769.5593522771023, parameters k is -65.17509079047028 and b is 429.3579436409865\n",
      "Iteration 2297, the loss is 2768.5938775622144, parameters k is -65.16209671932404 and b is 429.2788922575873\n",
      "Iteration 2298, the loss is 2767.6285828673867, parameters k is -65.1491026481778 and b is 429.1998408741881\n",
      "Iteration 2299, the loss is 2766.663468192619, parameters k is -65.13610857703156 and b is 429.1207894907889\n",
      "Iteration 2300, the loss is 2765.7204150346374, parameters k is -65.12311450588533 and b is 429.03778553821974\n",
      "Iteration 2301, the loss is 2764.755639754902, parameters k is -65.11012043473909 and b is 428.95873415482055\n",
      "Iteration 2302, the loss is 2763.7910444952345, parameters k is -65.09712636359285 and b is 428.87968277142136\n",
      "Iteration 2303, the loss is 2762.8484800627093, parameters k is -65.08413229244661 and b is 428.7966788188522\n",
      "Iteration 2304, the loss is 2761.8842241980733, parameters k is -65.07113822130037 and b is 428.717627435453\n",
      "Iteration 2305, the loss is 2760.920148353493, parameters k is -65.05814415015413 and b is 428.6385760520538\n",
      "Iteration 2306, the loss is 2759.9562525289766, parameters k is -65.0451500790079 and b is 428.5595246686546\n",
      "Iteration 2307, the loss is 2759.0143361968844, parameters k is -65.03215600786166 and b is 428.4765207160854\n",
      "Iteration 2308, the loss is 2758.050779767402, parameters k is -65.01916193671542 and b is 428.3974693326862\n",
      "Iteration 2309, the loss is 2757.087403357981, parameters k is -65.00616786556918 and b is 428.31841794928704\n",
      "Iteration 2310, the loss is 2756.145975751342, parameters k is -64.99317379442294 and b is 428.23541399671785\n",
      "Iteration 2311, the loss is 2755.1829387369567, parameters k is -64.9801797232767 and b is 428.15636261331866\n",
      "Iteration 2312, the loss is 2754.2200817426346, parameters k is -64.96718565213047 and b is 428.0773112299195\n",
      "Iteration 2313, the loss is 2753.2574047683675, parameters k is -64.95419158098423 and b is 427.9982598465203\n",
      "Iteration 2314, the loss is 2752.316625262159, parameters k is -64.94119750983799 and b is 427.9152558939511\n",
      "Iteration 2315, the loss is 2751.3542876829365, parameters k is -64.92820343869175 and b is 427.8362045105519\n",
      "Iteration 2316, the loss is 2750.3921301237615, parameters k is -64.91520936754551 and b is 427.7571531271527\n",
      "Iteration 2317, the loss is 2749.4518393430067, parameters k is -64.90221529639928 and b is 427.67414917458353\n",
      "Iteration 2318, the loss is 2748.4900211788736, parameters k is -64.88922122525304 and b is 427.59509779118434\n",
      "Iteration 2319, the loss is 2747.5283830348026, parameters k is -64.8762271541068 and b is 427.51604640778515\n",
      "Iteration 2320, the loss is 2746.566924910789, parameters k is -64.86323308296056 and b is 427.43699502438596\n",
      "Iteration 2321, the loss is 2745.6272822304645, parameters k is -64.85023901181432 and b is 427.3539910718168\n",
      "Iteration 2322, the loss is 2744.666163501488, parameters k is -64.83724494066809 and b is 427.2749396884176\n",
      "Iteration 2323, the loss is 2743.7052247925694, parameters k is -64.82425086952185 and b is 427.1958883050184\n",
      "Iteration 2324, the loss is 2742.7660708377016, parameters k is -64.81125679837561 and b is 427.1128843524492\n",
      "Iteration 2325, the loss is 2741.8054715238186, parameters k is -64.79826272722937 and b is 427.03383296905\n",
      "Iteration 2326, the loss is 2740.8450522299995, parameters k is -64.78526865608313 and b is 426.95478158565084\n",
      "Iteration 2327, the loss is 2739.906387000582, parameters k is -64.7722745849369 and b is 426.87177763308165\n",
      "Iteration 2328, the loss is 2738.946307101798, parameters k is -64.75928051379066 and b is 426.79272624968246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2329, the loss is 2737.986407223074, parameters k is -64.74628644264442 and b is 426.71367486628327\n",
      "Iteration 2330, the loss is 2737.0266873644086, parameters k is -64.73329237149818 and b is 426.6346234828841\n",
      "Iteration 2331, the loss is 2736.088670235425, parameters k is -64.72029830035194 and b is 426.5516195303149\n",
      "Iteration 2332, the loss is 2735.12928977179, parameters k is -64.7073042292057 and b is 426.4725681469157\n",
      "Iteration 2333, the loss is 2734.17008932822, parameters k is -64.69431015805947 and b is 426.3935167635165\n",
      "Iteration 2334, the loss is 2733.232560924694, parameters k is -64.68131608691323 and b is 426.31051281094733\n",
      "Iteration 2335, the loss is 2732.2736998761598, parameters k is -64.66832201576699 and b is 426.23146142754814\n",
      "Iteration 2336, the loss is 2731.315018847687, parameters k is -64.65532794462075 and b is 426.15241004414895\n",
      "Iteration 2337, the loss is 2730.3565178392746, parameters k is -64.64233387347451 and b is 426.07335866074976\n",
      "Iteration 2338, the loss is 2729.4411094670386, parameters k is -64.62933980232827 and b is 425.98640213901064\n",
      "Iteration 2339, the loss is 2728.504409739959, parameters k is -64.61634573118204 and b is 425.90339818644145\n",
      "Iteration 2340, the loss is 2727.5678799883863, parameters k is -64.6033516600358 and b is 425.82039423387226\n",
      "Iteration 2341, the loss is 2726.6315202123114, parameters k is -64.59035758888956 and b is 425.7373902813031\n",
      "Iteration 2342, the loss is 2725.6953304117405, parameters k is -64.57736351774332 and b is 425.6543863287339\n",
      "Iteration 2343, the loss is 2724.7593105866704, parameters k is -64.56436944659708 and b is 425.5713823761647\n",
      "Iteration 2344, the loss is 2723.8234607370914, parameters k is -64.55137537545085 and b is 425.4883784235955\n",
      "Iteration 2345, the loss is 2722.887780863022, parameters k is -64.53838130430461 and b is 425.40537447102633\n",
      "Iteration 2346, the loss is 2721.9522709644493, parameters k is -64.52538723315837 and b is 425.32237051845715\n",
      "Iteration 2347, the loss is 2721.0169310413767, parameters k is -64.51239316201213 and b is 425.23936656588796\n",
      "Iteration 2348, the loss is 2720.0817610938047, parameters k is -64.4993990908659 and b is 425.1563626133188\n",
      "Iteration 2349, the loss is 2719.1467611217345, parameters k is -64.48640501971965 and b is 425.0733586607496\n",
      "Iteration 2350, the loss is 2718.2119311251613, parameters k is -64.47341094857342 and b is 424.9903547081804\n",
      "Iteration 2351, the loss is 2717.2772711040884, parameters k is -64.46041687742718 and b is 424.9073507556112\n",
      "Iteration 2352, the loss is 2716.342781058522, parameters k is -64.44742280628094 and b is 424.82434680304203\n",
      "Iteration 2353, the loss is 2715.4084609884503, parameters k is -64.4344287351347 and b is 424.74134285047285\n",
      "Iteration 2354, the loss is 2714.47431089388, parameters k is -64.42143466398846 and b is 424.65833889790366\n",
      "Iteration 2355, the loss is 2713.540330774807, parameters k is -64.40844059284223 and b is 424.5753349453345\n",
      "Iteration 2356, the loss is 2712.6065206312446, parameters k is -64.39544652169599 and b is 424.4923309927653\n",
      "Iteration 2357, the loss is 2711.672880463174, parameters k is -64.38245245054975 and b is 424.4093270401961\n",
      "Iteration 2358, the loss is 2710.739410270604, parameters k is -64.36945837940351 and b is 424.3263230876269\n",
      "Iteration 2359, the loss is 2709.8061100535356, parameters k is -64.35646430825727 and b is 424.24331913505773\n",
      "Iteration 2360, the loss is 2708.8729798119707, parameters k is -64.34347023711103 and b is 424.16031518248855\n",
      "Iteration 2361, the loss is 2707.940019545897, parameters k is -64.3304761659648 and b is 424.07731122991936\n",
      "Iteration 2362, the loss is 2707.0072292553327, parameters k is -64.31748209481856 and b is 423.9943072773502\n",
      "Iteration 2363, the loss is 2706.0746089402655, parameters k is -64.30448802367232 and b is 423.911303324781\n",
      "Iteration 2364, the loss is 2705.1421586006995, parameters k is -64.29149395252608 and b is 423.8282993722118\n",
      "Iteration 2365, the loss is 2704.209878236634, parameters k is -64.27849988137984 and b is 423.7452954196426\n",
      "Iteration 2366, the loss is 2703.277767848067, parameters k is -64.2655058102336 and b is 423.66229146707343\n",
      "Iteration 2367, the loss is 2702.3458274350032, parameters k is -64.25251173908737 and b is 423.57928751450424\n",
      "Iteration 2368, the loss is 2701.4140569974356, parameters k is -64.23951766794113 and b is 423.49628356193506\n",
      "Iteration 2369, the loss is 2700.482456535375, parameters k is -64.22652359679489 and b is 423.4132796093659\n",
      "Iteration 2370, the loss is 2699.5510260488077, parameters k is -64.21352952564865 and b is 423.3302756567967\n",
      "Iteration 2371, the loss is 2698.61976553774, parameters k is -64.20053545450241 and b is 423.2472717042275\n",
      "Iteration 2372, the loss is 2697.6886750021813, parameters k is -64.18754138335618 and b is 423.1642677516583\n",
      "Iteration 2373, the loss is 2696.757754442114, parameters k is -64.17454731220994 and b is 423.08126379908913\n",
      "Iteration 2374, the loss is 2695.827003857551, parameters k is -64.1615532410637 and b is 422.99825984651994\n",
      "Iteration 2375, the loss is 2694.896423248486, parameters k is -64.14855916991746 and b is 422.91525589395076\n",
      "Iteration 2376, the loss is 2693.9660126149283, parameters k is -64.13556509877122 and b is 422.83225194138157\n",
      "Iteration 2377, the loss is 2693.035771956865, parameters k is -64.12257102762499 and b is 422.7492479888124\n",
      "Iteration 2378, the loss is 2692.105701274303, parameters k is -64.10957695647875 and b is 422.6662440362432\n",
      "Iteration 2379, the loss is 2691.175800567239, parameters k is -64.09658288533251 and b is 422.583240083674\n",
      "Iteration 2380, the loss is 2690.246069835678, parameters k is -64.08358881418627 and b is 422.5002361311048\n",
      "Iteration 2381, the loss is 2689.3165090796147, parameters k is -64.07059474304003 and b is 422.41723217853564\n",
      "Iteration 2382, the loss is 2688.3871182990583, parameters k is -64.0576006718938 and b is 422.33422822596646\n",
      "Iteration 2383, the loss is 2687.457897493995, parameters k is -64.04460660074756 and b is 422.25122427339727\n",
      "Iteration 2384, the loss is 2686.528846664438, parameters k is -64.03161252960132 and b is 422.1682203208281\n",
      "Iteration 2385, the loss is 2685.5999658103756, parameters k is -64.01861845845508 and b is 422.0852163682589\n",
      "Iteration 2386, the loss is 2684.6712549318167, parameters k is -64.00562438730884 and b is 422.0022124156897\n",
      "Iteration 2387, the loss is 2683.742714028757, parameters k is -63.9926303161626 and b is 421.9192084631205\n",
      "Iteration 2388, the loss is 2682.814343101193, parameters k is -63.97963624501635 and b is 421.83620451055134\n",
      "Iteration 2389, the loss is 2681.886142149136, parameters k is -63.966642173870106 and b is 421.75320055798215\n",
      "Iteration 2390, the loss is 2680.95811117258, parameters k is -63.95364810272386 and b is 421.67019660541297\n",
      "Iteration 2391, the loss is 2680.030250171518, parameters k is -63.940654031577616 and b is 421.5871926528438\n",
      "Iteration 2392, the loss is 2679.1025591459634, parameters k is -63.92765996043137 and b is 421.5041887002746\n",
      "Iteration 2393, the loss is 2678.1750380959024, parameters k is -63.914665889285125 and b is 421.4211847477054\n",
      "Iteration 2394, the loss is 2677.2476870213445, parameters k is -63.90167181813888 and b is 421.3381807951362\n",
      "Iteration 2395, the loss is 2676.3205059222873, parameters k is -63.888677746992634 and b is 421.25517684256704\n",
      "Iteration 2396, the loss is 2675.3934947987336, parameters k is -63.87568367584639 and b is 421.17217288999785\n",
      "Iteration 2397, the loss is 2674.4666536506734, parameters k is -63.862689604700144 and b is 421.08916893742867\n",
      "Iteration 2398, the loss is 2673.539982478119, parameters k is -63.8496955335539 and b is 421.0061649848595\n",
      "Iteration 2399, the loss is 2672.61348128106, parameters k is -63.83670146240765 and b is 420.9231610322903\n",
      "Iteration 2400, the loss is 2671.687150059507, parameters k is -63.82370739126141 and b is 420.8401570797211\n",
      "Iteration 2401, the loss is 2670.7609888134484, parameters k is -63.81071332011516 and b is 420.7571531271519\n",
      "Iteration 2402, the loss is 2669.8349975428982, parameters k is -63.79771924896892 and b is 420.67414917458274\n",
      "Iteration 2403, the loss is 2668.9091762478415, parameters k is -63.78472517782267 and b is 420.59114522201355\n",
      "Iteration 2404, the loss is 2667.9835249282874, parameters k is -63.77173110667643 and b is 420.50814126944437\n",
      "Iteration 2405, the loss is 2667.0580435842303, parameters k is -63.75873703553018 and b is 420.4251373168752\n",
      "Iteration 2406, the loss is 2666.132732215677, parameters k is -63.745742964383936 and b is 420.342133364306\n",
      "Iteration 2407, the loss is 2665.207590822624, parameters k is -63.73274889323769 and b is 420.2591294117368\n",
      "Iteration 2408, the loss is 2664.2826194050735, parameters k is -63.719754822091446 and b is 420.1761254591676\n",
      "Iteration 2409, the loss is 2663.357817963019, parameters k is -63.7067607509452 and b is 420.09312150659844\n",
      "Iteration 2410, the loss is 2662.433186496465, parameters k is -63.693766679798955 and b is 420.01011755402925\n",
      "Iteration 2411, the loss is 2661.508725005413, parameters k is -63.68077260865271 and b is 419.92711360146006\n",
      "Iteration 2412, the loss is 2660.584433489863, parameters k is -63.667778537506464 and b is 419.8441096488909\n",
      "Iteration 2413, the loss is 2659.6603119498095, parameters k is -63.65478446636022 and b is 419.7611056963217\n",
      "Iteration 2414, the loss is 2658.7363603852605, parameters k is -63.641790395213974 and b is 419.6781017437525\n",
      "Iteration 2415, the loss is 2657.812578796208, parameters k is -63.62879632406773 and b is 419.5950977911833\n",
      "Iteration 2416, the loss is 2656.888967182658, parameters k is -63.61580225292148 and b is 419.51209383861413\n",
      "Iteration 2417, the loss is 2655.965525544605, parameters k is -63.60280818177524 and b is 419.42908988604495\n",
      "Iteration 2418, the loss is 2655.0422538820603, parameters k is -63.58981411062899 and b is 419.34608593347576\n",
      "Iteration 2419, the loss is 2654.1191521950104, parameters k is -63.57682003948275 and b is 419.2630819809066\n",
      "Iteration 2420, the loss is 2653.19622048346, parameters k is -63.5638259683365 and b is 419.1800780283374\n",
      "Iteration 2421, the loss is 2652.273458747409, parameters k is -63.55083189719026 and b is 419.0970740757682\n",
      "Iteration 2422, the loss is 2651.350866986861, parameters k is -63.53783782604401 and b is 419.014070123199\n",
      "Iteration 2423, the loss is 2650.4284452018123, parameters k is -63.524843754897766 and b is 418.93106617062983\n",
      "Iteration 2424, the loss is 2649.5061933922652, parameters k is -63.51184968375152 and b is 418.84806221806065\n",
      "Iteration 2425, the loss is 2648.5841115582143, parameters k is -63.498855612605276 and b is 418.76505826549146\n",
      "Iteration 2426, the loss is 2647.6621996996705, parameters k is -63.48586154145903 and b is 418.6820543129223\n",
      "Iteration 2427, the loss is 2646.740457816621, parameters k is -63.472867470312785 and b is 418.5990503603531\n",
      "Iteration 2428, the loss is 2645.8188859090756, parameters k is -63.45987339916654 and b is 418.5160464077839\n",
      "Iteration 2429, the loss is 2644.8974839770285, parameters k is -63.446879328020295 and b is 418.4330424552147\n",
      "Iteration 2430, the loss is 2643.976252020482, parameters k is -63.43388525687405 and b is 418.35003850264553\n",
      "Iteration 2431, the loss is 2643.0551900394394, parameters k is -63.420891185727804 and b is 418.26703455007635\n",
      "Iteration 2432, the loss is 2642.13429803389, parameters k is -63.40789711458156 and b is 418.18403059750716\n",
      "Iteration 2433, the loss is 2641.213576003846, parameters k is -63.39490304343531 and b is 418.101026644938\n",
      "Iteration 2434, the loss is 2640.293023949303, parameters k is -63.38190897228907 and b is 418.0180226923688\n",
      "Iteration 2435, the loss is 2639.372641870261, parameters k is -63.36891490114282 and b is 417.9350187397996\n",
      "Iteration 2436, the loss is 2638.452429766714, parameters k is -63.35592082999658 and b is 417.8520147872304\n",
      "Iteration 2437, the loss is 2637.532387638669, parameters k is -63.34292675885033 and b is 417.76901083466123\n",
      "Iteration 2438, the loss is 2636.6125154861243, parameters k is -63.32993268770409 and b is 417.68600688209204\n",
      "Iteration 2439, the loss is 2635.6928133090846, parameters k is -63.31693861655784 and b is 417.60300292952286\n",
      "Iteration 2440, the loss is 2634.7732811075366, parameters k is -63.303944545411596 and b is 417.5199989769537\n",
      "Iteration 2441, the loss is 2633.8539188814975, parameters k is -63.29095047426535 and b is 417.4369950243845\n",
      "Iteration 2442, the loss is 2632.9347266309583, parameters k is -63.277956403119106 and b is 417.3539910718153\n",
      "Iteration 2443, the loss is 2632.015704355914, parameters k is -63.26496233197286 and b is 417.2709871192461\n",
      "Iteration 2444, the loss is 2631.096852056369, parameters k is -63.251968260826615 and b is 417.18798316667693\n",
      "Iteration 2445, the loss is 2630.1781697323286, parameters k is -63.23897418968037 and b is 417.10497921410774\n",
      "Iteration 2446, the loss is 2629.259657383789, parameters k is -63.225980118534125 and b is 417.02197526153856\n",
      "Iteration 2447, the loss is 2628.3413150107494, parameters k is -63.21298604738788 and b is 416.93897130896937\n",
      "Iteration 2448, the loss is 2627.4231426132046, parameters k is -63.199991976241634 and b is 416.8559673564002\n",
      "Iteration 2449, the loss is 2626.5051401911683, parameters k is -63.18699790509539 and b is 416.772963403831\n",
      "Iteration 2450, the loss is 2625.587307744628, parameters k is -63.17400383394914 and b is 416.6899594512618\n",
      "Iteration 2451, the loss is 2624.669645273588, parameters k is -63.1610097628029 and b is 416.6069554986926\n",
      "Iteration 2452, the loss is 2623.7521527780495, parameters k is -63.14801569165665 and b is 416.52395154612344\n",
      "Iteration 2453, the loss is 2622.834830258013, parameters k is -63.13502162051041 and b is 416.44094759355426\n",
      "Iteration 2454, the loss is 2621.917677713476, parameters k is -63.12202754936416 and b is 416.35794364098507\n",
      "Iteration 2455, the loss is 2621.000695144436, parameters k is -63.10903347821792 and b is 416.2749396884159\n",
      "Iteration 2456, the loss is 2620.0838825508968, parameters k is -63.09603940707167 and b is 416.1919357358467\n",
      "Iteration 2457, the loss is 2619.167239932861, parameters k is -63.083045335925426 and b is 416.1089317832775\n",
      "Iteration 2458, the loss is 2618.250767290324, parameters k is -63.07005126477918 and b is 416.0259278307083\n",
      "Iteration 2459, the loss is 2617.3344646232867, parameters k is -63.057057193632936 and b is 415.94292387813914\n",
      "Iteration 2460, the loss is 2616.4183319317494, parameters k is -63.04406312248669 and b is 415.85991992556995\n",
      "Iteration 2461, the loss is 2615.5023692157138, parameters k is -63.031069051340445 and b is 415.77691597300077\n",
      "Iteration 2462, the loss is 2614.586576475184, parameters k is -63.0180749801942 and b is 415.6939120204316\n",
      "Iteration 2463, the loss is 2613.670953710145, parameters k is -63.005080909047955 and b is 415.6109080678624\n",
      "Iteration 2464, the loss is 2612.75550092061, parameters k is -62.99208683790171 and b is 415.5279041152932\n",
      "Iteration 2465, the loss is 2611.840218106573, parameters k is -62.979092766755464 and b is 415.444900162724\n",
      "Iteration 2466, the loss is 2610.902276470177, parameters k is -62.96609869560922 and b is 415.36584877932484\n",
      "Iteration 2467, the loss is 2609.987323006623, parameters k is -62.95310462446297 and b is 415.28284482675565\n",
      "Iteration 2468, the loss is 2609.072539518565, parameters k is -62.94011055331673 and b is 415.19984087418646\n",
      "Iteration 2469, the loss is 2608.135096652185, parameters k is -62.92711648217048 and b is 415.1207894907873\n",
      "Iteration 2470, the loss is 2607.2206425146055, parameters k is -62.91412241102424 and b is 415.0377855382181\n",
      "Iteration 2471, the loss is 2606.3063583525236, parameters k is -62.90112833987799 and b is 414.9547815856489\n",
      "Iteration 2472, the loss is 2605.369414256155, parameters k is -62.88813426873175 and b is 414.8757302022497\n",
      "Iteration 2473, the loss is 2604.4554594445563, parameters k is -62.8751401975855 and b is 414.7927262496805\n",
      "Iteration 2474, the loss is 2603.5188547432244, parameters k is -62.86214612643926 and b is 414.71367486628134\n",
      "Iteration 2475, the loss is 2602.605229282097, parameters k is -62.84915205529301 and b is 414.63067091371215\n",
      "Iteration 2476, the loss is 2601.6917737964745, parameters k is -62.836157984146766 and b is 414.54766696114297\n",
      "Iteration 2477, the loss is 2600.755667865156, parameters k is -62.82316391300052 and b is 414.4686155777438\n",
      "Iteration 2478, the loss is 2599.8425417300123, parameters k is -62.810169841854275 and b is 414.3856116251746\n",
      "Iteration 2479, the loss is 2598.929585570363, parameters k is -62.79717577070803 and b is 414.3026076726054\n",
      "Iteration 2480, the loss is 2597.9939784090607, parameters k is -62.784181699561785 and b is 414.2235562892062\n",
      "Iteration 2481, the loss is 2597.081351599889, parameters k is -62.77118762841554 and b is 414.14055233663703\n",
      "Iteration 2482, the loss is 2596.168894766217, parameters k is -62.758193557269294 and b is 414.05754838406784\n",
      "Iteration 2483, the loss is 2595.2337863749267, parameters k is -62.74519948612305 and b is 413.97849700066865\n",
      "Iteration 2484, the loss is 2594.3216588917344, parameters k is -62.732205414976804 and b is 413.89549304809947\n",
      "Iteration 2485, the loss is 2593.3868898954793, parameters k is -62.71921134383056 and b is 413.8164416647003\n",
      "Iteration 2486, the loss is 2592.475091762765, parameters k is -62.70621727268431 and b is 413.7334377121311\n",
      "Iteration 2487, the loss is 2591.5634636055547, parameters k is -62.69322320153807 and b is 413.6504337595619\n",
      "Iteration 2488, the loss is 2590.6291933793086, parameters k is -62.68022913039182 and b is 413.5713823761627\n",
      "Iteration 2489, the loss is 2589.7178945725736, parameters k is -62.66723505924558 and b is 413.48837842359353\n",
      "Iteration 2490, the loss is 2588.8067657413367, parameters k is -62.65424098809933 and b is 413.40537447102434\n",
      "Iteration 2491, the loss is 2587.8729942851087, parameters k is -62.64124691695309 and b is 413.32632308762516\n",
      "Iteration 2492, the loss is 2586.9621948043477, parameters k is -62.62825284580684 and b is 413.24331913505597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2493, the loss is 2586.0515652990866, parameters k is -62.615258774660596 and b is 413.1603151824868\n",
      "Iteration 2494, the loss is 2585.1182926128695, parameters k is -62.60226470351435 and b is 413.0812637990876\n",
      "Iteration 2495, the loss is 2584.207992458092, parameters k is -62.589270632368105 and b is 412.9982598465184\n",
      "Iteration 2496, the loss is 2583.29786227881, parameters k is -62.57627656122186 and b is 412.9152558939492\n",
      "Iteration 2497, the loss is 2582.3650883626074, parameters k is -62.563282490075615 and b is 412.83620451055003\n",
      "Iteration 2498, the loss is 2581.455287533808, parameters k is -62.55028841892937 and b is 412.75320055798085\n",
      "Iteration 2499, the loss is 2580.5228530126396, parameters k is -62.537294347783124 and b is 412.67414917458166\n",
      "Iteration 2500, the loss is 2579.6133815343105, parameters k is -62.52430027663688 and b is 412.59114522201247\n",
      "Iteration 2501, the loss is 2578.7040800314853, parameters k is -62.511306205490634 and b is 412.5081412694433\n",
      "Iteration 2502, the loss is 2577.772144280336, parameters k is -62.49831213434439 and b is 412.4290898860441\n",
      "Iteration 2503, the loss is 2576.8631721279826, parameters k is -62.48531806319814 and b is 412.3460859334749\n",
      "Iteration 2504, the loss is 2575.954369951137, parameters k is -62.4723239920519 and b is 412.2630819809057\n",
      "Iteration 2505, the loss is 2575.022932969995, parameters k is -62.45932992090565 and b is 412.18403059750653\n",
      "Iteration 2506, the loss is 2574.114460143623, parameters k is -62.44633584975941 and b is 412.10102664493735\n",
      "Iteration 2507, the loss is 2573.206157292752, parameters k is -62.43334177861316 and b is 412.01802269236816\n",
      "Iteration 2508, the loss is 2572.2752190816304, parameters k is -62.42034770746692 and b is 411.938971308969\n",
      "Iteration 2509, the loss is 2571.367245581233, parameters k is -62.40735363632067 and b is 411.8559673563998\n",
      "Iteration 2510, the loss is 2570.4594420563367, parameters k is -62.394359565174426 and b is 411.7729634038306\n",
      "Iteration 2511, the loss is 2569.5290026152284, parameters k is -62.38136549402818 and b is 411.6939120204314\n",
      "Iteration 2512, the loss is 2568.6215284408104, parameters k is -62.368371422881935 and b is 411.6109080678622\n",
      "Iteration 2513, the loss is 2567.691428394735, parameters k is -62.35537735173569 and b is 411.53185668446304\n",
      "Iteration 2514, the loss is 2566.7842835707957, parameters k is -62.342383280589445 and b is 411.44885273189385\n",
      "Iteration 2515, the loss is 2565.8773087223517, parameters k is -62.3293892094432 and b is 411.36584877932466\n",
      "Iteration 2516, the loss is 2564.947707446296, parameters k is -62.316395138296954 and b is 411.2867973959255\n",
      "Iteration 2517, the loss is 2564.041061948331, parameters k is -62.30340106715071 and b is 411.2037934433563\n",
      "Iteration 2518, the loss is 2563.1345864258715, parameters k is -62.290406996004464 and b is 411.1207894907871\n",
      "Iteration 2519, the loss is 2562.2054839198227, parameters k is -62.27741292485822 and b is 411.0417381073879\n",
      "Iteration 2520, the loss is 2561.2993377478374, parameters k is -62.26441885371197 and b is 410.9587341548187\n",
      "Iteration 2521, the loss is 2560.3933615513533, parameters k is -62.25142478256573 and b is 410.87573020224954\n",
      "Iteration 2522, the loss is 2559.4647578153176, parameters k is -62.23843071141948 and b is 410.79667881885035\n",
      "Iteration 2523, the loss is 2558.5591109693087, parameters k is -62.22543664027324 and b is 410.71367486628117\n",
      "Iteration 2524, the loss is 2557.653634098803, parameters k is -62.21244256912699 and b is 410.630670913712\n",
      "Iteration 2525, the loss is 2556.7255291327833, parameters k is -62.19944849798075 and b is 410.5516195303128\n",
      "Iteration 2526, the loss is 2555.820381612752, parameters k is -62.1864544268345 and b is 410.4686155777436\n",
      "Iteration 2527, the loss is 2554.8926160417714, parameters k is -62.173460355688256 and b is 410.3895641943444\n",
      "Iteration 2528, the loss is 2553.987797872217, parameters k is -62.16046628454201 and b is 410.30656024177523\n",
      "Iteration 2529, the loss is 2553.083149678162, parameters k is -62.147472213395766 and b is 410.22355628920604\n",
      "Iteration 2530, the loss is 2552.155882877193, parameters k is -62.13447814224952 and b is 410.14450490580685\n",
      "Iteration 2531, the loss is 2551.2515640336183, parameters k is -62.121484071103275 and b is 410.06150095323767\n",
      "Iteration 2532, the loss is 2550.347415165542, parameters k is -62.10848999995703 and b is 409.9784970006685\n",
      "Iteration 2533, the loss is 2549.4206471345838, parameters k is -62.095495928810784 and b is 409.8994456172693\n",
      "Iteration 2534, the loss is 2548.5168276169825, parameters k is -62.08250185766454 and b is 409.8164416647001\n",
      "Iteration 2535, the loss is 2547.613178074893, parameters k is -62.069507786518294 and b is 409.7334377121309\n",
      "Iteration 2536, the loss is 2546.6869088139447, parameters k is -62.05651371537205 and b is 409.65438632873173\n",
      "Iteration 2537, the loss is 2545.783588622325, parameters k is -62.0435196442258 and b is 409.57138237616255\n",
      "Iteration 2538, the loss is 2544.880438406209, parameters k is -62.03052557307956 and b is 409.48837842359336\n",
      "Iteration 2539, the loss is 2543.954667915275, parameters k is -62.01753150193331 and b is 409.40932704019417\n",
      "Iteration 2540, the loss is 2543.051847049632, parameters k is -62.00453743078707 and b is 409.326323087625\n",
      "Iteration 2541, the loss is 2542.126415953737, parameters k is -61.99154335964082 and b is 409.2472717042258\n",
      "Iteration 2542, the loss is 2541.223924438577, parameters k is -61.97854928849458 and b is 409.1642677516566\n",
      "Iteration 2543, the loss is 2540.321602898909, parameters k is -61.96555521734833 and b is 409.0812637990874\n",
      "Iteration 2544, the loss is 2539.3966705730286, parameters k is -61.952561146202086 and b is 409.00221241568823\n",
      "Iteration 2545, the loss is 2538.4946783838413, parameters k is -61.93956707505584 and b is 408.91920846311905\n",
      "Iteration 2546, the loss is 2537.592856170155, parameters k is -61.926573003909596 and b is 408.83620451054986\n",
      "Iteration 2547, the loss is 2536.668422614286, parameters k is -61.91357893276335 and b is 408.7571531271507\n",
      "Iteration 2548, the loss is 2535.766929751073, parameters k is -61.900584861617105 and b is 408.6741491745815\n",
      "Iteration 2549, the loss is 2534.8656068633704, parameters k is -61.88759079047086 and b is 408.5911452220123\n",
      "Iteration 2550, the loss is 2533.941672077512, parameters k is -61.874596719324614 and b is 408.5120938386131\n",
      "Iteration 2551, the loss is 2533.040678540278, parameters k is -61.86160264817837 and b is 408.4290898860439\n",
      "Iteration 2552, the loss is 2532.1398549785463, parameters k is -61.848608577032124 and b is 408.34608593347474\n",
      "Iteration 2553, the loss is 2531.2164189627074, parameters k is -61.83561450588588 and b is 408.26703455007555\n",
      "Iteration 2554, the loss is 2530.315924751454, parameters k is -61.82262043473963 and b is 408.18403059750636\n",
      "Iteration 2555, the loss is 2529.3928281306466, parameters k is -61.80962636359339 and b is 408.1049792141072\n",
      "Iteration 2556, the loss is 2528.492663269868, parameters k is -61.79663229244714 and b is 408.021975261538\n",
      "Iteration 2557, the loss is 2527.592668384595, parameters k is -61.7836382213009 and b is 407.9389713089688\n",
      "Iteration 2558, the loss is 2526.670070533802, parameters k is -61.77064415015465 and b is 407.8599199255696\n",
      "Iteration 2559, the loss is 2525.770404999002, parameters k is -61.75765007900841 and b is 407.7769159730004\n",
      "Iteration 2560, the loss is 2524.8709094397013, parameters k is -61.74465600786216 and b is 407.69391202043124\n",
      "Iteration 2561, the loss is 2523.9488103589247, parameters k is -61.731661936715916 and b is 407.61486063703205\n",
      "Iteration 2562, the loss is 2523.049644150102, parameters k is -61.71866786556967 and b is 407.53185668446287\n",
      "Iteration 2563, the loss is 2522.1506479167815, parameters k is -61.705673794423426 and b is 407.4488527318937\n",
      "Iteration 2564, the loss is 2521.229047606016, parameters k is -61.69267972327718 and b is 407.3698013484945\n",
      "Iteration 2565, the loss is 2520.330380723169, parameters k is -61.679685652130935 and b is 407.2867973959253\n",
      "Iteration 2566, the loss is 2519.4318838158247, parameters k is -61.66669158098469 and b is 407.2037934433561\n",
      "Iteration 2567, the loss is 2518.510782275077, parameters k is -61.653697509838445 and b is 407.12474205995693\n",
      "Iteration 2568, the loss is 2517.6126147182054, parameters k is -61.6407034386922 and b is 407.04173810738774\n",
      "Iteration 2569, the loss is 2516.691852572494, parameters k is -61.627709367545954 and b is 406.96268672398855\n",
      "Iteration 2570, the loss is 2515.7940143661035, parameters k is -61.61471529639971 and b is 406.87968277141937\n",
      "Iteration 2571, the loss is 2514.8963461352128, parameters k is -61.60172122525346 and b is 406.7966788188502\n",
      "Iteration 2572, the loss is 2513.976082759512, parameters k is -61.58872715410722 and b is 406.717627435451\n",
      "Iteration 2573, the loss is 2513.0787438790985, parameters k is -61.57573308296097 and b is 406.6346234828818\n",
      "Iteration 2574, the loss is 2512.1815749741854, parameters k is -61.56273901181473 and b is 406.5516195303126\n",
      "Iteration 2575, the loss is 2511.2618103684995, parameters k is -61.54974494066848 and b is 406.47256814691343\n",
      "Iteration 2576, the loss is 2510.3649708140656, parameters k is -61.53675086952224 and b is 406.38956419434425\n",
      "Iteration 2577, the loss is 2509.4683012351297, parameters k is -61.52375679837599 and b is 406.30656024177506\n",
      "Iteration 2578, the loss is 2508.549035399458, parameters k is -61.510762727229746 and b is 406.22750885837587\n",
      "Iteration 2579, the loss is 2507.6526951709993, parameters k is -61.4977686560835 and b is 406.1445049058067\n",
      "Iteration 2580, the loss is 2506.7565249180416, parameters k is -61.484774584937256 and b is 406.0615009532375\n",
      "Iteration 2581, the loss is 2505.837757852381, parameters k is -61.47178051379101 and b is 405.9824495698383\n",
      "Iteration 2582, the loss is 2504.9419169499006, parameters k is -61.458786442644765 and b is 405.8994456172691\n",
      "Iteration 2583, the loss is 2504.0234892792732, parameters k is -61.44579237149852 and b is 405.82039423386993\n",
      "Iteration 2584, the loss is 2503.1279777272703, parameters k is -61.432798300352275 and b is 405.73739028130075\n",
      "Iteration 2585, the loss is 2502.2326361507744, parameters k is -61.41980422920603 and b is 405.65438632873156\n",
      "Iteration 2586, the loss is 2501.3147072501592, parameters k is -61.406810158059784 and b is 405.5753349453324\n",
      "Iteration 2587, the loss is 2500.4196950241358, parameters k is -61.39381608691354 and b is 405.4923309927632\n",
      "Iteration 2588, the loss is 2499.5248527736107, parameters k is -61.38082201576729 and b is 405.409327040194\n",
      "Iteration 2589, the loss is 2498.607422643018, parameters k is -61.36782794462105 and b is 405.3302756567948\n",
      "Iteration 2590, the loss is 2497.712909742964, parameters k is -61.3548338734748 and b is 405.2472717042256\n",
      "Iteration 2591, the loss is 2496.8185668184165, parameters k is -61.34183980232856 and b is 405.16426775165644\n",
      "Iteration 2592, the loss is 2495.9016354578357, parameters k is -61.32884573118231 and b is 405.08521636825725\n",
      "Iteration 2593, the loss is 2495.0076218837653, parameters k is -61.31585166003607 and b is 405.00221241568806\n",
      "Iteration 2594, the loss is 2494.0910299182196, parameters k is -61.30285758888982 and b is 404.9231610322889\n",
      "Iteration 2595, the loss is 2493.197345694623, parameters k is -61.289863517743576 and b is 404.8401570797197\n",
      "Iteration 2596, the loss is 2492.303831446531, parameters k is -61.27686944659733 and b is 404.7571531271505\n",
      "Iteration 2597, the loss is 2491.387738250993, parameters k is -61.263875375451086 and b is 404.6781017437513\n",
      "Iteration 2598, the loss is 2490.494553353381, parameters k is -61.25088130430484 and b is 404.5950977911821\n",
      "Iteration 2599, the loss is 2489.601538431263, parameters k is -61.237887233158595 and b is 404.51209383861294\n",
      "Iteration 2600, the loss is 2488.6859440057456, parameters k is -61.22489316201235 and b is 404.43304245521375\n",
      "Iteration 2601, the loss is 2487.7932584341092, parameters k is -61.211899090866105 and b is 404.35003850264457\n",
      "Iteration 2602, the loss is 2486.9007428379714, parameters k is -61.19890501971986 and b is 404.2670345500754\n",
      "Iteration 2603, the loss is 2485.9856471824623, parameters k is -61.185910948573614 and b is 404.1879831666762\n",
      "Iteration 2604, the loss is 2485.093460936807, parameters k is -61.17291687742737 and b is 404.104979214107\n",
      "Iteration 2605, the loss is 2484.2014446666435, parameters k is -61.15992280628112 and b is 404.0219752615378\n",
      "Iteration 2606, the loss is 2483.286847781152, parameters k is -61.14692873513488 and b is 403.94292387813863\n",
      "Iteration 2607, the loss is 2482.395160861469, parameters k is -61.13393466398863 and b is 403.85991992556944\n",
      "Iteration 2608, the loss is 2481.4809033710126, parameters k is -61.12094059284239 and b is 403.78086854217025\n",
      "Iteration 2609, the loss is 2480.5895458018063, parameters k is -61.10794652169614 and b is 403.69786458960107\n",
      "Iteration 2610, the loss is 2479.6983582080975, parameters k is -61.0949524505499 and b is 403.6148606370319\n",
      "Iteration 2611, the loss is 2478.7845994876584, parameters k is -61.08195837940365 and b is 403.5358092536327\n",
      "Iteration 2612, the loss is 2477.893741244431, parameters k is -61.06896430825741 and b is 403.4528053010635\n",
      "Iteration 2613, the loss is 2477.0030529767023, parameters k is -61.05597023711116 and b is 403.3698013484943\n",
      "Iteration 2614, the loss is 2476.089793026269, parameters k is -61.042976165964916 and b is 403.29074996509513\n",
      "Iteration 2615, the loss is 2475.1994341090194, parameters k is -61.02998209481867 and b is 403.20774601252594\n",
      "Iteration 2616, the loss is 2474.309245167271, parameters k is -61.016988023672425 and b is 403.12474205995676\n",
      "Iteration 2617, the loss is 2473.3964839868495, parameters k is -61.00399395252618 and b is 403.04569067655757\n",
      "Iteration 2618, the loss is 2472.5066243955775, parameters k is -60.990999881379935 and b is 402.9626867239884\n",
      "Iteration 2619, the loss is 2471.616934779807, parameters k is -60.97800581023369 and b is 402.8796827714192\n",
      "Iteration 2620, the loss is 2470.7046723694007, parameters k is -60.965011739087444 and b is 402.80063138802\n",
      "Iteration 2621, the loss is 2469.815312104107, parameters k is -60.9520176679412 and b is 402.7176274354508\n",
      "Iteration 2622, the loss is 2468.903389088739, parameters k is -60.939023596794954 and b is 402.63857605205163\n",
      "Iteration 2623, the loss is 2468.0143581739194, parameters k is -60.92602952564871 and b is 402.55557209948245\n",
      "Iteration 2624, the loss is 2467.125497234605, parameters k is -60.91303545450246 and b is 402.47256814691326\n",
      "Iteration 2625, the loss is 2466.2140729892544, parameters k is -60.90004138335622 and b is 402.39351676351407\n",
      "Iteration 2626, the loss is 2465.3255414004084, parameters k is -60.88704731220997 and b is 402.3105128109449\n",
      "Iteration 2627, the loss is 2464.4371797870745, parameters k is -60.87405324106373 and b is 402.2275088583757\n",
      "Iteration 2628, the loss is 2463.5262543117315, parameters k is -60.86105916991748 and b is 402.1484574749765\n",
      "Iteration 2629, the loss is 2462.638222048867, parameters k is -60.84806509877124 and b is 402.0654535224073\n",
      "Iteration 2630, the loss is 2461.750359761503, parameters k is -60.83507102762499 and b is 401.98244956983814\n",
      "Iteration 2631, the loss is 2460.839933056175, parameters k is -60.822076956478746 and b is 401.90339818643895\n",
      "Iteration 2632, the loss is 2459.95240011929, parameters k is -60.8090828853325 and b is 401.82039423386976\n",
      "Iteration 2633, the loss is 2459.0650371579045, parameters k is -60.796088814186255 and b is 401.7373902813006\n",
      "Iteration 2634, the loss is 2458.1551092225964, parameters k is -60.78309474304001 and b is 401.6583388979014\n",
      "Iteration 2635, the loss is 2457.268075611685, parameters k is -60.770100671893765 and b is 401.5753349453322\n",
      "Iteration 2636, the loss is 2456.3584870714085, parameters k is -60.75710660074752 and b is 401.496283561933\n",
      "Iteration 2637, the loss is 2455.471782810977, parameters k is -60.744112529601274 and b is 401.4132796093638\n",
      "Iteration 2638, the loss is 2454.585248526043, parameters k is -60.73111845845503 and b is 401.33027565679464\n",
      "Iteration 2639, the loss is 2453.676158755785, parameters k is -60.718124387308784 and b is 401.25122427339545\n",
      "Iteration 2640, the loss is 2452.7899538213287, parameters k is -60.70513031616254 and b is 401.16822032082626\n",
      "Iteration 2641, the loss is 2451.9039188623765, parameters k is -60.69213624501629 and b is 401.0852163682571\n",
      "Iteration 2642, the loss is 2450.99532786213, parameters k is -60.67914217387005 and b is 401.0061649848579\n",
      "Iteration 2643, the loss is 2450.1096222536507, parameters k is -60.6661481027238 and b is 400.9231610322887\n",
      "Iteration 2644, the loss is 2449.2240866206776, parameters k is -60.65315403157756 and b is 400.8401570797195\n",
      "Iteration 2645, the loss is 2448.3159943904416, parameters k is -60.64015996043131 and b is 400.7611056963203\n",
      "Iteration 2646, the loss is 2447.4307881079426, parameters k is -60.62716588928507 and b is 400.67810174375114\n",
      "Iteration 2647, the loss is 2446.54575180094, parameters k is -60.61417181813882 and b is 400.59509779118196\n",
      "Iteration 2648, the loss is 2445.63815834072, parameters k is -60.601177746992576 and b is 400.51604640778277\n",
      "Iteration 2649, the loss is 2444.7534513842015, parameters k is -60.58818367584633 and b is 400.4330424552136\n",
      "Iteration 2650, the loss is 2443.8461973190165, parameters k is -60.575189604700086 and b is 400.3539910718144\n",
      "Iteration 2651, the loss is 2442.961819712968, parameters k is -60.56219553355384 and b is 400.2709871192452\n",
      "Iteration 2652, the loss is 2442.0776120824275, parameters k is -60.549201462407595 and b is 400.187983166676\n",
      "Iteration 2653, the loss is 2441.170856787251, parameters k is -60.53620739126135 and b is 400.10893178327683\n",
      "Iteration 2654, the loss is 2440.286978507188, parameters k is -60.523213320115104 and b is 400.02592783070764\n",
      "Iteration 2655, the loss is 2439.4032702026234, parameters k is -60.51021924896886 and b is 399.94292387813846\n",
      "Iteration 2656, the loss is 2438.497013677465, parameters k is -60.497225177822614 and b is 399.86387249473927\n",
      "Iteration 2657, the loss is 2437.6136347233755, parameters k is -60.48423110667637 and b is 399.7808685421701\n",
      "Iteration 2658, the loss is 2436.730425744788, parameters k is -60.47123703553012 and b is 399.6978645896009\n",
      "Iteration 2659, the loss is 2435.824667989639, parameters k is -60.45824296438388 and b is 399.6188132062017\n",
      "Iteration 2660, the loss is 2434.9417883615306, parameters k is -60.44524889323763 and b is 399.5358092536325\n",
      "Iteration 2661, the loss is 2434.059078708921, parameters k is -60.43225482209139 and b is 399.45280530106334\n",
      "Iteration 2662, the loss is 2433.153819723788, parameters k is -60.41926075094514 and b is 399.37375391766415\n",
      "Iteration 2663, the loss is 2432.2714394216537, parameters k is -60.4062666797989 and b is 399.29074996509496\n",
      "Iteration 2664, the loss is 2431.3665198315553, parameters k is -60.39327260865265 and b is 399.21169858169577\n",
      "Iteration 2665, the loss is 2430.4844688799026, parameters k is -60.380278537506406 and b is 399.1286946291266\n",
      "Iteration 2666, the loss is 2429.602587903744, parameters k is -60.36728446636016 and b is 399.0456906765574\n",
      "Iteration 2667, the loss is 2428.698167083664, parameters k is -60.354290395213916 and b is 398.9666392931582\n",
      "Iteration 2668, the loss is 2427.8166154579794, parameters k is -60.34129632406767 and b is 398.883635340589\n",
      "Iteration 2669, the loss is 2426.9352338078024, parameters k is -60.328302252921425 and b is 398.80063138801984\n",
      "Iteration 2670, the loss is 2426.0313117577375, parameters k is -60.31530818177518 and b is 398.72158000462065\n",
      "Iteration 2671, the loss is 2425.150259458035, parameters k is -60.302314110628934 and b is 398.63857605205146\n",
      "Iteration 2672, the loss is 2424.269377133835, parameters k is -60.28932003948269 and b is 398.5555720994823\n",
      "Iteration 2673, the loss is 2423.365953853779, parameters k is -60.276325968336444 and b is 398.4765207160831\n",
      "Iteration 2674, the loss is 2422.485400880054, parameters k is -60.2633318971902 and b is 398.3935167635139\n",
      "Iteration 2675, the loss is 2421.6050178818327, parameters k is -60.25033782604395 and b is 398.3105128109447\n",
      "Iteration 2676, the loss is 2420.7020933717877, parameters k is -60.23734375489771 and b is 398.2314614275455\n",
      "Iteration 2677, the loss is 2419.822039724044, parameters k is -60.22434968375146 and b is 398.14845747497634\n",
      "Iteration 2678, the loss is 2418.9194546090366, parameters k is -60.21135561260522 and b is 398.06940609157715\n",
      "Iteration 2679, the loss is 2418.0397303117675, parameters k is -60.19836154145897 and b is 397.98640213900796\n",
      "Iteration 2680, the loss is 2417.160175990002, parameters k is -60.18536747031273 and b is 397.9033981864388\n",
      "Iteration 2681, the loss is 2416.258089645007, parameters k is -60.17237339916648 and b is 397.8243468030396\n",
      "Iteration 2682, the loss is 2415.3788646737157, parameters k is -60.159379328020236 and b is 397.7413428504704\n",
      "Iteration 2683, the loss is 2414.4998096779254, parameters k is -60.14638525687399 and b is 397.6583388979012\n",
      "Iteration 2684, the loss is 2413.598222102945, parameters k is -60.133391185727746 and b is 397.579287514502\n",
      "Iteration 2685, the loss is 2412.719496457636, parameters k is -60.1203971145815 and b is 397.49628356193284\n",
      "Iteration 2686, the loss is 2411.840940787819, parameters k is -60.107403043435255 and b is 397.41327960936366\n",
      "Iteration 2687, the loss is 2410.939851982856, parameters k is -60.09440897228901 and b is 397.33422822596447\n",
      "Iteration 2688, the loss is 2410.0616256635185, parameters k is -60.081414901142765 and b is 397.2512242733953\n",
      "Iteration 2689, the loss is 2409.1835693196813, parameters k is -60.06842082999652 and b is 397.1682203208261\n",
      "Iteration 2690, the loss is 2408.2829792847297, parameters k is -60.055426758850274 and b is 397.0891689374269\n",
      "Iteration 2691, the loss is 2407.4052522913685, parameters k is -60.04243268770403 and b is 397.0061649848577\n",
      "Iteration 2692, the loss is 2406.5050016514565, parameters k is -60.02943861655778 and b is 396.92711360145853\n",
      "Iteration 2693, the loss is 2405.6276040085736, parameters k is -60.01644454541154 and b is 396.84410964888934\n",
      "Iteration 2694, the loss is 2404.750376341192, parameters k is -60.00345047426529 and b is 396.76110569632016\n",
      "Iteration 2695, the loss is 2403.8506244712858, parameters k is -59.99045640311905 and b is 396.68205431292097\n",
      "Iteration 2696, the loss is 2402.973726154385, parameters k is -59.9774623319728 and b is 396.5990503603518\n",
      "Iteration 2697, the loss is 2402.0969978129833, parameters k is -59.96446826082656 and b is 396.5160464077826\n",
      "Iteration 2698, the loss is 2401.197744713091, parameters k is -59.95147418968031 and b is 396.4369950243834\n",
      "Iteration 2699, the loss is 2400.3213457221686, parameters k is -59.938480118534066 and b is 396.3539910718142\n",
      "Iteration 2700, the loss is 2399.4451167067423, parameters k is -59.92548604738782 and b is 396.27098711924504\n",
      "Iteration 2701, the loss is 2398.546362376865, parameters k is -59.912491976241576 and b is 396.19193573584585\n",
      "Iteration 2702, the loss is 2397.670462711918, parameters k is -59.89949790509533 and b is 396.10893178327666\n",
      "Iteration 2703, the loss is 2396.7720477770786, parameters k is -59.886503833949085 and b is 396.02988039987747\n",
      "Iteration 2704, the loss is 2395.8964774626047, parameters k is -59.87350976280284 and b is 395.9468764473083\n",
      "Iteration 2705, the loss is 2395.021077123637, parameters k is -59.860515691656595 and b is 395.8638724947391\n",
      "Iteration 2706, the loss is 2394.1231609588103, parameters k is -59.84752162051035 and b is 395.7848211113399\n",
      "Iteration 2707, the loss is 2393.248089970314, parameters k is -59.834527549364104 and b is 395.7018171587707\n",
      "Iteration 2708, the loss is 2392.373188957322, parameters k is -59.82153347821786 and b is 395.61881320620154\n",
      "Iteration 2709, the loss is 2391.4757715625115, parameters k is -59.80853940707161 and b is 395.53976182280235\n",
      "Iteration 2710, the loss is 2390.6011998999934, parameters k is -59.79554533592537 and b is 395.45675787023316\n",
      "Iteration 2711, the loss is 2389.726798212976, parameters k is -59.78255126477912 and b is 395.373753917664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2712, the loss is 2388.8298795881765, parameters k is -59.76955719363288 and b is 395.2947025342648\n",
      "Iteration 2713, the loss is 2387.9558072516397, parameters k is -59.75656312248663 and b is 395.2116985816956\n",
      "Iteration 2714, the loss is 2387.0819048906014, parameters k is -59.74356905134039 and b is 395.1286946291264\n",
      "Iteration 2715, the loss is 2386.1854850358136, parameters k is -59.73057498019414 and b is 395.0496432457272\n",
      "Iteration 2716, the loss is 2385.311912025255, parameters k is -59.717580909047896 and b is 394.96663929315804\n",
      "Iteration 2717, the loss is 2384.4158315655027, parameters k is -59.70458683790165 and b is 394.88758790975885\n",
      "Iteration 2718, the loss is 2383.542587905422, parameters k is -59.691592766755406 and b is 394.80458395718966\n",
      "Iteration 2719, the loss is 2382.669514220839, parameters k is -59.67859869560916 and b is 394.7215800046205\n",
      "Iteration 2720, the loss is 2381.7739325310995, parameters k is -59.665604624462915 and b is 394.6425286212213\n",
      "Iteration 2721, the loss is 2380.9011881969955, parameters k is -59.65261055331667 and b is 394.5595246686521\n",
      "Iteration 2722, the loss is 2380.02861383839, parameters k is -59.639616482170425 and b is 394.4765207160829\n",
      "Iteration 2723, the loss is 2379.133530918669, parameters k is -59.62662241102418 and b is 394.3974693326837\n",
      "Iteration 2724, the loss is 2378.2612859105407, parameters k is -59.613628339877934 and b is 394.31446538011454\n",
      "Iteration 2725, the loss is 2377.3892108779087, parameters k is -59.60063426873169 and b is 394.23146142754536\n",
      "Iteration 2726, the loss is 2376.4946267281994, parameters k is -59.58764019758544 and b is 394.15241004414617\n",
      "Iteration 2727, the loss is 2375.6228810460493, parameters k is -59.5746461264392 and b is 394.069406091577\n",
      "Iteration 2728, the loss is 2374.7513053394005, parameters k is -59.56165205529295 and b is 393.9864021390078\n",
      "Iteration 2729, the loss is 2373.857219959707, parameters k is -59.54865798414671 and b is 393.9073507556086\n",
      "Iteration 2730, the loss is 2372.9859736035305, parameters k is -59.53566391300046 and b is 393.8243468030394\n",
      "Iteration 2731, the loss is 2372.0922276188685, parameters k is -59.52266984185422 and b is 393.74529541964023\n",
      "Iteration 2732, the loss is 2371.243990817683, parameters k is -59.50967577070797 and b is 393.6583388979011\n",
      "Iteration 2733, the loss is 2370.350563582978, parameters k is -59.49668169956173 and b is 393.5792875145019\n",
      "Iteration 2734, the loss is 2369.4799865282835, parameters k is -59.48368762841548 and b is 393.4962835619327\n",
      "Iteration 2735, the loss is 2368.6095794490834, parameters k is -59.470693557269236 and b is 393.41327960936354\n",
      "Iteration 2736, the loss is 2367.7393423453914, parameters k is -59.45769948612299 and b is 393.33027565679436\n",
      "Iteration 2737, the loss is 2366.8692752171983, parameters k is -59.444705414976745 and b is 393.24727170422517\n",
      "Iteration 2738, the loss is 2365.999378064502, parameters k is -59.4317113438305 and b is 393.164267751656\n",
      "Iteration 2739, the loss is 2365.129650887307, parameters k is -59.418717272684255 and b is 393.0812637990868\n",
      "Iteration 2740, the loss is 2364.2600936856134, parameters k is -59.40572320153801 and b is 392.9982598465176\n",
      "Iteration 2741, the loss is 2363.390706459418, parameters k is -59.392729130391764 and b is 392.9152558939484\n",
      "Iteration 2742, the loss is 2362.5214892087233, parameters k is -59.37973505924552 and b is 392.83225194137924\n",
      "Iteration 2743, the loss is 2361.652441933529, parameters k is -59.366740988099274 and b is 392.74924798881005\n",
      "Iteration 2744, the loss is 2360.7835646338394, parameters k is -59.35374691695303 and b is 392.66624403624087\n",
      "Iteration 2745, the loss is 2359.914857309651, parameters k is -59.34075284580678 and b is 392.5832400836717\n",
      "Iteration 2746, the loss is 2359.046319960954, parameters k is -59.32775877466054 and b is 392.5002361311025\n",
      "Iteration 2747, the loss is 2358.177952587762, parameters k is -59.31476470351429 and b is 392.4172321785333\n",
      "Iteration 2748, the loss is 2357.309755190073, parameters k is -59.30177063236805 and b is 392.3342282259641\n",
      "Iteration 2749, the loss is 2356.4417277678845, parameters k is -59.2887765612218 and b is 392.25122427339494\n",
      "Iteration 2750, the loss is 2355.573870321192, parameters k is -59.27578249007556 and b is 392.16822032082575\n",
      "Iteration 2751, the loss is 2354.706182850002, parameters k is -59.26278841892931 and b is 392.08521636825657\n",
      "Iteration 2752, the loss is 2353.8386653543103, parameters k is -59.249794347783066 and b is 392.0022124156874\n",
      "Iteration 2753, the loss is 2352.9713178341253, parameters k is -59.23680027663682 and b is 391.9192084631182\n",
      "Iteration 2754, the loss is 2352.1041402894343, parameters k is -59.223806205490575 and b is 391.836204510549\n",
      "Iteration 2755, the loss is 2351.2371327202413, parameters k is -59.21081213434433 and b is 391.7532005579798\n",
      "Iteration 2756, the loss is 2350.370295126554, parameters k is -59.197818063198085 and b is 391.67019660541064\n",
      "Iteration 2757, the loss is 2349.503627508364, parameters k is -59.18482399205184 and b is 391.58719265284145\n",
      "Iteration 2758, the loss is 2348.6371298656786, parameters k is -59.171829920905594 and b is 391.50418870027227\n",
      "Iteration 2759, the loss is 2347.7708021984918, parameters k is -59.15883584975935 and b is 391.4211847477031\n",
      "Iteration 2760, the loss is 2346.9046445068007, parameters k is -59.145841778613104 and b is 391.3381807951339\n",
      "Iteration 2761, the loss is 2346.038656790614, parameters k is -59.13284770746686 and b is 391.2551768425647\n",
      "Iteration 2762, the loss is 2345.1728390499266, parameters k is -59.11985363632061 and b is 391.1721728899955\n",
      "Iteration 2763, the loss is 2344.3071912847413, parameters k is -59.10685956517437 and b is 391.08916893742634\n",
      "Iteration 2764, the loss is 2343.441713495055, parameters k is -59.09386549402812 and b is 391.00616498485715\n",
      "Iteration 2765, the loss is 2342.5764056808684, parameters k is -59.08087142288188 and b is 390.92316103228796\n",
      "Iteration 2766, the loss is 2341.7112678421845, parameters k is -59.06787735173563 and b is 390.8401570797188\n",
      "Iteration 2767, the loss is 2340.8462999790027, parameters k is -59.05488328058939 and b is 390.7571531271496\n",
      "Iteration 2768, the loss is 2339.981502091314, parameters k is -59.04188920944314 and b is 390.6741491745804\n",
      "Iteration 2769, the loss is 2339.1168741791275, parameters k is -59.028895138296896 and b is 390.5911452220112\n",
      "Iteration 2770, the loss is 2338.252416242445, parameters k is -59.01590106715065 and b is 390.50814126944204\n",
      "Iteration 2771, the loss is 2337.3881282812613, parameters k is -59.002906996004405 and b is 390.42513731687285\n",
      "Iteration 2772, the loss is 2336.524010295577, parameters k is -58.98991292485816 and b is 390.34213336430366\n",
      "Iteration 2773, the loss is 2335.6600622853907, parameters k is -58.976918853711915 and b is 390.2591294117345\n",
      "Iteration 2774, the loss is 2334.7962842507127, parameters k is -58.96392478256567 and b is 390.1761254591653\n",
      "Iteration 2775, the loss is 2333.9326761915268, parameters k is -58.950930711419424 and b is 390.0931215065961\n",
      "Iteration 2776, the loss is 2333.0692381078475, parameters k is -58.93793664027318 and b is 390.0101175540269\n",
      "Iteration 2777, the loss is 2332.2059699996635, parameters k is -58.924942569126934 and b is 389.92711360145773\n",
      "Iteration 2778, the loss is 2331.342871866982, parameters k is -58.91194849798069 and b is 389.84410964888855\n",
      "Iteration 2779, the loss is 2330.4799437098013, parameters k is -58.89895442683444 and b is 389.76110569631936\n",
      "Iteration 2780, the loss is 2329.617185528119, parameters k is -58.8859603556882 and b is 389.6781017437502\n",
      "Iteration 2781, the loss is 2328.754597321939, parameters k is -58.87296628454195 and b is 389.595097791181\n",
      "Iteration 2782, the loss is 2327.8921790912577, parameters k is -58.85997221339571 and b is 389.5120938386118\n",
      "Iteration 2783, the loss is 2327.029930836077, parameters k is -58.84697814224946 and b is 389.4290898860426\n",
      "Iteration 2784, the loss is 2326.167852556396, parameters k is -58.83398407110322 and b is 389.34608593347343\n",
      "Iteration 2785, the loss is 2325.305944252217, parameters k is -58.82098999995697 and b is 389.26308198090425\n",
      "Iteration 2786, the loss is 2324.444205923541, parameters k is -58.807995928810726 and b is 389.18007802833506\n",
      "Iteration 2787, the loss is 2323.582637570361, parameters k is -58.79500185766448 and b is 389.0970740757659\n",
      "Iteration 2788, the loss is 2322.7212391926837, parameters k is -58.782007786518236 and b is 389.0140701231967\n",
      "Iteration 2789, the loss is 2321.8600107905036, parameters k is -58.76901371537199 and b is 388.9310661706275\n",
      "Iteration 2790, the loss is 2320.9989523638274, parameters k is -58.756019644225745 and b is 388.8480622180583\n",
      "Iteration 2791, the loss is 2320.1380639126464, parameters k is -58.7430255730795 and b is 388.76505826548913\n",
      "Iteration 2792, the loss is 2319.277345436974, parameters k is -58.730031501933254 and b is 388.68205431291995\n",
      "Iteration 2793, the loss is 2318.4167969367954, parameters k is -58.71703743078701 and b is 388.59905036035076\n",
      "Iteration 2794, the loss is 2317.556418412118, parameters k is -58.704043359640764 and b is 388.5160464077816\n",
      "Iteration 2795, the loss is 2316.6962098629438, parameters k is -58.69104928849452 and b is 388.4330424552124\n",
      "Iteration 2796, the loss is 2315.8361712892674, parameters k is -58.67805521734827 and b is 388.3500385026432\n",
      "Iteration 2797, the loss is 2314.9763026910896, parameters k is -58.66506114620203 and b is 388.267034550074\n",
      "Iteration 2798, the loss is 2314.116604068412, parameters k is -58.65206707505578 and b is 388.18403059750483\n",
      "Iteration 2799, the loss is 2313.2570754212397, parameters k is -58.63907300390954 and b is 388.10102664493564\n",
      "Iteration 2800, the loss is 2312.397716749567, parameters k is -58.62607893276329 and b is 388.01802269236646\n",
      "Iteration 2801, the loss is 2311.5385280533933, parameters k is -58.61308486161705 and b is 387.9350187397973\n",
      "Iteration 2802, the loss is 2310.6795093327173, parameters k is -58.6000907904708 and b is 387.8520147872281\n",
      "Iteration 2803, the loss is 2309.8206605875444, parameters k is -58.587096719324556 and b is 387.7690108346589\n",
      "Iteration 2804, the loss is 2308.9619818178708, parameters k is -58.57410264817831 and b is 387.6860068820897\n",
      "Iteration 2805, the loss is 2308.103473023698, parameters k is -58.561108577032066 and b is 387.6030029295205\n",
      "Iteration 2806, the loss is 2307.245134205025, parameters k is -58.54811450588582 and b is 387.51999897695134\n",
      "Iteration 2807, the loss is 2306.3869653618526, parameters k is -58.535120434739575 and b is 387.43699502438216\n",
      "Iteration 2808, the loss is 2305.5289664941806, parameters k is -58.52212636359333 and b is 387.35399107181297\n",
      "Iteration 2809, the loss is 2304.6711376020094, parameters k is -58.509132292447084 and b is 387.2709871192438\n",
      "Iteration 2810, the loss is 2303.813478685335, parameters k is -58.49613822130084 and b is 387.1879831666746\n",
      "Iteration 2811, the loss is 2302.9559897441695, parameters k is -58.483144150154594 and b is 387.1049792141054\n",
      "Iteration 2812, the loss is 2302.0986707784964, parameters k is -58.47015007900835 and b is 387.0219752615362\n",
      "Iteration 2813, the loss is 2301.2415217883263, parameters k is -58.4571560078621 and b is 386.93897130896704\n",
      "Iteration 2814, the loss is 2300.3845427736596, parameters k is -58.44416193671586 and b is 386.85596735639785\n",
      "Iteration 2815, the loss is 2299.527733734487, parameters k is -58.43116786556961 and b is 386.77296340382867\n",
      "Iteration 2816, the loss is 2298.671094670816, parameters k is -58.41817379442337 and b is 386.6899594512595\n",
      "Iteration 2817, the loss is 2297.8146255826473, parameters k is -58.40517972327712 and b is 386.6069554986903\n",
      "Iteration 2818, the loss is 2296.9583264699763, parameters k is -58.39218565213088 and b is 386.5239515461211\n",
      "Iteration 2819, the loss is 2296.1021973328106, parameters k is -58.37919158098463 and b is 386.4409475935519\n",
      "Iteration 2820, the loss is 2295.2462381711402, parameters k is -58.366197509838386 and b is 386.35794364098274\n",
      "Iteration 2821, the loss is 2294.3904489849747, parameters k is -58.35320343869214 and b is 386.27493968841355\n",
      "Iteration 2822, the loss is 2293.534829774304, parameters k is -58.340209367545896 and b is 386.19193573584437\n",
      "Iteration 2823, the loss is 2292.679380539142, parameters k is -58.32721529639965 and b is 386.1089317832752\n",
      "Iteration 2824, the loss is 2291.824101279472, parameters k is -58.314221225253405 and b is 386.025927830706\n",
      "Iteration 2825, the loss is 2290.9689919953043, parameters k is -58.30122715410716 and b is 385.9429238781368\n",
      "Iteration 2826, the loss is 2290.1140526866375, parameters k is -58.288233082960915 and b is 385.8599199255676\n",
      "Iteration 2827, the loss is 2289.2592833534736, parameters k is -58.27523901181467 and b is 385.77691597299844\n",
      "Iteration 2828, the loss is 2288.4046839958096, parameters k is -58.262244940668424 and b is 385.69391202042925\n",
      "Iteration 2829, the loss is 2287.550254613645, parameters k is -58.24925086952218 and b is 385.61090806786007\n",
      "Iteration 2830, the loss is 2286.6959952069788, parameters k is -58.23625679837593 and b is 385.5279041152909\n",
      "Iteration 2831, the loss is 2285.8419057758165, parameters k is -58.22326272722969 and b is 385.4449001627217\n",
      "Iteration 2832, the loss is 2284.9879863201486, parameters k is -58.21026865608344 and b is 385.3618962101525\n",
      "Iteration 2833, the loss is 2284.134236839985, parameters k is -58.1972745849372 and b is 385.2788922575833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2834, the loss is 2283.28065733532, parameters k is -58.18428051379095 and b is 385.19588830501414\n",
      "Iteration 2835, the loss is 2282.4272478061594, parameters k is -58.17128644264471 and b is 385.11288435244495\n",
      "Iteration 2836, the loss is 2281.5740082524935, parameters k is -58.15829237149846 and b is 385.02988039987576\n",
      "Iteration 2837, the loss is 2280.720938674332, parameters k is -58.145298300352216 and b is 384.9468764473066\n",
      "Iteration 2838, the loss is 2279.868039071669, parameters k is -58.13230422920597 and b is 384.8638724947374\n",
      "Iteration 2839, the loss is 2279.015309444509, parameters k is -58.119310158059726 and b is 384.7808685421682\n",
      "Iteration 2840, the loss is 2278.1627497928475, parameters k is -58.10631608691348 and b is 384.697864589599\n",
      "Iteration 2841, the loss is 2277.3103601166854, parameters k is -58.093322015767235 and b is 384.61486063702984\n",
      "Iteration 2842, the loss is 2276.4581404160253, parameters k is -58.08032794462099 and b is 384.53185668446065\n",
      "Iteration 2843, the loss is 2275.6060906908633, parameters k is -58.067333873474745 and b is 384.44885273189146\n",
      "Iteration 2844, the loss is 2274.754210941203, parameters k is -58.0543398023285 and b is 384.3658487793223\n",
      "Iteration 2845, the loss is 2273.9025011670433, parameters k is -58.041345731182254 and b is 384.2828448267531\n",
      "Iteration 2846, the loss is 2273.050961368381, parameters k is -58.02835166003601 and b is 384.1998408741839\n",
      "Iteration 2847, the loss is 2272.199591545221, parameters k is -58.01535758888976 and b is 384.1168369216147\n",
      "Iteration 2848, the loss is 2271.3483916975633, parameters k is -58.00236351774352 and b is 384.03383296904553\n",
      "Iteration 2849, the loss is 2270.4973618254044, parameters k is -57.98936944659727 and b is 383.95082901647635\n",
      "Iteration 2850, the loss is 2269.646501928745, parameters k is -57.97637537545103 and b is 383.86782506390716\n",
      "Iteration 2851, the loss is 2268.7958120075887, parameters k is -57.96338130430478 and b is 383.784821111338\n",
      "Iteration 2852, the loss is 2267.945292061927, parameters k is -57.95038723315854 and b is 383.7018171587688\n",
      "Iteration 2853, the loss is 2267.094942091773, parameters k is -57.93739316201229 and b is 383.6188132061996\n",
      "Iteration 2854, the loss is 2266.244762097116, parameters k is -57.924399090866046 and b is 383.5358092536304\n",
      "Iteration 2855, the loss is 2265.394752077957, parameters k is -57.9114050197198 and b is 383.45280530106123\n",
      "Iteration 2856, the loss is 2264.5449120343, parameters k is -57.898410948573556 and b is 383.36980134849205\n",
      "Iteration 2857, the loss is 2263.6952419661434, parameters k is -57.88541687742731 and b is 383.28679739592286\n",
      "Iteration 2858, the loss is 2262.845741873489, parameters k is -57.872422806281065 and b is 383.2037934433537\n",
      "Iteration 2859, the loss is 2261.996411756336, parameters k is -57.85942873513482 and b is 383.1207894907845\n",
      "Iteration 2860, the loss is 2261.147251614678, parameters k is -57.846434663988575 and b is 383.0377855382153\n",
      "Iteration 2861, the loss is 2260.298261448523, parameters k is -57.83344059284233 and b is 382.9547815856461\n",
      "Iteration 2862, the loss is 2259.4494412578715, parameters k is -57.820446521696084 and b is 382.87177763307693\n",
      "Iteration 2863, the loss is 2258.600791042715, parameters k is -57.80745245054984 and b is 382.78877368050775\n",
      "Iteration 2864, the loss is 2257.752310803062, parameters k is -57.79445837940359 and b is 382.70576972793856\n",
      "Iteration 2865, the loss is 2256.904000538904, parameters k is -57.78146430825735 and b is 382.6227657753694\n",
      "Iteration 2866, the loss is 2256.055860250254, parameters k is -57.7684702371111 and b is 382.5397618228002\n",
      "Iteration 2867, the loss is 2255.2078899371013, parameters k is -57.75547616596486 and b is 382.456757870231\n",
      "Iteration 2868, the loss is 2254.3600895994487, parameters k is -57.74248209481861 and b is 382.3737539176618\n",
      "Iteration 2869, the loss is 2253.5124592372945, parameters k is -57.72948802367237 and b is 382.29074996509263\n",
      "Iteration 2870, the loss is 2252.6649988506433, parameters k is -57.71649395252612 and b is 382.20774601252344\n",
      "Iteration 2871, the loss is 2251.8177084394943, parameters k is -57.70349988137988 and b is 382.12474205995426\n",
      "Iteration 2872, the loss is 2250.9705880038414, parameters k is -57.69050581023363 and b is 382.0417381073851\n",
      "Iteration 2873, the loss is 2250.12363754369, parameters k is -57.677511739087386 and b is 381.9587341548159\n",
      "Iteration 2874, the loss is 2249.2768570590442, parameters k is -57.66451766794114 and b is 381.8757302022467\n",
      "Iteration 2875, the loss is 2248.4302465498895, parameters k is -57.651523596794895 and b is 381.7927262496775\n",
      "Iteration 2876, the loss is 2247.5838060162387, parameters k is -57.63852952564865 and b is 381.7097222971083\n",
      "Iteration 2877, the loss is 2246.737535458091, parameters k is -57.625535454502405 and b is 381.62671834453914\n",
      "Iteration 2878, the loss is 2245.8914348754392, parameters k is -57.61254138335616 and b is 381.54371439196996\n",
      "Iteration 2879, the loss is 2245.0455042682916, parameters k is -57.599547312209914 and b is 381.46071043940077\n",
      "Iteration 2880, the loss is 2244.1997436366405, parameters k is -57.58655324106367 and b is 381.3777064868316\n",
      "Iteration 2881, the loss is 2243.3541529804884, parameters k is -57.573559169917424 and b is 381.2947025342624\n",
      "Iteration 2882, the loss is 2242.5087322998406, parameters k is -57.56056509877118 and b is 381.2116985816932\n",
      "Iteration 2883, the loss is 2241.663481594696, parameters k is -57.54757102762493 and b is 381.128694629124\n",
      "Iteration 2884, the loss is 2240.818400865049, parameters k is -57.53457695647869 and b is 381.04569067655484\n",
      "Iteration 2885, the loss is 2239.9734901109005, parameters k is -57.52158288533244 and b is 380.96268672398566\n",
      "Iteration 2886, the loss is 2239.128749332258, parameters k is -57.5085888141862 and b is 380.87968277141647\n",
      "Iteration 2887, the loss is 2238.2841785291075, parameters k is -57.49559474303995 and b is 380.7966788188473\n",
      "Iteration 2888, the loss is 2237.439777701462, parameters k is -57.48260067189371 and b is 380.7136748662781\n",
      "Iteration 2889, the loss is 2236.5955468493166, parameters k is -57.46960660074746 and b is 380.6306709137089\n",
      "Iteration 2890, the loss is 2235.751485972669, parameters k is -57.456612529601216 and b is 380.5476669611397\n",
      "Iteration 2891, the loss is 2234.907595071525, parameters k is -57.44361845845497 and b is 380.46466300857054\n",
      "Iteration 2892, the loss is 2234.063874145884, parameters k is -57.430624387308725 and b is 380.38165905600135\n",
      "Iteration 2893, the loss is 2233.220323195736, parameters k is -57.41763031616248 and b is 380.29865510343217\n",
      "Iteration 2894, the loss is 2232.37694222109, parameters k is -57.404636245016235 and b is 380.215651150863\n",
      "Iteration 2895, the loss is 2231.533731221949, parameters k is -57.39164217386999 and b is 380.1326471982938\n",
      "Iteration 2896, the loss is 2230.690690198304, parameters k is -57.378648102723744 and b is 380.0496432457246\n",
      "Iteration 2897, the loss is 2229.8478191501595, parameters k is -57.3656540315775 and b is 379.9666392931554\n",
      "Iteration 2898, the loss is 2229.005118077518, parameters k is -57.352659960431254 and b is 379.88363534058624\n",
      "Iteration 2899, the loss is 2228.162586980374, parameters k is -57.33966588928501 and b is 379.80063138801705\n",
      "Iteration 2900, the loss is 2227.3202258587316, parameters k is -57.32667181813876 and b is 379.71762743544787\n",
      "Iteration 2901, the loss is 2226.478034712593, parameters k is -57.31367774699252 and b is 379.6346234828787\n",
      "Iteration 2902, the loss is 2225.636013541946, parameters k is -57.30068367584627 and b is 379.5516195303095\n",
      "Iteration 2903, the loss is 2224.7941623468087, parameters k is -57.28768960470003 and b is 379.4686155777403\n",
      "Iteration 2904, the loss is 2223.9524811271663, parameters k is -57.27469553355378 and b is 379.3856116251711\n",
      "Iteration 2905, the loss is 2223.110969883026, parameters k is -57.26170146240754 and b is 379.30260767260194\n",
      "Iteration 2906, the loss is 2222.2696286143855, parameters k is -57.24870739126129 and b is 379.21960372003275\n",
      "Iteration 2907, the loss is 2221.4284573212462, parameters k is -57.235713320115046 and b is 379.13659976746357\n",
      "Iteration 2908, the loss is 2220.587456003603, parameters k is -57.2227192489688 and b is 379.0535958148944\n",
      "Iteration 2909, the loss is 2219.746624661466, parameters k is -57.209725177822556 and b is 378.9705918623252\n",
      "Iteration 2910, the loss is 2218.905963294821, parameters k is -57.19673110667631 and b is 378.887587909756\n",
      "Iteration 2911, the loss is 2218.0654719036866, parameters k is -57.183737035530065 and b is 378.8045839571868\n",
      "Iteration 2912, the loss is 2217.225150488049, parameters k is -57.17074296438382 and b is 378.72158000461764\n",
      "Iteration 2913, the loss is 2216.38499904791, parameters k is -57.157748893237574 and b is 378.63857605204845\n",
      "Iteration 2914, the loss is 2215.5450175832702, parameters k is -57.14475482209133 and b is 378.55557209947926\n",
      "Iteration 2915, the loss is 2214.7052060941364, parameters k is -57.131760750945084 and b is 378.4725681469101\n",
      "Iteration 2916, the loss is 2213.8655645804993, parameters k is -57.11876667979884 and b is 378.3895641943409\n",
      "Iteration 2917, the loss is 2213.0260930423574, parameters k is -57.10577260865259 and b is 378.3065602417717\n",
      "Iteration 2918, the loss is 2212.186791479723, parameters k is -57.09277853750635 and b is 378.2235562892025\n",
      "Iteration 2919, the loss is 2211.347659892589, parameters k is -57.0797844663601 and b is 378.14055233663333\n",
      "Iteration 2920, the loss is 2210.508698280952, parameters k is -57.06679039521386 and b is 378.05754838406415\n",
      "Iteration 2921, the loss is 2209.6699066448195, parameters k is -57.05379632406761 and b is 377.97454443149496\n",
      "Iteration 2922, the loss is 2208.83128498418, parameters k is -57.04080225292137 and b is 377.8915404789258\n",
      "Iteration 2923, the loss is 2207.9928332990485, parameters k is -57.02780818177512 and b is 377.8085365263566\n",
      "Iteration 2924, the loss is 2207.154551589412, parameters k is -57.014814110628876 and b is 377.7255325737874\n",
      "Iteration 2925, the loss is 2206.316439855279, parameters k is -57.00182003948263 and b is 377.6425286212182\n",
      "Iteration 2926, the loss is 2205.478498096641, parameters k is -56.988825968336386 and b is 377.55952466864903\n",
      "Iteration 2927, the loss is 2204.640726313509, parameters k is -56.97583189719014 and b is 377.47652071607985\n",
      "Iteration 2928, the loss is 2203.8031245058746, parameters k is -56.962837826043895 and b is 377.39351676351066\n",
      "Iteration 2929, the loss is 2202.9656926737434, parameters k is -56.94984375489765 and b is 377.3105128109415\n",
      "Iteration 2930, the loss is 2202.128430817111, parameters k is -56.936849683751404 and b is 377.2275088583723\n",
      "Iteration 2931, the loss is 2201.2913389359774, parameters k is -56.92385561260516 and b is 377.1445049058031\n",
      "Iteration 2932, the loss is 2200.454417030345, parameters k is -56.910861541458914 and b is 377.0615009532339\n",
      "Iteration 2933, the loss is 2199.6176651002133, parameters k is -56.89786747031267 and b is 376.97849700066473\n",
      "Iteration 2934, the loss is 2198.7810831455827, parameters k is -56.88487339916642 and b is 376.89549304809555\n",
      "Iteration 2935, the loss is 2197.9446711664496, parameters k is -56.87187932802018 and b is 376.81248909552636\n",
      "Iteration 2936, the loss is 2197.1084291628204, parameters k is -56.85888525687393 and b is 376.7294851429572\n",
      "Iteration 2937, the loss is 2196.2723571346933, parameters k is -56.84589118572769 and b is 376.646481190388\n",
      "Iteration 2938, the loss is 2195.43645508206, parameters k is -56.83289711458144 and b is 376.5634772378188\n",
      "Iteration 2939, the loss is 2194.6007230049277, parameters k is -56.8199030434352 and b is 376.4804732852496\n",
      "Iteration 2940, the loss is 2193.765160903302, parameters k is -56.80690897228895 and b is 376.39746933268043\n",
      "Iteration 2941, the loss is 2192.929768777173, parameters k is -56.793914901142706 and b is 376.31446538011124\n",
      "Iteration 2942, the loss is 2192.094546626541, parameters k is -56.78092082999646 and b is 376.23146142754206\n",
      "Iteration 2943, the loss is 2191.2594944514126, parameters k is -56.767926758850216 and b is 376.1484574749729\n",
      "Iteration 2944, the loss is 2190.4246122517848, parameters k is -56.75493268770397 and b is 376.0654535224037\n",
      "Iteration 2945, the loss is 2189.5899000276586, parameters k is -56.741938616557725 and b is 375.9824495698345\n",
      "Iteration 2946, the loss is 2188.755357779028, parameters k is -56.72894454541148 and b is 375.8994456172653\n",
      "Iteration 2947, the loss is 2187.9209855059016, parameters k is -56.715950474265235 and b is 375.81644166469613\n",
      "Iteration 2948, the loss is 2187.0867832082768, parameters k is -56.70295640311899 and b is 375.73343771212694\n",
      "Iteration 2949, the loss is 2186.2527508861485, parameters k is -56.689962331972744 and b is 375.65043375955776\n",
      "Iteration 2950, the loss is 2185.4188885395197, parameters k is -56.6769682608265 and b is 375.56742980698857\n",
      "Iteration 2951, the loss is 2184.5851961683948, parameters k is -56.66397418968025 and b is 375.4844258544194\n",
      "Iteration 2952, the loss is 2183.7516737727756, parameters k is -56.65098011853401 and b is 375.4014219018502\n",
      "Iteration 2953, the loss is 2182.918321352644, parameters k is -56.63798604738776 and b is 375.318417949281\n",
      "Iteration 2954, the loss is 2182.0851389080203, parameters k is -56.62499197624152 and b is 375.2354139967118\n",
      "Iteration 2955, the loss is 2181.252126438896, parameters k is -56.61199790509527 and b is 375.15241004414264\n",
      "Iteration 2956, the loss is 2180.419283945272, parameters k is -56.59900383394903 and b is 375.06940609157346\n",
      "Iteration 2957, the loss is 2179.5866114271494, parameters k is -56.58600976280278 and b is 374.98640213900427\n",
      "Iteration 2958, the loss is 2178.7541088845232, parameters k is -56.573015691656536 and b is 374.9033981864351\n",
      "Iteration 2959, the loss is 2177.921776317402, parameters k is -56.56002162051029 and b is 374.8203942338659\n",
      "Iteration 2960, the loss is 2177.089613725778, parameters k is -56.547027549364046 and b is 374.7373902812967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2961, the loss is 2176.2576211096552, parameters k is -56.5340334782178 and b is 374.6543863287275\n",
      "Iteration 2962, the loss is 2175.425798469033, parameters k is -56.521039407071555 and b is 374.57138237615834\n",
      "Iteration 2963, the loss is 2174.594145803911, parameters k is -56.50804533592531 and b is 374.48837842358915\n",
      "Iteration 2964, the loss is 2173.7626631142894, parameters k is -56.495051264779065 and b is 374.40537447101997\n",
      "Iteration 2965, the loss is 2172.9313504001666, parameters k is -56.48205719363282 and b is 374.3223705184508\n",
      "Iteration 2966, the loss is 2172.100207661547, parameters k is -56.469063122486574 and b is 374.2393665658816\n",
      "Iteration 2967, the loss is 2171.269234898426, parameters k is -56.45606905134033 and b is 374.1563626133124\n",
      "Iteration 2968, the loss is 2170.4384321108064, parameters k is -56.44307498019408 and b is 374.0733586607432\n",
      "Iteration 2969, the loss is 2169.6077992986848, parameters k is -56.43008090904784 and b is 373.99035470817404\n",
      "Iteration 2970, the loss is 2168.777336462068, parameters k is -56.41708683790159 and b is 373.90735075560485\n",
      "Iteration 2971, the loss is 2167.9470436009433, parameters k is -56.40409276675535 and b is 373.82434680303567\n",
      "Iteration 2972, the loss is 2167.1169207153243, parameters k is -56.3910986956091 and b is 373.7413428504665\n",
      "Iteration 2973, the loss is 2166.286967805206, parameters k is -56.37810462446286 and b is 373.6583388978973\n",
      "Iteration 2974, the loss is 2165.4571848705878, parameters k is -56.36511055331661 and b is 373.5753349453281\n",
      "Iteration 2975, the loss is 2164.6275719114733, parameters k is -56.352116482170366 and b is 373.4923309927589\n",
      "Iteration 2976, the loss is 2163.7981289278537, parameters k is -56.33912241102412 and b is 373.40932704018974\n",
      "Iteration 2977, the loss is 2162.9688559197357, parameters k is -56.326128339877876 and b is 373.32632308762055\n",
      "Iteration 2978, the loss is 2162.1397528871157, parameters k is -56.31313426873163 and b is 373.24331913505137\n",
      "Iteration 2979, the loss is 2161.3108198300006, parameters k is -56.300140197585385 and b is 373.1603151824822\n",
      "Iteration 2980, the loss is 2160.482056748384, parameters k is -56.28714612643914 and b is 373.077311229913\n",
      "Iteration 2981, the loss is 2159.653463642269, parameters k is -56.274152055292895 and b is 372.9943072773438\n",
      "Iteration 2982, the loss is 2158.825040511653, parameters k is -56.26115798414665 and b is 372.9113033247746\n",
      "Iteration 2983, the loss is 2157.9967873565347, parameters k is -56.248163913000404 and b is 372.82829937220544\n",
      "Iteration 2984, the loss is 2157.168704176921, parameters k is -56.23516984185416 and b is 372.74529541963625\n",
      "Iteration 2985, the loss is 2156.340790972807, parameters k is -56.22217577070791 and b is 372.66229146706706\n",
      "Iteration 2986, the loss is 2155.513047744192, parameters k is -56.20918169956167 and b is 372.5792875144979\n",
      "Iteration 2987, the loss is 2154.685474491079, parameters k is -56.19618762841542 and b is 372.4962835619287\n",
      "Iteration 2988, the loss is 2153.8580712134626, parameters k is -56.18319355726918 and b is 372.4132796093595\n",
      "Iteration 2989, the loss is 2153.0308379113476, parameters k is -56.17019948612293 and b is 372.3302756567903\n",
      "Iteration 2990, the loss is 2152.203774584736, parameters k is -56.15720541497669 and b is 372.24727170422113\n",
      "Iteration 2991, the loss is 2151.376881233621, parameters k is -56.14421134383044 and b is 372.16426775165195\n",
      "Iteration 2992, the loss is 2150.550157858009, parameters k is -56.1312172726842 and b is 372.08126379908276\n",
      "Iteration 2993, the loss is 2149.7236044578967, parameters k is -56.11822320153795 and b is 371.9982598465136\n",
      "Iteration 2994, the loss is 2148.897221033288, parameters k is -56.105229130391706 and b is 371.9152558939444\n",
      "Iteration 2995, the loss is 2148.0710075841753, parameters k is -56.09223505924546 and b is 371.8322519413752\n",
      "Iteration 2996, the loss is 2147.2449641105623, parameters k is -56.079240988099215 and b is 371.749247988806\n",
      "Iteration 2997, the loss is 2146.419090612452, parameters k is -56.06624691695297 and b is 371.66624403623683\n",
      "Iteration 2998, the loss is 2145.5933870898416, parameters k is -56.053252845806725 and b is 371.58324008366765\n",
      "Iteration 2999, the loss is 2144.767853542729, parameters k is -56.04025877466048 and b is 371.50023613109846\n",
      "Iteration 3000, the loss is 2143.9424899711194, parameters k is -56.027264703514234 and b is 371.4172321785293\n",
      "Iteration 3001, the loss is 2143.1172963750073, parameters k is -56.01427063236799 and b is 371.3342282259601\n",
      "Iteration 3002, the loss is 2142.292272754401, parameters k is -56.001276561221744 and b is 371.2512242733909\n",
      "Iteration 3003, the loss is 2141.467419109291, parameters k is -55.9882824900755 and b is 371.1682203208217\n",
      "Iteration 3004, the loss is 2140.642735439681, parameters k is -55.97528841892925 and b is 371.08521636825253\n",
      "Iteration 3005, the loss is 2139.818221745572, parameters k is -55.96229434778301 and b is 371.00221241568335\n",
      "Iteration 3006, the loss is 2138.9938780269663, parameters k is -55.94930027663676 and b is 370.91920846311416\n",
      "Iteration 3007, the loss is 2138.1697042838564, parameters k is -55.93630620549052 and b is 370.836204510545\n",
      "Iteration 3008, the loss is 2137.345700516248, parameters k is -55.92331213434427 and b is 370.7532005579758\n",
      "Iteration 3009, the loss is 2136.5218667241393, parameters k is -55.91031806319803 and b is 370.6701966054066\n",
      "Iteration 3010, the loss is 2135.6982029075357, parameters k is -55.89732399205178 and b is 370.5871926528374\n",
      "Iteration 3011, the loss is 2134.87470906643, parameters k is -55.884329920905536 and b is 370.50418870026823\n",
      "Iteration 3012, the loss is 2134.051385200823, parameters k is -55.87133584975929 and b is 370.42118474769904\n",
      "Iteration 3013, the loss is 2133.2282313107153, parameters k is -55.858341778613045 and b is 370.33818079512986\n",
      "Iteration 3014, the loss is 2132.40524739611, parameters k is -55.8453477074668 and b is 370.2551768425607\n",
      "Iteration 3015, the loss is 2131.582433457007, parameters k is -55.832353636320555 and b is 370.1721728899915\n",
      "Iteration 3016, the loss is 2130.7597894934, parameters k is -55.81935956517431 and b is 370.0891689374223\n",
      "Iteration 3017, the loss is 2129.9373155052963, parameters k is -55.806365494028064 and b is 370.0061649848531\n",
      "Iteration 3018, the loss is 2129.1150114926913, parameters k is -55.79337142288182 and b is 369.92316103228393\n",
      "Iteration 3019, the loss is 2128.292877455587, parameters k is -55.780377351735574 and b is 369.84015707971474\n",
      "Iteration 3020, the loss is 2127.4709133939828, parameters k is -55.76738328058933 and b is 369.75715312714556\n",
      "Iteration 3021, the loss is 2126.649119307882, parameters k is -55.75438920944308 and b is 369.67414917457637\n",
      "Iteration 3022, the loss is 2125.827495197275, parameters k is -55.74139513829684 and b is 369.5911452220072\n",
      "Iteration 3023, the loss is 2124.980307350959, parameters k is -55.72840106715059 and b is 369.512093838608\n",
      "Iteration 3024, the loss is 2124.1332995246985, parameters k is -55.71540699600435 and b is 369.4330424552088\n",
      "Iteration 3025, the loss is 2123.3121641395487, parameters k is -55.7024129248581 and b is 369.3500385026396\n",
      "Iteration 3026, the loss is 2122.465495708326, parameters k is -55.68941885371186 and b is 369.27098711924043\n",
      "Iteration 3027, the loss is 2121.619007297163, parameters k is -55.67642478256561 and b is 369.19193573584124\n",
      "Iteration 3028, the loss is 2120.7983606374687, parameters k is -55.663430711419366 and b is 369.10893178327206\n",
      "Iteration 3029, the loss is 2119.952211621342, parameters k is -55.65043664027312 and b is 369.02988039987287\n",
      "Iteration 3030, the loss is 2119.106242625276, parameters k is -55.637442569126875 and b is 368.9508290164737\n",
      "Iteration 3031, the loss is 2118.2860846910344, parameters k is -55.62444849798063 and b is 368.8678250639045\n",
      "Iteration 3032, the loss is 2117.440455090003, parameters k is -55.611454426834385 and b is 368.7887736805053\n",
      "Iteration 3033, the loss is 2116.5950055090325, parameters k is -55.59846035568814 and b is 368.7097222971061\n",
      "Iteration 3034, the loss is 2115.7753363002466, parameters k is -55.585466284541894 and b is 368.6267183445369\n",
      "Iteration 3035, the loss is 2114.9302261143084, parameters k is -55.57247221339565 and b is 368.54766696113774\n",
      "Iteration 3036, the loss is 2114.0852959484314, parameters k is -55.559478142249404 and b is 368.46861557773855\n",
      "Iteration 3037, the loss is 2113.2405458026196, parameters k is -55.54648407110316 and b is 368.38956419433936\n",
      "Iteration 3038, the loss is 2112.421524694261, parameters k is -55.53348999995691 and b is 368.30656024177017\n",
      "Iteration 3039, the loss is 2111.577113943482, parameters k is -55.52049592881067 and b is 368.227508858371\n",
      "Iteration 3040, the loss is 2110.7328832127655, parameters k is -55.50750185766442 and b is 368.1484574749718\n",
      "Iteration 3041, the loss is 2109.914350829863, parameters k is -55.49450778651818 and b is 368.0654535224026\n",
      "Iteration 3042, the loss is 2109.070459494179, parameters k is -55.48151371537193 and b is 367.9864021390034\n",
      "Iteration 3043, the loss is 2108.2267481785566, parameters k is -55.46851964422569 and b is 367.9073507556042\n",
      "Iteration 3044, the loss is 2107.4087045211077, parameters k is -55.45552557307944 and b is 367.82434680303504\n",
      "Iteration 3045, the loss is 2106.565332600522, parameters k is -55.442531501933196 and b is 367.74529541963585\n",
      "Iteration 3046, the loss is 2105.722140699995, parameters k is -55.42953743078695 and b is 367.66624403623666\n",
      "Iteration 3047, the loss is 2104.9045857680017, parameters k is -55.416543359640706 and b is 367.5832400836675\n",
      "Iteration 3048, the loss is 2104.061733262512, parameters k is -55.40354928849446 and b is 367.5041887002683\n",
      "Iteration 3049, the loss is 2103.219060777079, parameters k is -55.390555217348215 and b is 367.4251373168691\n",
      "Iteration 3050, the loss is 2102.40199457054, parameters k is -55.37756114620197 and b is 367.3421333642999\n",
      "Iteration 3051, the loss is 2101.559661480147, parameters k is -55.364567075055724 and b is 367.2630819809007\n",
      "Iteration 3052, the loss is 2100.717508409811, parameters k is -55.35157300390948 and b is 367.18403059750153\n",
      "Iteration 3053, the loss is 2099.9009309287258, parameters k is -55.338578932763234 and b is 367.10102664493235\n",
      "Iteration 3054, the loss is 2099.059117253429, parameters k is -55.32558486161699 and b is 367.02197526153316\n",
      "Iteration 3055, the loss is 2098.2174835981887, parameters k is -55.31259079047074 and b is 366.94292387813397\n",
      "Iteration 3056, the loss is 2097.4013948425572, parameters k is -55.2995967193245 and b is 366.8599199255648\n",
      "Iteration 3057, the loss is 2096.5601005823555, parameters k is -55.28660264817825 and b is 366.7808685421656\n",
      "Iteration 3058, the loss is 2095.7189863422127, parameters k is -55.27360857703201 and b is 366.7018171587664\n",
      "Iteration 3059, the loss is 2094.9033863120353, parameters k is -55.26061450588576 and b is 366.6188132061972\n",
      "Iteration 3060, the loss is 2094.062611466929, parameters k is -55.24762043473952 and b is 366.539761822798\n",
      "Iteration 3061, the loss is 2093.222016641883, parameters k is -55.23462636359327 and b is 366.46071043939884\n",
      "Iteration 3062, the loss is 2092.406905337159, parameters k is -55.221632292447026 and b is 366.37770648682965\n",
      "Iteration 3063, the loss is 2091.566649907148, parameters k is -55.20863822130078 and b is 366.29865510343046\n",
      "Iteration 3064, the loss is 2090.726574497196, parameters k is -55.195644150154536 and b is 366.2196037200313\n",
      "Iteration 3065, the loss is 2089.9372559741596, parameters k is -55.18265007900829 and b is 366.13264719829215\n",
      "Iteration 3066, the loss is 2089.0974993141613, parameters k is -55.169656007862045 and b is 366.05359581489296\n",
      "Iteration 3067, the loss is 2088.283216685892, parameters k is -55.1566619367158 and b is 365.9705918623238\n",
      "Iteration 3068, the loss is 2087.469104033125, parameters k is -55.143667865569554 and b is 365.8875879097546\n",
      "Iteration 3069, the loss is 2086.65516135586, parameters k is -55.13067379442331 and b is 365.8045839571854\n",
      "Iteration 3070, the loss is 2085.8413886540934, parameters k is -55.117679723277064 and b is 365.7215800046162\n",
      "Iteration 3071, the loss is 2085.0277859278262, parameters k is -55.10468565213082 and b is 365.63857605204703\n",
      "Iteration 3072, the loss is 2084.214353177061, parameters k is -55.09169158098457 and b is 365.55557209947784\n",
      "Iteration 3073, the loss is 2083.4010904017928, parameters k is -55.07869750983833 and b is 365.47256814690866\n",
      "Iteration 3074, the loss is 2082.5879976020287, parameters k is -55.06570343869208 and b is 365.38956419433947\n",
      "Iteration 3075, the loss is 2081.775074777764, parameters k is -55.05270936754584 and b is 365.3065602417703\n",
      "Iteration 3076, the loss is 2080.962321929002, parameters k is -55.03971529639959 and b is 365.2235562892011\n",
      "Iteration 3077, the loss is 2080.1497390557347, parameters k is -55.02672122525335 and b is 365.1405523366319\n",
      "Iteration 3078, the loss is 2079.3373261579713, parameters k is -55.0137271541071 and b is 365.0575483840627\n",
      "Iteration 3079, the loss is 2078.525083235706, parameters k is -55.000733082960856 and b is 364.97454443149354\n",
      "Iteration 3080, the loss is 2077.713010288945, parameters k is -54.98773901181461 and b is 364.89154047892436\n",
      "Iteration 3081, the loss is 2076.901107317681, parameters k is -54.974744940668366 and b is 364.80853652635517\n",
      "Iteration 3082, the loss is 2076.0893743219185, parameters k is -54.96175086952212 and b is 364.725532573786\n",
      "Iteration 3083, the loss is 2075.2778113016557, parameters k is -54.948756798375875 and b is 364.6425286212168\n",
      "Iteration 3084, the loss is 2074.4664182568945, parameters k is -54.93576272722963 and b is 364.5595246686476\n",
      "Iteration 3085, the loss is 2073.6551951876313, parameters k is -54.922768656083385 and b is 364.4765207160784\n",
      "Iteration 3086, the loss is 2072.84414209387, parameters k is -54.90977458493714 and b is 364.39351676350924\n",
      "Iteration 3087, the loss is 2072.03325897561, parameters k is -54.896780513790894 and b is 364.31051281094005\n",
      "Iteration 3088, the loss is 2071.2225458328494, parameters k is -54.88378644264465 and b is 364.22750885837087\n",
      "Iteration 3089, the loss is 2070.4120026655874, parameters k is -54.8707923714984 and b is 364.1445049058017\n",
      "Iteration 3090, the loss is 2069.601629473826, parameters k is -54.85779830035216 and b is 364.0615009532325\n",
      "Iteration 3091, the loss is 2068.7914262575673, parameters k is -54.84480422920591 and b is 363.9784970006633\n",
      "Iteration 3092, the loss is 2067.981393016808, parameters k is -54.83181015805967 and b is 363.8954930480941\n",
      "Iteration 3093, the loss is 2067.1715297515507, parameters k is -54.81881608691342 and b is 363.81248909552494\n",
      "Iteration 3094, the loss is 2066.3618364617914, parameters k is -54.80582201576718 and b is 363.72948514295575\n",
      "Iteration 3095, the loss is 2065.5523131475315, parameters k is -54.79282794462093 and b is 363.64648119038657\n",
      "Iteration 3096, the loss is 2064.7429598087742, parameters k is -54.779833873474686 and b is 363.5634772378174\n",
      "Iteration 3097, the loss is 2063.933776445515, parameters k is -54.76683980232844 and b is 363.4804732852482\n",
      "Iteration 3098, the loss is 2063.1247630577573, parameters k is -54.753845731182196 and b is 363.397469332679\n",
      "Iteration 3099, the loss is 2062.3159196455013, parameters k is -54.74085166003595 and b is 363.3144653801098\n",
      "Iteration 3100, the loss is 2061.507246208744, parameters k is -54.727857588889705 and b is 363.23146142754064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3101, the loss is 2060.698742747486, parameters k is -54.71486351774346 and b is 363.14845747497145\n",
      "Iteration 3102, the loss is 2059.89040926173, parameters k is -54.701869446597215 and b is 363.06545352240227\n",
      "Iteration 3103, the loss is 2059.0822457514746, parameters k is -54.68887537545097 and b is 362.9824495698331\n",
      "Iteration 3104, the loss is 2058.2742522167173, parameters k is -54.675881304304724 and b is 362.8994456172639\n",
      "Iteration 3105, the loss is 2057.466428657465, parameters k is -54.66288723315848 and b is 362.8164416646947\n",
      "Iteration 3106, the loss is 2056.658775073707, parameters k is -54.64989316201223 and b is 362.7334377121255\n",
      "Iteration 3107, the loss is 2055.851291465453, parameters k is -54.63689909086599 and b is 362.65043375955634\n",
      "Iteration 3108, the loss is 2055.0439778327, parameters k is -54.62390501971974 and b is 362.56742980698715\n",
      "Iteration 3109, the loss is 2054.236834175445, parameters k is -54.6109109485735 and b is 362.48442585441796\n",
      "Iteration 3110, the loss is 2053.4298604936903, parameters k is -54.59791687742725 and b is 362.4014219018488\n",
      "Iteration 3111, the loss is 2052.623056787439, parameters k is -54.58492280628101 and b is 362.3184179492796\n",
      "Iteration 3112, the loss is 2051.816423056686, parameters k is -54.57192873513476 and b is 362.2354139967104\n",
      "Iteration 3113, the loss is 2051.0099593014324, parameters k is -54.558934663988516 and b is 362.1524100441412\n",
      "Iteration 3114, the loss is 2050.203665521679, parameters k is -54.54594059284227 and b is 362.06940609157203\n",
      "Iteration 3115, the loss is 2049.397541717429, parameters k is -54.532946521696026 and b is 361.98640213900285\n",
      "Iteration 3116, the loss is 2048.5915878886763, parameters k is -54.51995245054978 and b is 361.90339818643366\n",
      "Iteration 3117, the loss is 2047.785804035425, parameters k is -54.506958379403535 and b is 361.8203942338645\n",
      "Iteration 3118, the loss is 2046.9801901576748, parameters k is -54.49396430825729 and b is 361.7373902812953\n",
      "Iteration 3119, the loss is 2046.1747462554213, parameters k is -54.480970237111045 and b is 361.6543863287261\n",
      "Iteration 3120, the loss is 2045.3694723286721, parameters k is -54.4679761659648 and b is 361.5713823761569\n",
      "Iteration 3121, the loss is 2044.5643683774217, parameters k is -54.454982094818554 and b is 361.48837842358773\n",
      "Iteration 3122, the loss is 2043.7594344016702, parameters k is -54.44198802367231 and b is 361.40537447101855\n",
      "Iteration 3123, the loss is 2042.9546704014226, parameters k is -54.42899395252606 and b is 361.32237051844936\n",
      "Iteration 3124, the loss is 2042.1500763766708, parameters k is -54.41599988137982 and b is 361.2393665658802\n",
      "Iteration 3125, the loss is 2041.345652327423, parameters k is -54.40300581023357 and b is 361.156362613311\n",
      "Iteration 3126, the loss is 2040.5413982536743, parameters k is -54.39001173908733 and b is 361.0733586607418\n",
      "Iteration 3127, the loss is 2039.7373141554278, parameters k is -54.37701766794108 and b is 360.9903547081726\n",
      "Iteration 3128, the loss is 2038.9334000326796, parameters k is -54.36402359679484 and b is 360.90735075560343\n",
      "Iteration 3129, the loss is 2038.1296558854326, parameters k is -54.35102952564859 and b is 360.82434680303425\n",
      "Iteration 3130, the loss is 2037.3260817136854, parameters k is -54.33803545450235 and b is 360.74134285046506\n",
      "Iteration 3131, the loss is 2036.522677517438, parameters k is -54.3250413833561 and b is 360.6583388978959\n",
      "Iteration 3132, the loss is 2035.7194432966892, parameters k is -54.312047312209856 and b is 360.5753349453267\n",
      "Iteration 3133, the loss is 2034.916379051446, parameters k is -54.29905324106361 and b is 360.4923309927575\n",
      "Iteration 3134, the loss is 2034.1134847816988, parameters k is -54.286059169917365 and b is 360.4093270401883\n",
      "Iteration 3135, the loss is 2033.3107604874526, parameters k is -54.27306509877112 and b is 360.32632308761913\n",
      "Iteration 3136, the loss is 2032.5082061687065, parameters k is -54.260071027624875 and b is 360.24331913504994\n",
      "Iteration 3137, the loss is 2031.7058218254626, parameters k is -54.24707695647863 and b is 360.16031518248076\n",
      "Iteration 3138, the loss is 2030.903607457717, parameters k is -54.234082885332384 and b is 360.0773112299116\n",
      "Iteration 3139, the loss is 2030.1015630654715, parameters k is -54.22108881418614 and b is 359.9943072773424\n",
      "Iteration 3140, the loss is 2029.2996886487294, parameters k is -54.208094743039894 and b is 359.9113033247732\n",
      "Iteration 3141, the loss is 2028.4979842074863, parameters k is -54.19510067189365 and b is 359.828299372204\n",
      "Iteration 3142, the loss is 2027.6964497417405, parameters k is -54.1821066007474 and b is 359.74529541963483\n",
      "Iteration 3143, the loss is 2026.895085251499, parameters k is -54.16911252960116 and b is 359.66229146706564\n",
      "Iteration 3144, the loss is 2026.0938907367572, parameters k is -54.15611845845491 and b is 359.57928751449646\n",
      "Iteration 3145, the loss is 2025.2928661975125, parameters k is -54.14312438730867 and b is 359.49628356192727\n",
      "Iteration 3146, the loss is 2024.4920116337703, parameters k is -54.13013031616242 and b is 359.4132796093581\n",
      "Iteration 3147, the loss is 2023.6913270455298, parameters k is -54.11713624501618 and b is 359.3302756567889\n",
      "Iteration 3148, the loss is 2022.8908124327893, parameters k is -54.10414217386993 and b is 359.2472717042197\n",
      "Iteration 3149, the loss is 2022.0904677955468, parameters k is -54.091148102723686 and b is 359.1642677516505\n",
      "Iteration 3150, the loss is 2021.2902931338072, parameters k is -54.07815403157744 and b is 359.08126379908134\n",
      "Iteration 3151, the loss is 2020.4902884475641, parameters k is -54.065159960431195 and b is 358.99825984651216\n",
      "Iteration 3152, the loss is 2019.6904537368243, parameters k is -54.05216588928495 and b is 358.91525589394297\n",
      "Iteration 3153, the loss is 2018.890789001586, parameters k is -54.039171818138705 and b is 358.8322519413738\n",
      "Iteration 3154, the loss is 2018.091294241845, parameters k is -54.02617774699246 and b is 358.7492479888046\n",
      "Iteration 3155, the loss is 2017.2919694576042, parameters k is -54.013183675846214 and b is 358.6662440362354\n",
      "Iteration 3156, the loss is 2016.4928146488664, parameters k is -54.00018960469997 and b is 358.5832400836662\n",
      "Iteration 3157, the loss is 2015.6938298156292, parameters k is -53.987195533553724 and b is 358.50023613109704\n",
      "Iteration 3158, the loss is 2014.8950149578902, parameters k is -53.97420146240748 and b is 358.41723217852785\n",
      "Iteration 3159, the loss is 2014.096370075649, parameters k is -53.96120739126123 and b is 358.33422822595867\n",
      "Iteration 3160, the loss is 2013.297895168915, parameters k is -53.94821332011499 and b is 358.2512242733895\n",
      "Iteration 3161, the loss is 2012.499590237676, parameters k is -53.93521924896874 and b is 358.1682203208203\n",
      "Iteration 3162, the loss is 2011.70145528194, parameters k is -53.9222251778225 and b is 358.0852163682511\n",
      "Iteration 3163, the loss is 2010.9034903017039, parameters k is -53.90923110667625 and b is 358.0022124156819\n",
      "Iteration 3164, the loss is 2010.1056952969668, parameters k is -53.89623703553001 and b is 357.91920846311274\n",
      "Iteration 3165, the loss is 2009.3080702677312, parameters k is -53.88324296438376 and b is 357.83620451054355\n",
      "Iteration 3166, the loss is 2008.510615213998, parameters k is -53.870248893237516 and b is 357.75320055797437\n",
      "Iteration 3167, the loss is 2007.7133301357615, parameters k is -53.85725482209127 and b is 357.6701966054052\n",
      "Iteration 3168, the loss is 2006.916215033028, parameters k is -53.844260750945026 and b is 357.587192652836\n",
      "Iteration 3169, the loss is 2006.1192699057897, parameters k is -53.83126667979878 and b is 357.5041887002668\n",
      "Iteration 3170, the loss is 2005.3224947540566, parameters k is -53.818272608652535 and b is 357.4211847476976\n",
      "Iteration 3171, the loss is 2004.52588957782, parameters k is -53.80527853750629 and b is 357.33818079512844\n",
      "Iteration 3172, the loss is 2003.7294543770884, parameters k is -53.792284466360044 and b is 357.25517684255925\n",
      "Iteration 3173, the loss is 2002.9331891518552, parameters k is -53.7792903952138 and b is 357.17217288999007\n",
      "Iteration 3174, the loss is 2002.1370939021217, parameters k is -53.766296324067554 and b is 357.0891689374209\n",
      "Iteration 3175, the loss is 2001.341168627889, parameters k is -53.75330225292131 and b is 357.0061649848517\n",
      "Iteration 3176, the loss is 2000.5454133291569, parameters k is -53.74030818177506 and b is 356.9231610322825\n",
      "Iteration 3177, the loss is 1999.7498280059237, parameters k is -53.72731411062882 and b is 356.8401570797133\n",
      "Iteration 3178, the loss is 1998.9544126581918, parameters k is -53.71432003948257 and b is 356.75715312714414\n",
      "Iteration 3179, the loss is 1998.1591672859613, parameters k is -53.70132596833633 and b is 356.67414917457495\n",
      "Iteration 3180, the loss is 1997.3640918892297, parameters k is -53.68833189719008 and b is 356.59114522200576\n",
      "Iteration 3181, the loss is 1996.569186468001, parameters k is -53.67533782604384 and b is 356.5081412694366\n",
      "Iteration 3182, the loss is 1995.774451022271, parameters k is -53.66234375489759 and b is 356.4251373168674\n",
      "Iteration 3183, the loss is 1994.9798855520385, parameters k is -53.649349683751346 and b is 356.3421333642982\n",
      "Iteration 3184, the loss is 1994.1854900573078, parameters k is -53.6363556126051 and b is 356.259129411729\n",
      "Iteration 3185, the loss is 1993.3912645380778, parameters k is -53.623361541458856 and b is 356.17612545915983\n",
      "Iteration 3186, the loss is 1992.597208994349, parameters k is -53.61036747031261 and b is 356.09312150659065\n",
      "Iteration 3187, the loss is 1991.8033234261225, parameters k is -53.597373399166365 and b is 356.01011755402146\n",
      "Iteration 3188, the loss is 1991.0096078333925, parameters k is -53.58437932802012 and b is 355.9271136014523\n",
      "Iteration 3189, the loss is 1990.2160622161625, parameters k is -53.571385256873874 and b is 355.8441096488831\n",
      "Iteration 3190, the loss is 1989.4226865744336, parameters k is -53.55839118572763 and b is 355.7611056963139\n",
      "Iteration 3191, the loss is 1988.6294809082087, parameters k is -53.545397114581384 and b is 355.6781017437447\n",
      "Iteration 3192, the loss is 1987.8364452174824, parameters k is -53.53240304343514 and b is 355.59509779117553\n",
      "Iteration 3193, the loss is 1987.0435795022527, parameters k is -53.51940897228889 and b is 355.51209383860635\n",
      "Iteration 3194, the loss is 1986.250883762526, parameters k is -53.50641490114265 and b is 355.42908988603716\n",
      "Iteration 3195, the loss is 1985.4583579983014, parameters k is -53.4934208299964 and b is 355.346085933468\n",
      "Iteration 3196, the loss is 1984.6660022095746, parameters k is -53.48042675885016 and b is 355.2630819808988\n",
      "Iteration 3197, the loss is 1983.8738163963476, parameters k is -53.46743268770391 and b is 355.1800780283296\n",
      "Iteration 3198, the loss is 1983.0818005586216, parameters k is -53.45443861655767 and b is 355.0970740757604\n",
      "Iteration 3199, the loss is 1982.2632614156375, parameters k is -53.44144454541142 and b is 355.0180226923612\n",
      "Iteration 3200, the loss is 1981.471574928391, parameters k is -53.428450474265176 and b is 354.93501873979204\n",
      "Iteration 3201, the loss is 1980.6533751804398, parameters k is -53.41545640311893 and b is 354.85596735639285\n",
      "Iteration 3202, the loss is 1979.8353554525524, parameters k is -53.402462331972686 and b is 354.77691597299366\n",
      "Iteration 3203, the loss is 1979.0441576907574, parameters k is -53.38946826082644 and b is 354.6939120204245\n",
      "Iteration 3204, the loss is 1978.2264773579034, parameters k is -53.376474189680195 and b is 354.6148606370253\n",
      "Iteration 3205, the loss is 1977.4356089465875, parameters k is -53.36348011853395 and b is 354.5318566844561\n",
      "Iteration 3206, the loss is 1976.61826800877, parameters k is -53.350486047387705 and b is 354.4528053010569\n",
      "Iteration 3207, the loss is 1975.8011070910147, parameters k is -53.33749197624146 and b is 354.3737539176577\n",
      "Iteration 3208, the loss is 1975.010727405149, parameters k is -53.324497905095214 and b is 354.29074996508854\n",
      "Iteration 3209, the loss is 1974.1939058824303, parameters k is -53.31150383394897 and b is 354.21169858168935\n",
      "Iteration 3210, the loss is 1973.4038555470447, parameters k is -53.29850976280272 and b is 354.12869462912016\n",
      "Iteration 3211, the loss is 1972.5873734193624, parameters k is -53.28551569165648 and b is 354.049643245721\n",
      "Iteration 3212, the loss is 1971.7710713117383, parameters k is -53.27252162051023 and b is 353.9705918623218\n",
      "Iteration 3213, the loss is 1970.9815097018054, parameters k is -53.25952754936399 and b is 353.8875879097526\n",
      "Iteration 3214, the loss is 1970.1655469892164, parameters k is -53.24653347821774 and b is 353.8085365263534\n",
      "Iteration 3215, the loss is 1969.376314729763, parameters k is -53.2335394070715 and b is 353.7255325737842\n",
      "Iteration 3216, the loss is 1968.5606914122122, parameters k is -53.22054533592525 and b is 353.64648119038503\n",
      "Iteration 3217, the loss is 1967.771788503236, parameters k is -53.207551264779006 and b is 353.56347723781585\n",
      "Iteration 3218, the loss is 1966.9565045807194, parameters k is -53.19455719363276 and b is 353.48442585441666\n",
      "Iteration 3219, the loss is 1966.141400678262, parameters k is -53.181563122486516 and b is 353.40537447101747\n",
      "Iteration 3220, the loss is 1965.3529864947416, parameters k is -53.16856905134027 and b is 353.3223705184483\n",
      "Iteration 3221, the loss is 1964.538221987324, parameters k is -53.155574980194025 and b is 353.2433191350491\n",
      "Iteration 3222, the loss is 1963.7501371542776, parameters k is -53.14258090904778 and b is 353.1603151824799\n",
      "Iteration 3223, the loss is 1962.935712041897, parameters k is -53.129586837901535 and b is 353.0812637990807\n",
      "Iteration 3224, the loss is 1962.1214669495707, parameters k is -53.11659276675529 and b is 353.0022124156815\n",
      "Iteration 3225, the loss is 1961.3338708419792, parameters k is -53.103598695609044 and b is 352.91920846311234\n",
      "Iteration 3226, the loss is 1960.5199651446924, parameters k is -53.0906046244628 and b is 352.84015707971315\n",
      "Iteration 3227, the loss is 1959.732698387583, parameters k is -53.07761055331655 and b is 352.75715312714397\n",
      "Iteration 3228, the loss is 1958.9191320853297, parameters k is -53.06461648217031 and b is 352.6781017437448\n",
      "Iteration 3229, the loss is 1958.1057458031428, parameters k is -53.05162241102406 and b is 352.5990503603456\n",
      "Iteration 3230, the loss is 1957.3189677714804, parameters k is -53.03862833987782 and b is 352.5160464077764\n",
      "Iteration 3231, the loss is 1956.5059208843295, parameters k is -53.02563426873157 and b is 352.4369950243772\n",
      "Iteration 3232, the loss is 1955.7194722031443, parameters k is -53.01264019758533 and b is 352.353991071808\n",
      "Iteration 3233, the loss is 1954.906764711025, parameters k is -52.99964612643908 and b is 352.27493968840884\n",
      "Iteration 3234, the loss is 1954.1206453803227, parameters k is -52.986652055292836 and b is 352.19193573583965\n",
      "Iteration 3235, the loss is 1953.30827728324, parameters k is -52.97365798414659 and b is 352.11288435244046\n",
      "Iteration 3236, the loss is 1952.4960892062195, parameters k is -52.960663913000346 and b is 352.03383296904127\n",
      "Iteration 3237, the loss is 1951.7104586009702, parameters k is -52.9476698418541 and b is 351.9508290164721\n",
      "Iteration 3238, the loss is 1950.8986099189835, parameters k is -52.934675770707855 and b is 351.8717776330729\n",
      "Iteration 3239, the loss is 1950.1133086642092, parameters k is -52.92168169956161 and b is 351.7887736805037\n",
      "Iteration 3240, the loss is 1949.3017993772607, parameters k is -52.908687628415365 and b is 351.7097222971045\n",
      "Iteration 3241, the loss is 1948.4904701103733, parameters k is -52.89569355726912 and b is 351.63067091370533\n",
      "Iteration 3242, the loss is 1947.7056575810514, parameters k is -52.882699486122874 and b is 351.54766696113614\n",
      "Iteration 3243, the loss is 1946.8946677092013, parameters k is -52.86970541497663 and b is 351.46861557773695\n",
      "Iteration 3244, the loss is 1946.110184530359, parameters k is -52.85671134383038 and b is 351.38561162516777\n",
      "Iteration 3245, the loss is 1945.2995340535417, parameters k is -52.84371727268414 and b is 351.3065602417686\n",
      "Iteration 3246, the loss is 1944.4890635967863, parameters k is -52.83072320153789 and b is 351.2275088583694\n",
      "Iteration 3247, the loss is 1943.705069143399, parameters k is -52.81772913039165 and b is 351.1445049058002\n",
      "Iteration 3248, the loss is 1942.8949380816757, parameters k is -52.8047350592454 and b is 351.065453522401\n",
      "Iteration 3249, the loss is 1942.1112729787658, parameters k is -52.79174098809916 and b is 350.98244956983183\n",
      "Iteration 3250, the loss is 1941.3014813120838, parameters k is -52.77874691695291 and b is 350.90339818643264\n",
      "Iteration 3251, the loss is 1940.5181455596487, parameters k is -52.76575284580667 and b is 350.82039423386345\n",
      "Iteration 3252, the loss is 1939.708693288002, parameters k is -52.75275877466042 and b is 350.74134285046426\n",
      "Iteration 3253, the loss is 1938.8994210364149, parameters k is -52.739764703514176 and b is 350.6622914670651\n",
      "Iteration 3254, the loss is 1938.116574009435, parameters k is -52.72677063236793 and b is 350.5792875144959\n",
      "Iteration 3255, the loss is 1937.3076411528862, parameters k is -52.713776561221685 and b is 350.5002361310967\n",
      "Iteration 3256, the loss is 1936.5251234763805, parameters k is -52.70078249007544 and b is 350.4172321785275\n",
      "Iteration 3257, the loss is 1935.7165300148697, parameters k is -52.687788418929195 and b is 350.3381807951283\n",
      "Iteration 3258, the loss is 1934.9081165734133, parameters k is -52.67479434778295 and b is 350.25912941172913\n",
      "Iteration 3259, the loss is 1934.1260876223662, parameters k is -52.661800276636704 and b is 350.17612545915995\n",
      "Iteration 3260, the loss is 1933.3180135759476, parameters k is -52.64880620549046 and b is 350.09707407576076\n",
      "Iteration 3261, the loss is 1932.536313975376, parameters k is -52.635812134344214 and b is 350.0140701231916\n",
      "Iteration 3262, the loss is 1931.728579323994, parameters k is -52.62281806319797 and b is 349.9350187397924\n",
      "Iteration 3263, the loss is 1930.9210246926702, parameters k is -52.60982399205172 and b is 349.8559673563932\n",
      "Iteration 3264, the loss is 1930.139813817555, parameters k is -52.59682992090548 and b is 349.772963403824\n",
      "Iteration 3265, the loss is 1929.332598581269, parameters k is -52.58383584975923 and b is 349.6939120204248\n",
      "Iteration 3266, the loss is 1928.5517170566286, parameters k is -52.57084177861299 and b is 349.61090806785563\n",
      "Iteration 3267, the loss is 1927.7448412153822, parameters k is -52.55784770746674 and b is 349.53185668445644\n",
      "Iteration 3268, the loss is 1926.9642890412174, parameters k is -52.5448536363205 and b is 349.44885273188726\n",
      "Iteration 3269, the loss is 1926.1316295930626, parameters k is -52.53185956517425 and b is 349.373753917658\n",
      "Iteration 3270, the loss is 1925.3513961688518, parameters k is -52.518865494028006 and b is 349.2907499650888\n",
      "Iteration 3271, the loss is 1924.5713327201454, parameters k is -52.50587142288176 and b is 349.20774601251964\n",
      "Iteration 3272, the loss is 1923.7392233767268, parameters k is -52.492877351735515 and b is 349.1326471982904\n",
      "Iteration 3273, the loss is 1922.959478677973, parameters k is -52.47988328058927 and b is 349.0496432457212\n",
      "Iteration 3274, the loss is 1922.179903954719, parameters k is -52.466889209443025 and b is 348.966639293152\n",
      "Iteration 3275, the loss is 1921.3483447160377, parameters k is -52.45389513829678 and b is 348.89154047892276\n",
      "Iteration 3276, the loss is 1920.5690887427386, parameters k is -52.440901067150534 and b is 348.8085365263536\n",
      "Iteration 3277, the loss is 1919.790002744934, parameters k is -52.42790699600429 and b is 348.7255325737844\n",
      "Iteration 3278, the loss is 1918.98502454401, parameters k is -52.414912924858044 and b is 348.6464811903852\n",
      "Iteration 3279, the loss is 1918.1802263631482, parameters k is -52.4019188537118 and b is 348.567429806986\n",
      "Iteration 3280, the loss is 1917.3756082023426, parameters k is -52.38892478256555 and b is 348.4883784235868\n",
      "Iteration 3281, the loss is 1916.5971703049725, parameters k is -52.37593071141931 and b is 348.40537447101764\n",
      "Iteration 3282, the loss is 1915.7928915392038, parameters k is -52.36293664027306 and b is 348.32632308761845\n",
      "Iteration 3283, the loss is 1914.9887927934956, parameters k is -52.34994256912682 and b is 348.24727170421926\n",
      "Iteration 3284, the loss is 1914.21084362158, parameters k is -52.33694849798057 and b is 348.1642677516501\n",
      "Iteration 3285, the loss is 1913.4070842709086, parameters k is -52.32395442683433 and b is 348.0852163682509\n",
      "Iteration 3286, the loss is 1912.6035049402944, parameters k is -52.31096035568808 and b is 348.0061649848517\n",
      "Iteration 3287, the loss is 1911.8001056297421, parameters k is -52.297966284541836 and b is 347.9271136014525\n",
      "Iteration 3288, the loss is 1911.0228045582573, parameters k is -52.28497221339559 and b is 347.8441096488833\n",
      "Iteration 3289, the loss is 1910.21974464274, parameters k is -52.271978142249345 and b is 347.76505826548413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3290, the loss is 1909.4168647472848, parameters k is -52.2589840711031 and b is 347.68600688208494\n",
      "Iteration 3291, the loss is 1908.640052401253, parameters k is -52.245989999956855 and b is 347.60300292951575\n",
      "Iteration 3292, the loss is 1907.837511900832, parameters k is -52.23299592881061 and b is 347.52395154611656\n",
      "Iteration 3293, the loss is 1907.0351514204715, parameters k is -52.220001857664364 and b is 347.4449001627174\n",
      "Iteration 3294, the loss is 1906.2329709601693, parameters k is -52.20700778651812 and b is 347.3658487793182\n",
      "Iteration 3295, the loss is 1905.4568067145722, parameters k is -52.194013715371874 and b is 347.282844826749\n",
      "Iteration 3296, the loss is 1904.6549656493048, parameters k is -52.18101964422563 and b is 347.2037934433498\n",
      "Iteration 3297, the loss is 1903.8533046041018, parameters k is -52.16802557307938 and b is 347.1247420599506\n",
      "Iteration 3298, the loss is 1903.0776290839585, parameters k is -52.15503150193314 and b is 347.04173810738143\n",
      "Iteration 3299, the loss is 1902.2763074337868, parameters k is -52.14203743078689 and b is 346.96268672398224\n",
      "Iteration 3300, the loss is 1901.475165803678, parameters k is -52.12904335964065 and b is 346.88363534058306\n",
      "Iteration 3301, the loss is 1900.6742041936288, parameters k is -52.1160492884944 and b is 346.80458395718387\n",
      "Iteration 3302, the loss is 1899.8991767739133, parameters k is -52.10305521734816 and b is 346.7215800046147\n",
      "Iteration 3303, the loss is 1899.098554558901, parameters k is -52.09006114620191 and b is 346.6425286212155\n",
      "Iteration 3304, the loss is 1898.2981123639463, parameters k is -52.077067075055666 and b is 346.5634772378163\n",
      "Iteration 3305, the loss is 1897.5235736696882, parameters k is -52.06407300390942 and b is 346.4804732852471\n",
      "Iteration 3306, the loss is 1896.7234708697692, parameters k is -52.051078932763176 and b is 346.4014219018479\n",
      "Iteration 3307, the loss is 1895.923548089913, parameters k is -52.03808486161693 and b is 346.32237051844874\n",
      "Iteration 3308, the loss is 1895.123805330114, parameters k is -52.025090790470685 and b is 346.24331913504955\n",
      "Iteration 3309, the loss is 1894.3499147362838, parameters k is -52.01209671932444 and b is 346.16031518248036\n",
      "Iteration 3310, the loss is 1893.5505113715235, parameters k is -51.999102648178194 and b is 346.08126379908117\n",
      "Iteration 3311, the loss is 1892.7512880268202, parameters k is -51.98610857703195 and b is 346.002212415682\n",
      "Iteration 3312, the loss is 1891.9522447021784, parameters k is -51.973114505885704 and b is 345.9231610322828\n",
      "Iteration 3313, the loss is 1891.179002208781, parameters k is -51.96012043473946 and b is 345.8401570797136\n",
      "Iteration 3314, the loss is 1890.380298279177, parameters k is -51.94712636359321 and b is 345.7611056963144\n",
      "Iteration 3315, the loss is 1889.5817743696268, parameters k is -51.93413229244697 and b is 345.6820543129152\n",
      "Iteration 3316, the loss is 1888.8090206016864, parameters k is -51.92113822130072 and b is 345.59905036034604\n",
      "Iteration 3317, the loss is 1888.0108360871764, parameters k is -51.90814415015448 and b is 345.51999897694685\n",
      "Iteration 3318, the loss is 1887.2128315927257, parameters k is -51.89515007900823 and b is 345.44094759354766\n",
      "Iteration 3319, the loss is 1886.4150071183365, parameters k is -51.88215600786199 and b is 345.3618962101485\n",
      "Iteration 3320, the loss is 1885.6429014508233, parameters k is -51.86916193671574 and b is 345.2788922575793\n",
      "Iteration 3321, the loss is 1884.845416371467, parameters k is -51.856167865569496 and b is 345.1998408741801\n",
      "Iteration 3322, the loss is 1884.048111312173, parameters k is -51.84317379442325 and b is 345.1207894907809\n",
      "Iteration 3323, the loss is 1883.2764943701168, parameters k is -51.830179723277006 and b is 345.0377855382117\n",
      "Iteration 3324, the loss is 1882.4795287058582, parameters k is -51.81718565213076 and b is 344.95873415481253\n",
      "Iteration 3325, the loss is 1881.6827430616588, parameters k is -51.804191580984515 and b is 344.87968277141334\n",
      "Iteration 3326, the loss is 1880.8861374375192, parameters k is -51.79119750983827 and b is 344.80063138801415\n",
      "Iteration 3327, the loss is 1880.115168595892, parameters k is -51.778203438692024 and b is 344.71762743544497\n",
      "Iteration 3328, the loss is 1879.3189023667887, parameters k is -51.76520936754578 and b is 344.6385760520458\n",
      "Iteration 3329, the loss is 1878.5228161577452, parameters k is -51.752215296399534 and b is 344.5595246686466\n",
      "Iteration 3330, the loss is 1877.7523360415746, parameters k is -51.73922122525329 and b is 344.4765207160774\n",
      "Iteration 3331, the loss is 1876.9565892275675, parameters k is -51.72622715410704 and b is 344.3974693326782\n",
      "Iteration 3332, the loss is 1876.161022433621, parameters k is -51.7132330829608 and b is 344.318417949279\n",
      "Iteration 3333, the loss is 1875.3656356597337, parameters k is -51.70023901181455 and b is 344.23936656587983\n",
      "Iteration 3334, the loss is 1874.5958036439931, parameters k is -51.68724494066831 and b is 344.15636261331065\n",
      "Iteration 3335, the loss is 1873.800756265141, parameters k is -51.67425086952206 and b is 344.07731122991146\n",
      "Iteration 3336, the loss is 1873.005888906349, parameters k is -51.66125679837582 and b is 343.99825984651227\n",
      "Iteration 3337, the loss is 1872.2112015676196, parameters k is -51.64826272722957 and b is 343.9192084631131\n",
      "Iteration 3338, the loss is 1871.442017652307, parameters k is -51.635268656083326 and b is 343.8362045105439\n",
      "Iteration 3339, the loss is 1870.6476697086123, parameters k is -51.62227458493708 and b is 343.7571531271447\n",
      "Iteration 3340, the loss is 1869.8535017849767, parameters k is -51.609280513790836 and b is 343.6781017437455\n",
      "Iteration 3341, the loss is 1869.08480659512, parameters k is -51.59628644264459 and b is 343.59509779117633\n",
      "Iteration 3342, the loss is 1868.2909780665223, parameters k is -51.583292371498345 and b is 343.51604640777714\n",
      "Iteration 3343, the loss is 1867.4973295579855, parameters k is -51.5702983003521 and b is 343.43699502437795\n",
      "Iteration 3344, the loss is 1866.7038610695004, parameters k is -51.557304229205855 and b is 343.35794364097876\n",
      "Iteration 3345, the loss is 1865.935813980074, parameters k is -51.54431015805961 and b is 343.2749396884096\n",
      "Iteration 3346, the loss is 1865.142684886631, parameters k is -51.531316086913364 and b is 343.1958883050104\n",
      "Iteration 3347, the loss is 1864.3497358132497, parameters k is -51.51832201576712 and b is 343.1168369216112\n",
      "Iteration 3348, the loss is 1863.582177449277, parameters k is -51.50532794462087 and b is 343.033832969042\n",
      "Iteration 3349, the loss is 1862.7895677709303, parameters k is -51.49233387347463 and b is 342.9547815856428\n",
      "Iteration 3350, the loss is 1861.9971381126425, parameters k is -51.47933980232838 and b is 342.87573020224363\n",
      "Iteration 3351, the loss is 1861.2048884744115, parameters k is -51.46634573118214 and b is 342.79667881884444\n",
      "Iteration 3352, the loss is 1860.4379782108729, parameters k is -51.45335166003589 and b is 342.71367486627526\n",
      "Iteration 3353, the loss is 1859.646067967683, parameters k is -51.44035758888965 and b is 342.63462348287607\n",
      "Iteration 3354, the loss is 1858.8543377445515, parameters k is -51.4273635177434 and b is 342.5555720994769\n",
      "Iteration 3355, the loss is 1858.0879162064616, parameters k is -51.414369446597156 and b is 342.4725681469077\n",
      "Iteration 3356, the loss is 1857.2965253783684, parameters k is -51.40137537545091 and b is 342.3935167635085\n",
      "Iteration 3357, the loss is 1856.505314570331, parameters k is -51.388381304304666 and b is 342.3144653801093\n",
      "Iteration 3358, the loss is 1855.7142837823549, parameters k is -51.37538723315842 and b is 342.2354139967101\n",
      "Iteration 3359, the loss is 1854.9485103447, parameters k is -51.362393162012175 and b is 342.15241004414094\n",
      "Iteration 3360, the loss is 1854.1578189517593, parameters k is -51.34939909086593 and b is 342.07335866074175\n",
      "Iteration 3361, the loss is 1853.367307578878, parameters k is -51.336405019719685 and b is 341.99430727734256\n",
      "Iteration 3362, the loss is 1852.6020228666778, parameters k is -51.32341094857344 and b is 341.91130332477337\n",
      "Iteration 3363, the loss is 1851.8118508888329, parameters k is -51.310416877427194 and b is 341.8322519413742\n",
      "Iteration 3364, the loss is 1851.0218589310493, parameters k is -51.29742280628095 and b is 341.753200557975\n",
      "Iteration 3365, the loss is 1850.2320469933245, parameters k is -51.2844287351347 and b is 341.6741491745758\n",
      "Iteration 3366, the loss is 1849.4674103815548, parameters k is -51.27143466398846 and b is 341.5911452220066\n",
      "Iteration 3367, the loss is 1848.6779378388678, parameters k is -51.25844059284221 and b is 341.5120938386074\n",
      "Iteration 3368, the loss is 1847.888645316238, parameters k is -51.24544652169597 and b is 341.43304245520824\n",
      "Iteration 3369, the loss is 1847.0995328136694, parameters k is -51.23245245054972 and b is 341.35399107180905\n",
      "Iteration 3370, the loss is 1846.335544302328, parameters k is -51.21945837940348 and b is 341.27098711923986\n",
      "Iteration 3371, the loss is 1845.5467711947967, parameters k is -51.20646430825723 and b is 341.1919357358407\n",
      "Iteration 3372, the loss is 1844.7581781073256, parameters k is -51.193470237110986 and b is 341.1128843524415\n",
      "Iteration 3373, the loss is 1843.9946783214384, parameters k is -51.18047616596474 and b is 341.0298803998723\n",
      "Iteration 3374, the loss is 1843.2064246290026, parameters k is -51.167482094818496 and b is 340.9508290164731\n",
      "Iteration 3375, the loss is 1842.4183509566244, parameters k is -51.15448802367225 and b is 340.8717776330739\n",
      "Iteration 3376, the loss is 1841.6304573043087, parameters k is -51.141493952526005 and b is 340.79272624967473\n",
      "Iteration 3377, the loss is 1840.8676056188547, parameters k is -51.12849988137976 and b is 340.70972229710554\n",
      "Iteration 3378, the loss is 1840.0800513615718, parameters k is -51.115505810233515 and b is 340.63067091370635\n",
      "Iteration 3379, the loss is 1839.2926771243538, parameters k is -51.10251173908727 and b is 340.55161953030716\n",
      "Iteration 3380, the loss is 1838.5303141643524, parameters k is -51.089517667941024 and b is 340.468615577738\n",
      "Iteration 3381, the loss is 1837.743279322168, parameters k is -51.07652359679478 and b is 340.3895641943388\n",
      "Iteration 3382, the loss is 1836.956424500043, parameters k is -51.06352952564853 and b is 340.3105128109396\n",
      "Iteration 3383, the loss is 1836.1697496979757, parameters k is -51.05053545450229 and b is 340.2314614275404\n",
      "Iteration 3384, the loss is 1835.4080348384075, parameters k is -51.03754138335604 and b is 340.1484574749712\n",
      "Iteration 3385, the loss is 1834.6216994313786, parameters k is -51.0245473122098 and b is 340.06940609157203\n",
      "Iteration 3386, the loss is 1833.83554404441, parameters k is -51.01155324106355 and b is 339.99035470817284\n",
      "Iteration 3387, the loss is 1833.0743179102965, parameters k is -50.99855916991731 and b is 339.90735075560366\n",
      "Iteration 3388, the loss is 1832.288501918363, parameters k is -50.98556509877106 and b is 339.82829937220447\n",
      "Iteration 3389, the loss is 1831.5028659464883, parameters k is -50.97257102762482 and b is 339.7492479888053\n",
      "Iteration 3390, the loss is 1830.717409994675, parameters k is -50.95957695647857 and b is 339.6701966054061\n",
      "Iteration 3391, the loss is 1829.956831960993, parameters k is -50.946582885332326 and b is 339.5871926528369\n",
      "Iteration 3392, the loss is 1829.1717154042133, parameters k is -50.93358881418608 and b is 339.5081412694377\n",
      "Iteration 3393, the loss is 1828.3867788674963, parameters k is -50.920594743039835 and b is 339.4290898860385\n",
      "Iteration 3394, the loss is 1827.6020223508376, parameters k is -50.90760067189359 and b is 339.35003850263934\n",
      "Iteration 3395, the loss is 1826.8420924175855, parameters k is -50.894606600747345 and b is 339.26703455007015\n",
      "Iteration 3396, the loss is 1826.0576752959646, parameters k is -50.8816125296011 and b is 339.18798316667096\n",
      "Iteration 3397, the loss is 1825.2734381944017, parameters k is -50.868618458454854 and b is 339.10893178327177\n",
      "Iteration 3398, the loss is 1824.5139969866048, parameters k is -50.85562438730861 and b is 339.0259278307026\n",
      "Iteration 3399, the loss is 1823.730099280078, parameters k is -50.842630316162364 and b is 338.9468764473034\n",
      "Iteration 3400, the loss is 1822.946381593614, parameters k is -50.82963624501612 and b is 338.8678250639042\n",
      "Iteration 3401, the loss is 1822.1628439272058, parameters k is -50.81664217386987 and b is 338.788773680505\n",
      "Iteration 3402, the loss is 1821.4040508198386, parameters k is -50.80364810272363 and b is 338.70576972793583\n",
      "Iteration 3403, the loss is 1820.6208525484676, parameters k is -50.79065403157738 and b is 338.62671834453664\n",
      "Iteration 3404, the loss is 1819.8378342971575, parameters k is -50.77765996043114 and b is 338.54766696113745\n",
      "Iteration 3405, the loss is 1819.079529915245, parameters k is -50.76466588928489 and b is 338.46466300856827\n",
      "Iteration 3406, the loss is 1818.2968510589712, parameters k is -50.75167181813865 and b is 338.3856116251691\n",
      "Iteration 3407, the loss is 1817.5143522227572, parameters k is -50.7386777469924 and b is 338.3065602417699\n",
      "Iteration 3408, the loss is 1816.7320334066019, parameters k is -50.725683675846156 and b is 338.2275088583707\n",
      "Iteration 3409, the loss is 1815.9743771251215, parameters k is -50.71268960469991 and b is 338.1445049058015\n",
      "Iteration 3410, the loss is 1815.1923977040005, parameters k is -50.699695533553665 and b is 338.0654535224023\n",
      "Iteration 3411, the loss is 1814.4105983029422, parameters k is -50.68670146240742 and b is 337.98640213900313\n",
      "Iteration 3412, the loss is 1813.6534307469144, parameters k is -50.673707391261175 and b is 337.90339818643395\n",
      "Iteration 3413, the loss is 1812.8719707408927, parameters k is -50.66071332011493 and b is 337.82434680303476\n",
      "Iteration 3414, the loss is 1812.0906907549277, parameters k is -50.647719248968684 and b is 337.74529541963557\n",
      "Iteration 3415, the loss is 1811.3095907890254, parameters k is -50.63472517782244 and b is 337.6662440362364\n",
      "Iteration 3416, the loss is 1810.5530713334285, parameters k is -50.621731106676194 and b is 337.5832400836672\n",
      "Iteration 3417, the loss is 1809.772310762563, parameters k is -50.60873703552995 and b is 337.504188700268\n",
      "Iteration 3418, the loss is 1808.9917302117562, parameters k is -50.5957429643837 and b is 337.4251373168688\n",
      "Iteration 3419, the loss is 1808.2113296810085, parameters k is -50.58274889323746 and b is 337.3460859334696\n",
      "Iteration 3420, the loss is 1807.4554583258432, parameters k is -50.56975482209121 and b is 337.26308198090044\n",
      "Iteration 3421, the loss is 1806.6753971901317, parameters k is -50.55676075094497 and b is 337.18403059750125\n",
      "Iteration 3422, the loss is 1805.8955160744797, parameters k is -50.54376667979872 and b is 337.10497921410206\n",
      "Iteration 3423, the loss is 1805.14013344477, parameters k is -50.53077260865248 and b is 337.0219752615329\n",
      "Iteration 3424, the loss is 1804.3605917241523, parameters k is -50.51777853750623 and b is 336.9429238781337\n",
      "Iteration 3425, the loss is 1803.5812300235984, parameters k is -50.504784466359986 and b is 336.8638724947345\n",
      "Iteration 3426, the loss is 1802.8020483431037, parameters k is -50.49179039521374 and b is 336.7848211113353\n",
      "Iteration 3427, the loss is 1802.0473138138211, parameters k is -50.478796324067496 and b is 336.7018171587661\n",
      "Iteration 3428, the loss is 1801.2684715283651, parameters k is -50.46580225292125 and b is 336.62276577536693\n",
      "Iteration 3429, the loss is 1800.4898092629628, parameters k is -50.452808181775005 and b is 336.54371439196774\n",
      "Iteration 3430, the loss is 1799.7355634591377, parameters k is -50.43981411062876 and b is 336.46071043939855\n",
      "Iteration 3431, the loss is 1798.9572405887743, parameters k is -50.426820039482514 and b is 336.38165905599936\n",
      "Iteration 3432, the loss is 1798.179097738469, parameters k is -50.41382596833627 and b is 336.3026076726002\n",
      "Iteration 3433, the loss is 1797.4011349082293, parameters k is -50.400831897190024 and b is 336.223556289201\n",
      "Iteration 3434, the loss is 1796.6475372048317, parameters k is -50.38783782604378 and b is 336.1405523366318\n",
      "Iteration 3435, the loss is 1795.8699137696242, parameters k is -50.37484375489753 and b is 336.0615009532326\n",
      "Iteration 3436, the loss is 1795.0924703544738, parameters k is -50.36184968375129 and b is 335.9824495698334\n",
      "Iteration 3437, the loss is 1794.3393613765338, parameters k is -50.34885561260504 and b is 335.89944561726423\n",
      "Iteration 3438, the loss is 1793.562257356422, parameters k is -50.3358615414588 and b is 335.82039423386504\n",
      "Iteration 3439, the loss is 1792.7853333563735, parameters k is -50.32286747031255 and b is 335.74134285046586\n",
      "Iteration 3440, the loss is 1792.008589376379, parameters k is -50.30987339916631 and b is 335.66229146706667\n",
      "Iteration 3441, the loss is 1791.256128498869, parameters k is -50.29687932802006 and b is 335.5792875144975\n",
      "Iteration 3442, the loss is 1790.479723913913, parameters k is -50.283885256873816 and b is 335.5002361310983\n",
      "Iteration 3443, the loss is 1789.7034993490174, parameters k is -50.27089118572757 and b is 335.4211847476991\n",
      "Iteration 3444, the loss is 1788.951527196961, parameters k is -50.257897114581326 and b is 335.3381807951299\n",
      "Iteration 3445, the loss is 1788.1756420271017, parameters k is -50.24490304343508 and b is 335.2591294117307\n",
      "Iteration 3446, the loss is 1787.3999368773043, parameters k is -50.231908972288835 and b is 335.18007802833154\n",
      "Iteration 3447, the loss is 1786.6244117475624, parameters k is -50.21891490114259 and b is 335.10102664493235\n",
      "Iteration 3448, the loss is 1785.8730876959357, parameters k is -50.205920829996344 and b is 335.01802269236316\n",
      "Iteration 3449, the loss is 1785.097901961232, parameters k is -50.1929267588501 and b is 334.93897130896397\n",
      "Iteration 3450, the loss is 1784.3228962465867, parameters k is -50.179932687703854 and b is 334.8599199255648\n",
      "Iteration 3451, the loss is 1783.548070552005, parameters k is -50.16693861655761 and b is 334.7808685421656\n",
      "Iteration 3452, the loss is 1782.797394600808, parameters k is -50.15394454541136 and b is 334.6978645895964\n",
      "Iteration 3453, the loss is 1782.0229083012591, parameters k is -50.14095047426512 and b is 334.6188132061972\n",
      "Iteration 3454, the loss is 1781.2486020217739, parameters k is -50.12795640311887 and b is 334.539761822798\n",
      "Iteration 3455, the loss is 1780.4984147960329, parameters k is -50.11496233197263 and b is 334.45675787022884\n",
      "Iteration 3456, the loss is 1779.7244479115802, parameters k is -50.10196826082638 and b is 334.37770648682965\n",
      "Iteration 3457, the loss is 1778.950661047188, parameters k is -50.08897418968014 and b is 334.29865510343046\n",
      "Iteration 3458, the loss is 1778.1770542028537, parameters k is -50.07598011853389 and b is 334.2196037200313\n",
      "Iteration 3459, the loss is 1777.427515077542, parameters k is -50.062986047387646 and b is 334.1365997674621\n",
      "Iteration 3460, the loss is 1776.6542476282464, parameters k is -50.0499919762414 and b is 334.0575483840629\n",
      "Iteration 3461, the loss is 1775.8811601990126, parameters k is -50.036997905095156 and b is 333.9784970006637\n",
      "Iteration 3462, the loss is 1775.1321097991586, parameters k is -50.02400383394891 and b is 333.8954930480945\n",
      "Iteration 3463, the loss is 1774.359361764956, parameters k is -50.011009762802665 and b is 333.81644166469533\n",
      "Iteration 3464, the loss is 1773.5867937508156, parameters k is -49.99801569165642 and b is 333.73739028129614\n",
      "Iteration 3465, the loss is 1772.8144057567324, parameters k is -49.985021620510175 and b is 333.65833889789695\n",
      "Iteration 3466, the loss is 1772.0660034573089, parameters k is -49.97202754936393 and b is 333.57533494532777\n",
      "Iteration 3467, the loss is 1771.2939548582635, parameters k is -49.959033478217684 and b is 333.4962835619286\n",
      "Iteration 3468, the loss is 1770.5220862792787, parameters k is -49.94603940707144 and b is 333.4172321785294\n",
      "Iteration 3469, the loss is 1769.77417270531, parameters k is -49.93304533592519 and b is 333.3342282259602\n",
      "Iteration 3470, the loss is 1769.0026435213613, parameters k is -49.92005126477895 and b is 333.255176842561\n",
      "Iteration 3471, the loss is 1768.2312943574727, parameters k is -49.9070571936327 and b is 333.1761254591618\n",
      "Iteration 3472, the loss is 1767.4601252136436, parameters k is -49.89406312248646 and b is 333.09707407576263\n",
      "Iteration 3473, the loss is 1766.7128597401045, parameters k is -49.88106905134021 and b is 333.01407012319345\n",
      "Iteration 3474, the loss is 1765.9420299913113, parameters k is -49.86807498019397 and b is 332.93501873979426\n",
      "Iteration 3475, the loss is 1765.1713802625763, parameters k is -49.85508090904772 and b is 332.85596735639507\n",
      "Iteration 3476, the loss is 1764.4009105539028, parameters k is -49.842086837901476 and b is 332.7769159729959\n",
      "Iteration 3477, the loss is 1763.654293180796, parameters k is -49.82909276675523 and b is 332.6939120204267\n",
      "Iteration 3478, the loss is 1762.8841628671587, parameters k is -49.816098695608986 and b is 332.6148606370275\n",
      "Iteration 3479, the loss is 1762.1142125735823, parameters k is -49.80310462446274 and b is 332.5358092536283\n",
      "Iteration 3480, the loss is 1761.3680839259273, parameters k is -49.790110553316495 and b is 332.45280530105913\n",
      "Iteration 3481, the loss is 1760.5984730273876, parameters k is -49.77711648217025 and b is 332.37375391765994\n",
      "Iteration 3482, the loss is 1759.8290421489041, parameters k is -49.764122411024005 and b is 332.29470253426075\n",
      "Iteration 3483, the loss is 1759.0597912904807, parameters k is -49.75112833987776 and b is 332.21565115086156\n",
      "Iteration 3484, the loss is 1758.3143107432616, parameters k is -49.738134268731514 and b is 332.1326471982924\n",
      "Iteration 3485, the loss is 1757.5453992798743, parameters k is -49.72514019758527 and b is 332.0535958148932\n",
      "Iteration 3486, the loss is 1756.7766678365483, parameters k is -49.71214612643902 and b is 331.974544431494\n",
      "Iteration 3487, the loss is 1756.0316760147782, parameters k is -49.69915205529278 and b is 331.8915404789248\n",
      "Iteration 3488, the loss is 1755.2632839664882, parameters k is -49.68615798414653 and b is 331.8124890955256\n",
      "Iteration 3489, the loss is 1754.4950719382605, parameters k is -49.67316391300029 and b is 331.73343771212643\n",
      "Iteration 3490, the loss is 1753.727039930089, parameters k is -49.66016984185404 and b is 331.65438632872724\n",
      "Iteration 3491, the loss is 1752.9826962087523, parameters k is -49.6471757707078 and b is 331.57138237615806\n",
      "Iteration 3492, the loss is 1752.215003595617, parameters k is -49.63418169956155 and b is 331.49233099275887\n",
      "Iteration 3493, the loss is 1751.4474910025444, parameters k is -49.621187628415306 and b is 331.4132796093597\n",
      "Iteration 3494, the loss is 1750.703636006661, parameters k is -49.60819355726906 and b is 331.3302756567905\n",
      "Iteration 3495, the loss is 1749.9364628086253, parameters k is -49.595199486122816 and b is 331.2512242733913\n",
      "Iteration 3496, the loss is 1749.1694696306467, parameters k is -49.58220541497657 and b is 331.1721728899921\n",
      "Iteration 3497, the loss is 1748.426103360216, parameters k is -49.569211343830325 and b is 331.0891689374229\n",
      "Iteration 3498, the loss is 1747.682907065288, parameters k is -49.55621727268408 and b is 331.00616498485374\n",
      "Iteration 3499, the loss is 1746.9398807458583, parameters k is -49.543223201537835 and b is 330.92316103228455\n",
      "Iteration 3500, the loss is 1746.1970244019317, parameters k is -49.53022913039159 and b is 330.84015707971537\n",
      "Iteration 3501, the loss is 1745.454338033504, parameters k is -49.517235059245344 and b is 330.7571531271462\n",
      "Iteration 3502, the loss is 1744.7118216405759, parameters k is -49.5042409880991 and b is 330.674149174577\n",
      "Iteration 3503, the loss is 1743.9694752231499, parameters k is -49.49124691695285 and b is 330.5911452220078\n",
      "Iteration 3504, the loss is 1743.2272987812219, parameters k is -49.47825284580661 and b is 330.5081412694386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3505, the loss is 1742.485292314796, parameters k is -49.46525877466036 and b is 330.42513731686944\n",
      "Iteration 3506, the loss is 1741.7434558238713, parameters k is -49.45226470351412 and b is 330.34213336430025\n",
      "Iteration 3507, the loss is 1741.0017893084441, parameters k is -49.43927063236787 and b is 330.25912941173107\n",
      "Iteration 3508, the loss is 1740.2602927685195, parameters k is -49.42627656122163 and b is 330.1761254591619\n",
      "Iteration 3509, the loss is 1739.5189662040952, parameters k is -49.41328249007538 and b is 330.0931215065927\n",
      "Iteration 3510, the loss is 1738.7778096151717, parameters k is -49.40028841892914 and b is 330.0101175540235\n",
      "Iteration 3511, the loss is 1738.0368230017452, parameters k is -49.38729434778289 and b is 329.9271136014543\n",
      "Iteration 3512, the loss is 1737.296006363822, parameters k is -49.374300276636646 and b is 329.84410964888514\n",
      "Iteration 3513, the loss is 1736.555359701399, parameters k is -49.3613062054904 and b is 329.76110569631595\n",
      "Iteration 3514, the loss is 1735.8148830144735, parameters k is -49.348312134344155 and b is 329.67810174374677\n",
      "Iteration 3515, the loss is 1735.07457630305, parameters k is -49.33531806319791 and b is 329.5950977911776\n",
      "Iteration 3516, the loss is 1734.334439567129, parameters k is -49.322323992051665 and b is 329.5120938386084\n",
      "Iteration 3517, the loss is 1733.5944728067068, parameters k is -49.30932992090542 and b is 329.4290898860392\n",
      "Iteration 3518, the loss is 1732.8546760217841, parameters k is -49.296335849759174 and b is 329.34608593347\n",
      "Iteration 3519, the loss is 1732.1150492123613, parameters k is -49.28334177861293 and b is 329.26308198090084\n",
      "Iteration 3520, the loss is 1731.3755923784406, parameters k is -49.270347707466684 and b is 329.18007802833165\n",
      "Iteration 3521, the loss is 1730.6363055200206, parameters k is -49.25735363632044 and b is 329.09707407576246\n",
      "Iteration 3522, the loss is 1729.8971886370998, parameters k is -49.24435956517419 and b is 329.0140701231933\n",
      "Iteration 3523, the loss is 1729.1582417296763, parameters k is -49.23136549402795 and b is 328.9310661706241\n",
      "Iteration 3524, the loss is 1728.4194647977565, parameters k is -49.2183714228817 and b is 328.8480622180549\n",
      "Iteration 3525, the loss is 1727.6808578413386, parameters k is -49.20537735173546 and b is 328.7650582654857\n",
      "Iteration 3526, the loss is 1726.9424208604196, parameters k is -49.19238328058921 and b is 328.68205431291653\n",
      "Iteration 3527, the loss is 1726.2041538549997, parameters k is -49.17938920944297 and b is 328.59905036034735\n",
      "Iteration 3528, the loss is 1725.4660568250795, parameters k is -49.16639513829672 and b is 328.51604640777816\n",
      "Iteration 3529, the loss is 1724.7281297706627, parameters k is -49.153401067150476 and b is 328.433042455209\n",
      "Iteration 3530, the loss is 1723.9903726917444, parameters k is -49.14040699600423 and b is 328.3500385026398\n",
      "Iteration 3531, the loss is 1723.252785588326, parameters k is -49.127412924857985 and b is 328.2670345500706\n",
      "Iteration 3532, the loss is 1722.5153684604097, parameters k is -49.11441885371174 and b is 328.1840305975014\n",
      "Iteration 3533, the loss is 1721.7781213079897, parameters k is -49.101424782565495 and b is 328.10102664493223\n",
      "Iteration 3534, the loss is 1721.0410441310742, parameters k is -49.08843071141925 and b is 328.01802269236305\n",
      "Iteration 3535, the loss is 1720.3041369296575, parameters k is -49.075436640273004 and b is 327.93501873979386\n",
      "Iteration 3536, the loss is 1719.5673997037418, parameters k is -49.06244256912676 and b is 327.8520147872247\n",
      "Iteration 3537, the loss is 1718.830832453324, parameters k is -49.049448497980514 and b is 327.7690108346555\n",
      "Iteration 3538, the loss is 1718.094435178409, parameters k is -49.03645442683427 and b is 327.6860068820863\n",
      "Iteration 3539, the loss is 1717.3582078789937, parameters k is -49.02346035568802 and b is 327.6030029295171\n",
      "Iteration 3540, the loss is 1716.622150555082, parameters k is -49.01046628454178 and b is 327.51999897694793\n",
      "Iteration 3541, the loss is 1715.8862632066662, parameters k is -48.99747221339553 and b is 327.43699502437875\n",
      "Iteration 3542, the loss is 1715.1505458337497, parameters k is -48.98447814224929 and b is 327.35399107180956\n",
      "Iteration 3543, the loss is 1714.4149984363362, parameters k is -48.97148407110304 and b is 327.2709871192404\n",
      "Iteration 3544, the loss is 1713.6796210144228, parameters k is -48.9584899999568 and b is 327.1879831666712\n",
      "Iteration 3545, the loss is 1712.9444135680098, parameters k is -48.94549592881055 and b is 327.104979214102\n",
      "Iteration 3546, the loss is 1712.2093760970984, parameters k is -48.932501857664306 and b is 327.0219752615328\n",
      "Iteration 3547, the loss is 1711.4745086016826, parameters k is -48.91950778651806 and b is 326.93897130896363\n",
      "Iteration 3548, the loss is 1710.7158235675874, parameters k is -48.906513715371815 and b is 326.85991992556444\n",
      "Iteration 3549, the loss is 1709.9573185535494, parameters k is -48.89351964422557 and b is 326.78086854216525\n",
      "Iteration 3550, the loss is 1709.222939783591, parameters k is -48.880525573079325 and b is 326.69786458959607\n",
      "Iteration 3551, the loss is 1708.4647741645908, parameters k is -48.86753150193308 and b is 326.6188132061969\n",
      "Iteration 3552, the loss is 1707.730724745109, parameters k is -48.854537430786834 and b is 326.5358092536277\n",
      "Iteration 3553, the loss is 1706.9728985211473, parameters k is -48.84154335964059 and b is 326.4567578702285\n",
      "Iteration 3554, the loss is 1706.215252317241, parameters k is -48.828549288494344 and b is 326.3777064868293\n",
      "Iteration 3555, the loss is 1705.4816916232169, parameters k is -48.8155552173481 and b is 326.2947025342601\n",
      "Iteration 3556, the loss is 1704.7243848143446, parameters k is -48.80256114620185 and b is 326.21565115086094\n",
      "Iteration 3557, the loss is 1704.0150801616614, parameters k is -48.78956707505561 and b is 326.1286946291218\n",
      "Iteration 3558, the loss is 1703.258092102747, parameters k is -48.77657300390936 and b is 326.0496432457226\n",
      "Iteration 3559, the loss is 1702.5252007101965, parameters k is -48.76357893276312 and b is 325.96663929315343\n",
      "Iteration 3560, the loss is 1701.792479293148, parameters k is -48.75058486161687 and b is 325.88363534058425\n",
      "Iteration 3561, the loss is 1701.0599278515988, parameters k is -48.73759079047063 and b is 325.80063138801506\n",
      "Iteration 3562, the loss is 1700.3275463855493, parameters k is -48.72459671932438 and b is 325.7176274354459\n",
      "Iteration 3563, the loss is 1699.5953348950038, parameters k is -48.711602648178136 and b is 325.6346234828767\n",
      "Iteration 3564, the loss is 1698.863293379955, parameters k is -48.69860857703189 and b is 325.5516195303075\n",
      "Iteration 3565, the loss is 1698.1314218404054, parameters k is -48.685614505885646 and b is 325.4686155777383\n",
      "Iteration 3566, the loss is 1697.3997202763626, parameters k is -48.6726204347394 and b is 325.38561162516913\n",
      "Iteration 3567, the loss is 1696.6681886878139, parameters k is -48.659626363593155 and b is 325.30260767259995\n",
      "Iteration 3568, the loss is 1695.9368270747668, parameters k is -48.64663229244691 and b is 325.21960372003076\n",
      "Iteration 3569, the loss is 1695.2056354372219, parameters k is -48.633638221300664 and b is 325.1365997674616\n",
      "Iteration 3570, the loss is 1694.4505805231133, parameters k is -48.62064415015442 and b is 325.0575483840624\n",
      "Iteration 3571, the loss is 1693.7197182360421, parameters k is -48.607650079008174 and b is 324.9745444314932\n",
      "Iteration 3572, the loss is 1692.989025924475, parameters k is -48.59465600786193 and b is 324.891540478924\n",
      "Iteration 3573, the loss is 1692.2585035884074, parameters k is -48.58166193671568 and b is 324.8085365263548\n",
      "Iteration 3574, the loss is 1691.5281512278386, parameters k is -48.56866786556944 and b is 324.72553257378564\n",
      "Iteration 3575, the loss is 1690.7979688427727, parameters k is -48.55567379442319 and b is 324.64252862121646\n",
      "Iteration 3576, the loss is 1690.0679564332038, parameters k is -48.54267972327695 and b is 324.55952466864727\n",
      "Iteration 3577, the loss is 1689.3381139991372, parameters k is -48.5296856521307 and b is 324.4765207160781\n",
      "Iteration 3578, the loss is 1688.608441540571, parameters k is -48.51669158098446 and b is 324.3935167635089\n",
      "Iteration 3579, the loss is 1687.8789390575055, parameters k is -48.50369750983821 and b is 324.3105128109397\n",
      "Iteration 3580, the loss is 1687.1496065499384, parameters k is -48.490703438691966 and b is 324.2275088583705\n",
      "Iteration 3581, the loss is 1686.4204440178726, parameters k is -48.47770936754572 and b is 324.14450490580134\n",
      "Iteration 3582, the loss is 1685.6914514613072, parameters k is -48.464715296399476 and b is 324.06150095323216\n",
      "Iteration 3583, the loss is 1684.962628880243, parameters k is -48.45172122525323 and b is 323.97849700066297\n",
      "Iteration 3584, the loss is 1684.2339762746765, parameters k is -48.438727154106985 and b is 323.8954930480938\n",
      "Iteration 3585, the loss is 1683.5054936446124, parameters k is -48.42573308296074 and b is 323.8124890955246\n",
      "Iteration 3586, the loss is 1682.7771809900478, parameters k is -48.412739011814494 and b is 323.7294851429554\n",
      "Iteration 3587, the loss is 1682.0490383109866, parameters k is -48.39974494066825 and b is 323.6464811903862\n",
      "Iteration 3588, the loss is 1681.3210656074225, parameters k is -48.386750869522004 and b is 323.56347723781704\n",
      "Iteration 3589, the loss is 1680.5932628793591, parameters k is -48.37375679837576 and b is 323.48047328524785\n",
      "Iteration 3590, the loss is 1679.865630126796, parameters k is -48.36076272722951 and b is 323.39746933267867\n",
      "Iteration 3591, the loss is 1679.138167349734, parameters k is -48.34776865608327 and b is 323.3144653801095\n",
      "Iteration 3592, the loss is 1678.4108745481699, parameters k is -48.33477458493702 and b is 323.2314614275403\n",
      "Iteration 3593, the loss is 1677.6837517221118, parameters k is -48.32178051379078 and b is 323.1484574749711\n",
      "Iteration 3594, the loss is 1676.9567988715485, parameters k is -48.30878644264453 and b is 323.0654535224019\n",
      "Iteration 3595, the loss is 1676.2300159964868, parameters k is -48.29579237149829 and b is 322.98244956983274\n",
      "Iteration 3596, the loss is 1675.5034030969277, parameters k is -48.28279830035204 and b is 322.89944561726355\n",
      "Iteration 3597, the loss is 1674.7769601728648, parameters k is -48.269804229205796 and b is 322.81644166469437\n",
      "Iteration 3598, the loss is 1674.0263884031951, parameters k is -48.25681015805955 and b is 322.7373902812952\n",
      "Iteration 3599, the loss is 1673.3002748296103, parameters k is -48.243816086913306 and b is 322.654386328726\n",
      "Iteration 3600, the loss is 1672.5743312315287, parameters k is -48.23082201576706 and b is 322.5713823761568\n",
      "Iteration 3601, the loss is 1671.8485576089436, parameters k is -48.217827944620815 and b is 322.4883784235876\n",
      "Iteration 3602, the loss is 1671.1229539618616, parameters k is -48.20483387347457 and b is 322.40537447101843\n",
      "Iteration 3603, the loss is 1670.3975202902807, parameters k is -48.191839802328325 and b is 322.32237051844925\n",
      "Iteration 3604, the loss is 1669.6722565941973, parameters k is -48.17884573118208 and b is 322.23936656588006\n",
      "Iteration 3605, the loss is 1668.9471628736171, parameters k is -48.165851660035834 and b is 322.1563626133109\n",
      "Iteration 3606, the loss is 1668.2222391285356, parameters k is -48.15285758888959 and b is 322.0733586607417\n",
      "Iteration 3607, the loss is 1667.4974853589556, parameters k is -48.13986351774334 and b is 321.9903547081725\n",
      "Iteration 3608, the loss is 1666.7729015648729, parameters k is -48.1268694465971 and b is 321.9073507556033\n",
      "Iteration 3609, the loss is 1666.048487746295, parameters k is -48.11387537545085 and b is 321.82434680303413\n",
      "Iteration 3610, the loss is 1665.3242439032153, parameters k is -48.10088130430461 and b is 321.74134285046495\n",
      "Iteration 3611, the loss is 1664.6001700356362, parameters k is -48.08788723315836 and b is 321.65833889789576\n",
      "Iteration 3612, the loss is 1663.8762661435558, parameters k is -48.07489316201212 and b is 321.5753349453266\n",
      "Iteration 3613, the loss is 1663.1525322269772, parameters k is -48.06189909086587 and b is 321.4923309927574\n",
      "Iteration 3614, the loss is 1662.4289682859, parameters k is -48.048905019719626 and b is 321.4093270401882\n",
      "Iteration 3615, the loss is 1661.7055743203207, parameters k is -48.03591094857338 and b is 321.326323087619\n",
      "Iteration 3616, the loss is 1660.9823503302432, parameters k is -48.022916877427136 and b is 321.24331913504983\n",
      "Iteration 3617, the loss is 1660.2592963156642, parameters k is -48.00992280628089 and b is 321.16031518248064\n",
      "Iteration 3618, the loss is 1659.5364122765902, parameters k is -47.996928735134645 and b is 321.07731122991146\n",
      "Iteration 3619, the loss is 1658.8136982130106, parameters k is -47.9839346639884 and b is 320.9943072773423\n",
      "Iteration 3620, the loss is 1658.0911541249347, parameters k is -47.970940592842155 and b is 320.9113033247731\n",
      "Iteration 3621, the loss is 1657.368780012359, parameters k is -47.95794652169591 and b is 320.8282993722039\n",
      "Iteration 3622, the loss is 1656.646575875282, parameters k is -47.944952450549664 and b is 320.7452954196347\n",
      "Iteration 3623, the loss is 1655.9245417137085, parameters k is -47.93195837940342 and b is 320.66229146706553\n",
      "Iteration 3624, the loss is 1655.2026775276324, parameters k is -47.91896430825717 and b is 320.57928751449634\n",
      "Iteration 3625, the loss is 1654.4809833170575, parameters k is -47.90597023711093 and b is 320.49628356192716\n",
      "Iteration 3626, the loss is 1653.7348946918214, parameters k is -47.89297616596468 and b is 320.41723217852797\n",
      "Iteration 3627, the loss is 1653.0135298317236, parameters k is -47.87998209481844 and b is 320.3342282259588\n",
      "Iteration 3628, the loss is 1652.2923349471255, parameters k is -47.86698802367219 and b is 320.2512242733896\n",
      "Iteration 3629, the loss is 1651.5713100380294, parameters k is -47.85399395252595 and b is 320.1682203208204\n",
      "Iteration 3630, the loss is 1650.8504551044332, parameters k is -47.8409998813797 and b is 320.0852163682512\n",
      "Iteration 3631, the loss is 1650.1297701463343, parameters k is -47.828005810233456 and b is 320.00221241568204\n",
      "Iteration 3632, the loss is 1649.4092551637395, parameters k is -47.81501173908721 and b is 319.91920846311285\n",
      "Iteration 3633, the loss is 1648.688910156643, parameters k is -47.802017667940966 and b is 319.83620451054367\n",
      "Iteration 3634, the loss is 1647.9687351250482, parameters k is -47.78902359679472 and b is 319.7532005579745\n",
      "Iteration 3635, the loss is 1647.2487300689513, parameters k is -47.776029525648475 and b is 319.6701966054053\n",
      "Iteration 3636, the loss is 1646.5288949883573, parameters k is -47.76303545450223 and b is 319.5871926528361\n",
      "Iteration 3637, the loss is 1645.8092298832637, parameters k is -47.750041383355985 and b is 319.5041887002669\n",
      "Iteration 3638, the loss is 1645.08973475367, parameters k is -47.73704731220974 and b is 319.42118474769774\n",
      "Iteration 3639, the loss is 1644.3704095995765, parameters k is -47.724053241063494 and b is 319.33818079512855\n",
      "Iteration 3640, the loss is 1643.6512544209827, parameters k is -47.71105916991725 and b is 319.25517684255937\n",
      "Iteration 3641, the loss is 1642.9322692178891, parameters k is -47.698065098771 and b is 319.1721728899902\n",
      "Iteration 3642, the loss is 1642.2134539902966, parameters k is -47.68507102762476 and b is 319.089168937421\n",
      "Iteration 3643, the loss is 1641.4948087382047, parameters k is -47.67207695647851 and b is 319.0061649848518\n",
      "Iteration 3644, the loss is 1640.7763334616113, parameters k is -47.65908288533227 and b is 318.9231610322826\n",
      "Iteration 3645, the loss is 1640.0580281605207, parameters k is -47.64608881418602 and b is 318.84015707971344\n",
      "Iteration 3646, the loss is 1639.3398928349275, parameters k is -47.63309474303978 and b is 318.75715312714425\n",
      "Iteration 3647, the loss is 1638.621927484836, parameters k is -47.62010067189353 and b is 318.67414917457506\n",
      "Iteration 3648, the loss is 1637.9041321102463, parameters k is -47.60710660074729 and b is 318.5911452220059\n",
      "Iteration 3649, the loss is 1637.1865067111553, parameters k is -47.59411252960104 and b is 318.5081412694367\n",
      "Iteration 3650, the loss is 1636.4690512875636, parameters k is -47.581118458454796 and b is 318.4251373168675\n",
      "Iteration 3651, the loss is 1635.7517658394736, parameters k is -47.56812438730855 and b is 318.3421333642983\n",
      "Iteration 3652, the loss is 1635.034650366884, parameters k is -47.555130316162305 and b is 318.25912941172913\n",
      "Iteration 3653, the loss is 1634.3177048697974, parameters k is -47.54213624501606 and b is 318.17612545915995\n",
      "Iteration 3654, the loss is 1633.5760993889983, parameters k is -47.529142173869815 and b is 318.09707407576076\n",
      "Iteration 3655, the loss is 1632.8594832423837, parameters k is -47.51614810272357 and b is 318.0140701231916\n",
      "Iteration 3656, the loss is 1632.1430370712692, parameters k is -47.503154031577324 and b is 317.9310661706224\n",
      "Iteration 3657, the loss is 1631.4267608756602, parameters k is -47.49015996043108 and b is 317.8480622180532\n",
      "Iteration 3658, the loss is 1630.7106546555483, parameters k is -47.477165889284834 and b is 317.765058265484\n",
      "Iteration 3659, the loss is 1629.9947184109394, parameters k is -47.46417181813859 and b is 317.68205431291483\n",
      "Iteration 3660, the loss is 1629.278952141829, parameters k is -47.45117774699234 and b is 317.59905036034564\n",
      "Iteration 3661, the loss is 1628.5633558482175, parameters k is -47.4381836758461 and b is 317.51604640777646\n",
      "Iteration 3662, the loss is 1627.8479295301092, parameters k is -47.42518960469985 and b is 317.43304245520727\n",
      "Iteration 3663, the loss is 1627.1326731874988, parameters k is -47.41219553355361 and b is 317.3500385026381\n",
      "Iteration 3664, the loss is 1626.417586820389, parameters k is -47.39920146240736 and b is 317.2670345500689\n",
      "Iteration 3665, the loss is 1625.7026704287812, parameters k is -47.38620739126112 and b is 317.1840305974997\n",
      "Iteration 3666, the loss is 1624.9879240126727, parameters k is -47.37321332011487 and b is 317.1010266449305\n",
      "Iteration 3667, the loss is 1624.2733475720634, parameters k is -47.360219248968626 and b is 317.01802269236134\n",
      "Iteration 3668, the loss is 1623.558941106956, parameters k is -47.34722517782238 and b is 316.93501873979216\n",
      "Iteration 3669, the loss is 1622.8447046173492, parameters k is -47.334231106676135 and b is 316.85201478722297\n",
      "Iteration 3670, the loss is 1622.130638103241, parameters k is -47.32123703552989 and b is 316.7690108346538\n",
      "Iteration 3671, the loss is 1621.4167415646355, parameters k is -47.308242964383645 and b is 316.6860068820846\n",
      "Iteration 3672, the loss is 1620.7030150015278, parameters k is -47.2952488932374 and b is 316.6030029295154\n",
      "Iteration 3673, the loss is 1619.9894584139236, parameters k is -47.282254822091154 and b is 316.5199989769462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3674, the loss is 1619.276071801815, parameters k is -47.26926075094491 and b is 316.43699502437704\n",
      "Iteration 3675, the loss is 1618.5628551652098, parameters k is -47.256266679798664 and b is 316.35399107180785\n",
      "Iteration 3676, the loss is 1617.8498085041026, parameters k is -47.24327260865242 and b is 316.27098711923867\n",
      "Iteration 3677, the loss is 1617.1369318185007, parameters k is -47.23027853750617 and b is 316.1879831666695\n",
      "Iteration 3678, the loss is 1616.4242251083951, parameters k is -47.21728446635993 and b is 316.1049792141003\n",
      "Iteration 3679, the loss is 1615.7116883737892, parameters k is -47.20429039521368 and b is 316.0219752615311\n",
      "Iteration 3680, the loss is 1614.9993216146877, parameters k is -47.19129632406744 and b is 315.9389713089619\n",
      "Iteration 3681, the loss is 1614.2871248310819, parameters k is -47.17830225292119 and b is 315.85596735639274\n",
      "Iteration 3682, the loss is 1613.5500024947185, parameters k is -47.16530818177495 and b is 315.77691597299355\n",
      "Iteration 3683, the loss is 1612.838135061593, parameters k is -47.1523141106287 and b is 315.69391202042436\n",
      "Iteration 3684, the loss is 1612.1264376039665, parameters k is -47.139320039482456 and b is 315.6109080678552\n",
      "Iteration 3685, the loss is 1611.41491012184, parameters k is -47.12632596833621 and b is 315.527904115286\n",
      "Iteration 3686, the loss is 1610.7035526152133, parameters k is -47.113331897189966 and b is 315.4449001627168\n",
      "Iteration 3687, the loss is 1609.9923650840894, parameters k is -47.10033782604372 and b is 315.3618962101476\n",
      "Iteration 3688, the loss is 1609.2813475284659, parameters k is -47.087343754897475 and b is 315.27889225757843\n",
      "Iteration 3689, the loss is 1608.570499948341, parameters k is -47.07434968375123 and b is 315.19588830500925\n",
      "Iteration 3690, the loss is 1607.8598223437161, parameters k is -47.061355612604984 and b is 315.11288435244006\n",
      "Iteration 3691, the loss is 1607.1493147145925, parameters k is -47.04836154145874 and b is 315.0298803998709\n",
      "Iteration 3692, the loss is 1606.438977060968, parameters k is -47.035367470312494 and b is 314.9468764473017\n",
      "Iteration 3693, the loss is 1605.728809382845, parameters k is -47.02237339916625 and b is 314.8638724947325\n",
      "Iteration 3694, the loss is 1605.0188116802221, parameters k is -47.00937932802 and b is 314.7808685421633\n",
      "Iteration 3695, the loss is 1604.308983953099, parameters k is -46.99638525687376 and b is 314.69786458959413\n",
      "Iteration 3696, the loss is 1603.5993262014763, parameters k is -46.98339118572751 and b is 314.61486063702495\n",
      "Iteration 3697, the loss is 1602.8898384253548, parameters k is -46.97039711458127 and b is 314.53185668445576\n",
      "Iteration 3698, the loss is 1602.180520624734, parameters k is -46.95740304343502 and b is 314.4488527318866\n",
      "Iteration 3699, the loss is 1601.4713727996132, parameters k is -46.94440897228878 and b is 314.3658487793174\n",
      "Iteration 3700, the loss is 1600.7623949499916, parameters k is -46.93141490114253 and b is 314.2828448267482\n",
      "Iteration 3701, the loss is 1600.053587075871, parameters k is -46.918420829996286 and b is 314.199840874179\n",
      "Iteration 3702, the loss is 1599.34494917725, parameters k is -46.90542675885004 and b is 314.11683692160983\n",
      "Iteration 3703, the loss is 1598.6364812541303, parameters k is -46.892432687703796 and b is 314.03383296904065\n",
      "Iteration 3704, the loss is 1597.9281833065102, parameters k is -46.87943861655755 and b is 313.95082901647146\n",
      "Iteration 3705, the loss is 1597.2200553343912, parameters k is -46.866444545411305 and b is 313.8678250639023\n",
      "Iteration 3706, the loss is 1596.5120973377732, parameters k is -46.85345047426506 and b is 313.7848211113331\n",
      "Iteration 3707, the loss is 1595.8043093166539, parameters k is -46.840456403118814 and b is 313.7018171587639\n",
      "Iteration 3708, the loss is 1595.0966912710358, parameters k is -46.82746233197257 and b is 313.6188132061947\n",
      "Iteration 3709, the loss is 1594.3892432009186, parameters k is -46.814468260826324 and b is 313.53580925362553\n",
      "Iteration 3710, the loss is 1593.6566040089876, parameters k is -46.80147418968008 and b is 313.45675787022634\n",
      "Iteration 3711, the loss is 1592.9494852893483, parameters k is -46.78848011853383 and b is 313.37375391765715\n",
      "Iteration 3712, the loss is 1592.242536545206, parameters k is -46.77548604738759 and b is 313.29074996508797\n",
      "Iteration 3713, the loss is 1591.5357577765678, parameters k is -46.76249197624134 and b is 313.2077460125188\n",
      "Iteration 3714, the loss is 1590.8291489834278, parameters k is -46.7494979050951 and b is 313.1247420599496\n",
      "Iteration 3715, the loss is 1590.1227101657873, parameters k is -46.73650383394885 and b is 313.0417381073804\n",
      "Iteration 3716, the loss is 1589.4164413236467, parameters k is -46.72350976280261 and b is 312.9587341548112\n",
      "Iteration 3717, the loss is 1588.71034245701, parameters k is -46.71051569165636 and b is 312.87573020224204\n",
      "Iteration 3718, the loss is 1588.0044135658684, parameters k is -46.697521620510116 and b is 312.79272624967285\n",
      "Iteration 3719, the loss is 1587.2986546502307, parameters k is -46.68452754936387 and b is 312.70972229710367\n",
      "Iteration 3720, the loss is 1586.5930657100946, parameters k is -46.671533478217626 and b is 312.6267183445345\n",
      "Iteration 3721, the loss is 1585.8876467454575, parameters k is -46.65853940707138 and b is 312.5437143919653\n",
      "Iteration 3722, the loss is 1585.1823977563206, parameters k is -46.645545335925135 and b is 312.4607104393961\n",
      "Iteration 3723, the loss is 1584.4773187426836, parameters k is -46.63255126477889 and b is 312.3777064868269\n",
      "Iteration 3724, the loss is 1583.772409704546, parameters k is -46.619557193632644 and b is 312.29470253425774\n",
      "Iteration 3725, the loss is 1583.067670641911, parameters k is -46.6065631224864 and b is 312.21169858168855\n",
      "Iteration 3726, the loss is 1582.3631015547735, parameters k is -46.593569051340154 and b is 312.12869462911937\n",
      "Iteration 3727, the loss is 1581.6587024431392, parameters k is -46.58057498019391 and b is 312.0456906765502\n",
      "Iteration 3728, the loss is 1580.9544733070045, parameters k is -46.56758090904766 and b is 311.962686723981\n",
      "Iteration 3729, the loss is 1580.2504141463653, parameters k is -46.55458683790142 and b is 311.8796827714118\n",
      "Iteration 3730, the loss is 1579.5465249612314, parameters k is -46.54159276675517 and b is 311.7966788188426\n",
      "Iteration 3731, the loss is 1578.8428057515987, parameters k is -46.52859869560893 and b is 311.71367486627344\n",
      "Iteration 3732, the loss is 1578.1392565174647, parameters k is -46.51560462446268 and b is 311.63067091370425\n",
      "Iteration 3733, the loss is 1577.4358772588319, parameters k is -46.50261055331644 and b is 311.54766696113506\n",
      "Iteration 3734, the loss is 1576.7326679756957, parameters k is -46.48961648217019 and b is 311.4646630085659\n",
      "Iteration 3735, the loss is 1576.0296286680627, parameters k is -46.476622411023946 and b is 311.3816590559967\n",
      "Iteration 3736, the loss is 1575.3267593359305, parameters k is -46.4636283398777 and b is 311.2986551034275\n",
      "Iteration 3737, the loss is 1574.6240599792998, parameters k is -46.450634268731456 and b is 311.2156511508583\n",
      "Iteration 3738, the loss is 1573.895903931806, parameters k is -46.43764019758521 and b is 311.13659976745913\n",
      "Iteration 3739, the loss is 1573.1935339256524, parameters k is -46.424646126438965 and b is 311.05359581488995\n",
      "Iteration 3740, the loss is 1572.491333894995, parameters k is -46.41165205529272 and b is 310.97059186232076\n",
      "Iteration 3741, the loss is 1571.789303839842, parameters k is -46.398657984146475 and b is 310.8875879097516\n",
      "Iteration 3742, the loss is 1571.0874437601863, parameters k is -46.38566391300023 and b is 310.8045839571824\n",
      "Iteration 3743, the loss is 1570.3857536560333, parameters k is -46.372669841853984 and b is 310.7215800046132\n",
      "Iteration 3744, the loss is 1569.6842335273795, parameters k is -46.35967577070774 and b is 310.638576052044\n",
      "Iteration 3745, the loss is 1568.9828833742256, parameters k is -46.34668169956149 and b is 310.55557209947483\n",
      "Iteration 3746, the loss is 1568.2817031965735, parameters k is -46.33368762841525 and b is 310.47256814690564\n",
      "Iteration 3747, the loss is 1567.5806929944208, parameters k is -46.320693557269 and b is 310.38956419433646\n",
      "Iteration 3748, the loss is 1566.8798527677657, parameters k is -46.30769948612276 and b is 310.3065602417673\n",
      "Iteration 3749, the loss is 1566.179182516618, parameters k is -46.29470541497651 and b is 310.2235562891981\n",
      "Iteration 3750, the loss is 1565.4786822409642, parameters k is -46.28171134383027 and b is 310.1405523366289\n",
      "Iteration 3751, the loss is 1564.7783519408124, parameters k is -46.26871727268402 and b is 310.0575483840597\n",
      "Iteration 3752, the loss is 1564.0781916161618, parameters k is -46.255723201537776 and b is 309.97454443149053\n",
      "Iteration 3753, the loss is 1563.3782012670104, parameters k is -46.24272913039153 and b is 309.89154047892134\n",
      "Iteration 3754, the loss is 1562.6783808933599, parameters k is -46.229735059245286 and b is 309.80853652635216\n",
      "Iteration 3755, the loss is 1561.9787304952108, parameters k is -46.21674098809904 and b is 309.72553257378297\n",
      "Iteration 3756, the loss is 1561.2792500725618, parameters k is -46.203746916952795 and b is 309.6425286212138\n",
      "Iteration 3757, the loss is 1560.5799396254108, parameters k is -46.19075284580655 and b is 309.5595246686446\n",
      "Iteration 3758, the loss is 1559.880799153762, parameters k is -46.177758774660305 and b is 309.4765207160754\n",
      "Iteration 3759, the loss is 1559.1818286576129, parameters k is -46.16476470351406 and b is 309.3935167635062\n",
      "Iteration 3760, the loss is 1558.4830281369636, parameters k is -46.151770632367814 and b is 309.31051281093704\n",
      "Iteration 3761, the loss is 1557.7843975918177, parameters k is -46.13877656122157 and b is 309.22750885836786\n",
      "Iteration 3762, the loss is 1557.085937022168, parameters k is -46.12578249007532 and b is 309.14450490579867\n",
      "Iteration 3763, the loss is 1556.387646428021, parameters k is -46.11278841892908 and b is 309.0615009532295\n",
      "Iteration 3764, the loss is 1555.6895258093716, parameters k is -46.09979434778283 and b is 308.9784970006603\n",
      "Iteration 3765, the loss is 1554.9915751662263, parameters k is -46.08680027663659 and b is 308.8954930480911\n",
      "Iteration 3766, the loss is 1554.2679022631705, parameters k is -46.07380620549034 and b is 308.8164416646919\n",
      "Iteration 3767, the loss is 1553.5702809705015, parameters k is -46.0608121343441 and b is 308.73343771212274\n",
      "Iteration 3768, the loss is 1552.8728296533345, parameters k is -46.04781806319785 and b is 308.65043375955355\n",
      "Iteration 3769, the loss is 1552.1755483116635, parameters k is -46.03482399205161 and b is 308.56742980698436\n",
      "Iteration 3770, the loss is 1551.4784369454953, parameters k is -46.02182992090536 and b is 308.4844258544152\n",
      "Iteration 3771, the loss is 1550.7814955548258, parameters k is -46.008835849759116 and b is 308.401421901846\n",
      "Iteration 3772, the loss is 1550.084724139656, parameters k is -45.99584177861287 and b is 308.3184179492768\n",
      "Iteration 3773, the loss is 1549.3881226999906, parameters k is -45.982847707466625 and b is 308.2354139967076\n",
      "Iteration 3774, the loss is 1548.6916912358245, parameters k is -45.96985363632038 and b is 308.15241004413843\n",
      "Iteration 3775, the loss is 1547.9954297471556, parameters k is -45.956859565174135 and b is 308.06940609156925\n",
      "Iteration 3776, the loss is 1547.29933823399, parameters k is -45.94386549402789 and b is 307.98640213900006\n",
      "Iteration 3777, the loss is 1546.6034166963223, parameters k is -45.930871422881644 and b is 307.9033981864309\n",
      "Iteration 3778, the loss is 1545.9076651341566, parameters k is -45.9178773517354 and b is 307.8203942338617\n",
      "Iteration 3779, the loss is 1545.2120835474905, parameters k is -45.904883280589154 and b is 307.7373902812925\n",
      "Iteration 3780, the loss is 1544.5166719363265, parameters k is -45.89188920944291 and b is 307.6543863287233\n",
      "Iteration 3781, the loss is 1543.8214303006596, parameters k is -45.87889513829666 and b is 307.57138237615413\n",
      "Iteration 3782, the loss is 1543.1263586404962, parameters k is -45.86590106715042 and b is 307.48837842358495\n",
      "Iteration 3783, the loss is 1542.4314569558308, parameters k is -45.85290699600417 and b is 307.40537447101576\n",
      "Iteration 3784, the loss is 1541.736725246668, parameters k is -45.83991292485793 and b is 307.3223705184466\n",
      "Iteration 3785, the loss is 1541.0421635130026, parameters k is -45.82691885371168 and b is 307.2393665658774\n",
      "Iteration 3786, the loss is 1540.3477717548383, parameters k is -45.81392478256544 and b is 307.1563626133082\n",
      "Iteration 3787, the loss is 1539.6535499721754, parameters k is -45.80093071141919 and b is 307.073358660739\n",
      "Iteration 3788, the loss is 1538.9594981650125, parameters k is -45.787936640272946 and b is 306.99035470816983\n",
      "Iteration 3789, the loss is 1538.2656163333506, parameters k is -45.7749425691267 and b is 306.90735075560065\n",
      "Iteration 3790, the loss is 1537.5719044771881, parameters k is -45.761948497980455 and b is 306.82434680303146\n",
      "Iteration 3791, the loss is 1536.8783625965252, parameters k is -45.74895442683421 and b is 306.7413428504623\n",
      "Iteration 3792, the loss is 1536.1849906913656, parameters k is -45.735960355687965 and b is 306.6583388978931\n",
      "Iteration 3793, the loss is 1535.4917887617037, parameters k is -45.72296628454172 and b is 306.5753349453239\n",
      "Iteration 3794, the loss is 1534.7725990030845, parameters k is -45.709972213395474 and b is 306.4962835619247\n",
      "Iteration 3795, the loss is 1534.0797264238993, parameters k is -45.69697814224923 and b is 306.4132796093555\n",
      "Iteration 3796, the loss is 1533.3870238202155, parameters k is -45.683984071102984 and b is 306.33027565678634\n",
      "Iteration 3797, the loss is 1532.6944911920334, parameters k is -45.67098999995674 and b is 306.24727170421716\n",
      "Iteration 3798, the loss is 1532.0021285393495, parameters k is -45.65799592881049 and b is 306.16426775164797\n",
      "Iteration 3799, the loss is 1531.3099358621657, parameters k is -45.64500185766425 and b is 306.0812637990788\n",
      "Iteration 3800, the loss is 1530.617913160485, parameters k is -45.632007786518 and b is 305.9982598465096\n",
      "Iteration 3801, the loss is 1529.9260604343024, parameters k is -45.61901371537176 and b is 305.9152558939404\n",
      "Iteration 3802, the loss is 1529.2343776836215, parameters k is -45.60601964422551 and b is 305.8322519413712\n",
      "Iteration 3803, the loss is 1528.5428649084386, parameters k is -45.59302557307927 and b is 305.74924798880204\n",
      "Iteration 3804, the loss is 1527.8515221087562, parameters k is -45.58003150193302 and b is 305.66624403623285\n",
      "Iteration 3805, the loss is 1527.1603492845766, parameters k is -45.567037430786776 and b is 305.58324008366367\n",
      "Iteration 3806, the loss is 1526.469346435895, parameters k is -45.55404335964053 and b is 305.5002361310945\n",
      "Iteration 3807, the loss is 1525.7785135627166, parameters k is -45.541049288494285 and b is 305.4172321785253\n",
      "Iteration 3808, the loss is 1525.0878506650363, parameters k is -45.52805521734804 and b is 305.3342282259561\n",
      "Iteration 3809, the loss is 1524.3973577428565, parameters k is -45.515061146201795 and b is 305.2512242733869\n",
      "Iteration 3810, the loss is 1523.7070347961774, parameters k is -45.50206707505555 and b is 305.16822032081774\n",
      "Iteration 3811, the loss is 1523.0168818249972, parameters k is -45.489073003909304 and b is 305.08521636824855\n",
      "Iteration 3812, the loss is 1522.326898829319, parameters k is -45.47607893276306 and b is 305.00221241567937\n",
      "Iteration 3813, the loss is 1521.6370858091411, parameters k is -45.463084861616814 and b is 304.9192084631102\n",
      "Iteration 3814, the loss is 1520.947442764463, parameters k is -45.45009079047057 and b is 304.836204510541\n",
      "Iteration 3815, the loss is 1520.2579696952866, parameters k is -45.43709671932432 and b is 304.7532005579718\n",
      "Iteration 3816, the loss is 1519.5686666016077, parameters k is -45.42410264817808 and b is 304.6701966054026\n",
      "Iteration 3817, the loss is 1518.879533483432, parameters k is -45.41110857703183 and b is 304.58719265283344\n",
      "Iteration 3818, the loss is 1518.190570340755, parameters k is -45.39811450588559 and b is 304.50418870026425\n",
      "Iteration 3819, the loss is 1517.5017771735781, parameters k is -45.38512043473934 and b is 304.42118474769507\n",
      "Iteration 3820, the loss is 1516.813153981903, parameters k is -45.3721263635931 and b is 304.3381807951259\n",
      "Iteration 3821, the loss is 1516.1247007657278, parameters k is -45.35913229244685 and b is 304.2551768425567\n",
      "Iteration 3822, the loss is 1515.4099941515422, parameters k is -45.346138221300606 and b is 304.1761254591575\n",
      "Iteration 3823, the loss is 1514.7218702858438, parameters k is -45.33314415015436 and b is 304.0931215065883\n",
      "Iteration 3824, the loss is 1514.0339163956464, parameters k is -45.320150079008116 and b is 304.01011755401913\n",
      "Iteration 3825, the loss is 1513.3461324809502, parameters k is -45.30715600786187 and b is 303.92711360144995\n",
      "Iteration 3826, the loss is 1512.6585185417512, parameters k is -45.294161936715625 and b is 303.84410964888076\n",
      "Iteration 3827, the loss is 1511.9710745780567, parameters k is -45.28116786556938 and b is 303.7611056963116\n",
      "Iteration 3828, the loss is 1511.283800589857, parameters k is -45.268173794423134 and b is 303.6781017437424\n",
      "Iteration 3829, the loss is 1510.5966965771627, parameters k is -45.25517972327689 and b is 303.5950977911732\n",
      "Iteration 3830, the loss is 1509.9097625399656, parameters k is -45.242185652130644 and b is 303.512093838604\n",
      "Iteration 3831, the loss is 1509.22299847827, parameters k is -45.2291915809844 and b is 303.42908988603483\n",
      "Iteration 3832, the loss is 1508.536404392072, parameters k is -45.21619750983815 and b is 303.34608593346564\n",
      "Iteration 3833, the loss is 1507.8499802813803, parameters k is -45.20320343869191 and b is 303.26308198089646\n",
      "Iteration 3834, the loss is 1507.163726146185, parameters k is -45.19020936754566 and b is 303.1800780283273\n",
      "Iteration 3835, the loss is 1506.4776419864888, parameters k is -45.17721529639942 and b is 303.0970740757581\n",
      "Iteration 3836, the loss is 1505.7917278022942, parameters k is -45.16422122525317 and b is 303.0140701231889\n",
      "Iteration 3837, the loss is 1505.1059835936, parameters k is -45.15122715410693 and b is 302.9310661706197\n",
      "Iteration 3838, the loss is 1504.4204093604094, parameters k is -45.13823308296068 and b is 302.84806221805053\n",
      "Iteration 3839, the loss is 1503.7350051027136, parameters k is -45.125239011814436 and b is 302.76505826548134\n",
      "Iteration 3840, the loss is 1503.0497708205207, parameters k is -45.11224494066819 and b is 302.68205431291216\n",
      "Iteration 3841, the loss is 1502.364706513828, parameters k is -45.099250869521946 and b is 302.59905036034297\n",
      "Iteration 3842, the loss is 1501.6798121826346, parameters k is -45.0862567983757 and b is 302.5160464077738\n",
      "Iteration 3843, the loss is 1500.9950878269458, parameters k is -45.073262727229455 and b is 302.4330424552046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3844, the loss is 1500.3105334467525, parameters k is -45.06026865608321 and b is 302.3500385026354\n",
      "Iteration 3845, the loss is 1499.6261490420604, parameters k is -45.047274584936964 and b is 302.2670345500662\n",
      "Iteration 3846, the loss is 1498.9419346128711, parameters k is -45.03428051379072 and b is 302.18403059749704\n",
      "Iteration 3847, the loss is 1498.2578901591778, parameters k is -45.021286442644474 and b is 302.10102664492786\n",
      "Iteration 3848, the loss is 1497.5740156809884, parameters k is -45.00829237149823 and b is 302.01802269235867\n",
      "Iteration 3849, the loss is 1496.8903111783002, parameters k is -44.99529830035198 and b is 301.9350187397895\n",
      "Iteration 3850, the loss is 1496.2067766511088, parameters k is -44.98230422920574 and b is 301.8520147872203\n",
      "Iteration 3851, the loss is 1495.496712556338, parameters k is -44.96931015805949 and b is 301.7729634038211\n",
      "Iteration 3852, the loss is 1494.8135073796238, parameters k is -44.95631608691325 and b is 301.6899594512519\n",
      "Iteration 3853, the loss is 1494.1304721784124, parameters k is -44.943322015767 and b is 301.60695549868274\n",
      "Iteration 3854, the loss is 1493.447606952701, parameters k is -44.93032794462076 and b is 301.52395154611355\n",
      "Iteration 3855, the loss is 1492.764911702491, parameters k is -44.91733387347451 and b is 301.44094759354437\n",
      "Iteration 3856, the loss is 1492.0823864277786, parameters k is -44.904339802328266 and b is 301.3579436409752\n",
      "Iteration 3857, the loss is 1491.4000311285665, parameters k is -44.89134573118202 and b is 301.274939688406\n",
      "Iteration 3858, the loss is 1490.7178458048566, parameters k is -44.878351660035776 and b is 301.1919357358368\n",
      "Iteration 3859, the loss is 1490.0358304566457, parameters k is -44.86535758888953 and b is 301.1089317832676\n",
      "Iteration 3860, the loss is 1489.3539850839377, parameters k is -44.852363517743285 and b is 301.02592783069844\n",
      "Iteration 3861, the loss is 1488.6723096867263, parameters k is -44.83936944659704 and b is 300.94292387812925\n",
      "Iteration 3862, the loss is 1487.9908042650193, parameters k is -44.826375375450795 and b is 300.85991992556006\n",
      "Iteration 3863, the loss is 1487.3094688188073, parameters k is -44.81338130430455 and b is 300.7769159729909\n",
      "Iteration 3864, the loss is 1486.6283033481004, parameters k is -44.800387233158304 and b is 300.6939120204217\n",
      "Iteration 3865, the loss is 1485.947307852893, parameters k is -44.78739316201206 and b is 300.6109080678525\n",
      "Iteration 3866, the loss is 1485.266482333183, parameters k is -44.77439909086581 and b is 300.5279041152833\n",
      "Iteration 3867, the loss is 1484.585826788977, parameters k is -44.76140501971957 and b is 300.44490016271413\n",
      "Iteration 3868, the loss is 1483.9053412202688, parameters k is -44.74841094857332 and b is 300.36189621014495\n",
      "Iteration 3869, the loss is 1483.225025627063, parameters k is -44.73541687742708 and b is 300.27889225757576\n",
      "Iteration 3870, the loss is 1482.5448800093543, parameters k is -44.72242280628083 and b is 300.1958883050066\n",
      "Iteration 3871, the loss is 1481.864904367148, parameters k is -44.70942873513459 and b is 300.1128843524374\n",
      "Iteration 3872, the loss is 1481.1850987004425, parameters k is -44.69643466398834 and b is 300.0298803998682\n",
      "Iteration 3873, the loss is 1480.505463009238, parameters k is -44.683440592842096 and b is 299.946876447299\n",
      "Iteration 3874, the loss is 1479.825997293532, parameters k is -44.67044652169585 and b is 299.86387249472983\n",
      "Iteration 3875, the loss is 1479.1467015533265, parameters k is -44.657452450549606 and b is 299.78086854216065\n",
      "Iteration 3876, the loss is 1478.4675757886218, parameters k is -44.64445837940336 and b is 299.69786458959146\n",
      "Iteration 3877, the loss is 1477.7886199994168, parameters k is -44.631464308257115 and b is 299.6148606370223\n",
      "Iteration 3878, the loss is 1477.1098341857148, parameters k is -44.61847023711087 and b is 299.5318566844531\n",
      "Iteration 3879, the loss is 1476.4042532353794, parameters k is -44.605476165964625 and b is 299.4528053010539\n",
      "Iteration 3880, the loss is 1475.7257967721516, parameters k is -44.59248209481838 and b is 299.3698013484847\n",
      "Iteration 3881, the loss is 1475.0475102844246, parameters k is -44.579488023672134 and b is 299.2867973959155\n",
      "Iteration 3882, the loss is 1474.369393772198, parameters k is -44.56649395252589 and b is 299.20379344334634\n",
      "Iteration 3883, the loss is 1473.6914472354736, parameters k is -44.55349988137964 and b is 299.12078949077716\n",
      "Iteration 3884, the loss is 1473.0136706742471, parameters k is -44.5405058102334 and b is 299.03778553820797\n",
      "Iteration 3885, the loss is 1472.3360640885223, parameters k is -44.52751173908715 and b is 298.9547815856388\n",
      "Iteration 3886, the loss is 1471.658627478297, parameters k is -44.51451766794091 and b is 298.8717776330696\n",
      "Iteration 3887, the loss is 1470.9813608435745, parameters k is -44.50152359679466 and b is 298.7887736805004\n",
      "Iteration 3888, the loss is 1470.304264184348, parameters k is -44.48852952564842 and b is 298.7057697279312\n",
      "Iteration 3889, the loss is 1469.6273375006233, parameters k is -44.47553545450217 and b is 298.62276577536204\n",
      "Iteration 3890, the loss is 1468.950580792401, parameters k is -44.462541383355926 and b is 298.53976182279285\n",
      "Iteration 3891, the loss is 1468.2739940596769, parameters k is -44.44954731220968 and b is 298.45675787022367\n",
      "Iteration 3892, the loss is 1467.5975773024543, parameters k is -44.436553241063436 and b is 298.3737539176545\n",
      "Iteration 3893, the loss is 1466.9213305207306, parameters k is -44.42355916991719 and b is 298.2907499650853\n",
      "Iteration 3894, the loss is 1466.2452537145093, parameters k is -44.410565098770945 and b is 298.2077460125161\n",
      "Iteration 3895, the loss is 1465.5693468837865, parameters k is -44.3975710276247 and b is 298.1247420599469\n",
      "Iteration 3896, the loss is 1464.8936100285657, parameters k is -44.384576956478455 and b is 298.04173810737774\n",
      "Iteration 3897, the loss is 1464.218043148843, parameters k is -44.37158288533221 and b is 297.95873415480855\n",
      "Iteration 3898, the loss is 1463.5426462446235, parameters k is -44.358588814185964 and b is 297.87573020223937\n",
      "Iteration 3899, the loss is 1462.8674193159015, parameters k is -44.34559474303972 and b is 297.7927262496702\n",
      "Iteration 3900, the loss is 1462.192362362681, parameters k is -44.33260067189347 and b is 297.709722297101\n",
      "Iteration 3901, the loss is 1461.5174753849597, parameters k is -44.31960660074723 and b is 297.6267183445318\n",
      "Iteration 3902, the loss is 1460.8427583827402, parameters k is -44.30661252960098 and b is 297.5437143919626\n",
      "Iteration 3903, the loss is 1460.1682113560228, parameters k is -44.29361845845474 and b is 297.46071043939344\n",
      "Iteration 3904, the loss is 1459.4938343048016, parameters k is -44.28062438730849 and b is 297.37770648682425\n",
      "Iteration 3905, the loss is 1458.8196272290852, parameters k is -44.26763031616225 and b is 297.29470253425507\n",
      "Iteration 3906, the loss is 1458.1455901288662, parameters k is -44.254636245016 and b is 297.2116985816859\n",
      "Iteration 3907, the loss is 1457.4444923229662, parameters k is -44.24164217386976 and b is 297.1326471982867\n",
      "Iteration 3908, the loss is 1456.7707845732268, parameters k is -44.22864810272351 and b is 297.0496432457175\n",
      "Iteration 3909, the loss is 1456.0972467989855, parameters k is -44.215654031577266 and b is 296.9666392931483\n",
      "Iteration 3910, the loss is 1455.423879000245, parameters k is -44.20265996043102 and b is 296.88363534057913\n",
      "Iteration 3911, the loss is 1454.7506811770033, parameters k is -44.189665889284775 and b is 296.80063138800995\n",
      "Iteration 3912, the loss is 1454.0776533292637, parameters k is -44.17667181813853 and b is 296.71762743544076\n",
      "Iteration 3913, the loss is 1453.404795457024, parameters k is -44.163677746992285 and b is 296.6346234828716\n",
      "Iteration 3914, the loss is 1452.7321075602856, parameters k is -44.15068367584604 and b is 296.5516195303024\n",
      "Iteration 3915, the loss is 1452.0595896390453, parameters k is -44.137689604699794 and b is 296.4686155777332\n",
      "Iteration 3916, the loss is 1451.3872416933064, parameters k is -44.12469553355355 and b is 296.385611625164\n",
      "Iteration 3917, the loss is 1450.7150637230686, parameters k is -44.111701462407304 and b is 296.30260767259483\n",
      "Iteration 3918, the loss is 1450.0430557283298, parameters k is -44.09870739126106 and b is 296.21960372002565\n",
      "Iteration 3919, the loss is 1449.3712177090918, parameters k is -44.08571332011481 and b is 296.13659976745646\n",
      "Iteration 3920, the loss is 1448.6995496653535, parameters k is -44.07271924896857 and b is 296.0535958148873\n",
      "Iteration 3921, the loss is 1448.0280515971172, parameters k is -44.05972517782232 and b is 295.9705918623181\n",
      "Iteration 3922, the loss is 1447.3567235043809, parameters k is -44.04673110667608 and b is 295.8875879097489\n",
      "Iteration 3923, the loss is 1446.6855653871467, parameters k is -44.03373703552983 and b is 295.8045839571797\n",
      "Iteration 3924, the loss is 1446.0145772454096, parameters k is -44.02074296438359 and b is 295.72158000461053\n",
      "Iteration 3925, the loss is 1445.3437590791732, parameters k is -44.00774889323734 and b is 295.63857605204134\n",
      "Iteration 3926, the loss is 1444.6731108884376, parameters k is -43.994754822091096 and b is 295.55557209947216\n",
      "Iteration 3927, the loss is 1444.0026326732045, parameters k is -43.98176075094485 and b is 295.472568146903\n",
      "Iteration 3928, the loss is 1443.332324433468, parameters k is -43.968766679798605 and b is 295.3895641943338\n",
      "Iteration 3929, the loss is 1442.662186169232, parameters k is -43.95577260865236 and b is 295.3065602417646\n",
      "Iteration 3930, the loss is 1441.9922178804984, parameters k is -43.942778537506115 and b is 295.2235562891954\n",
      "Iteration 3931, the loss is 1441.3224195672644, parameters k is -43.92978446635987 and b is 295.14055233662623\n",
      "Iteration 3932, the loss is 1440.65279122953, parameters k is -43.916790395213624 and b is 295.05754838405704\n",
      "Iteration 3933, the loss is 1439.9833328672967, parameters k is -43.90379632406738 and b is 294.97454443148786\n",
      "Iteration 3934, the loss is 1439.3140444805651, parameters k is -43.890802252921134 and b is 294.89154047891867\n",
      "Iteration 3935, the loss is 1438.6174298191015, parameters k is -43.87780818177489 and b is 294.8124890955195\n",
      "Iteration 3936, the loss is 1437.9484707828462, parameters k is -43.86481411062864 and b is 294.7294851429503\n",
      "Iteration 3937, the loss is 1437.2796817220906, parameters k is -43.8518200394824 and b is 294.6464811903811\n",
      "Iteration 3938, the loss is 1436.6110626368356, parameters k is -43.83882596833615 and b is 294.5634772378119\n",
      "Iteration 3939, the loss is 1435.9426135270808, parameters k is -43.82583189718991 and b is 294.48047328524274\n",
      "Iteration 3940, the loss is 1435.2743343928282, parameters k is -43.81283782604366 and b is 294.39746933267355\n",
      "Iteration 3941, the loss is 1434.6062252340719, parameters k is -43.79984375489742 and b is 294.31446538010437\n",
      "Iteration 3942, the loss is 1433.93828605082, parameters k is -43.78684968375117 and b is 294.2314614275352\n",
      "Iteration 3943, the loss is 1433.270516843065, parameters k is -43.773855612604926 and b is 294.148457474966\n",
      "Iteration 3944, the loss is 1432.6029176108127, parameters k is -43.76086154145868 and b is 294.0654535223968\n",
      "Iteration 3945, the loss is 1431.935488354059, parameters k is -43.747867470312436 and b is 293.9824495698276\n",
      "Iteration 3946, the loss is 1431.2682290728064, parameters k is -43.73487339916619 and b is 293.89944561725844\n",
      "Iteration 3947, the loss is 1430.6011397670559, parameters k is -43.721879328019945 and b is 293.81644166468925\n",
      "Iteration 3948, the loss is 1429.9066176253755, parameters k is -43.7088852568737 and b is 293.73739028129006\n",
      "Iteration 3949, the loss is 1429.2398576700994, parameters k is -43.695891185727454 and b is 293.6543863287209\n",
      "Iteration 3950, the loss is 1428.5456749234593, parameters k is -43.68289711458121 and b is 293.5753349453217\n",
      "Iteration 3951, the loss is 1427.879244318661, parameters k is -43.669903043434964 and b is 293.4923309927525\n",
      "Iteration 3952, the loss is 1427.185400967053, parameters k is -43.65690897228872 and b is 293.4132796093533\n",
      "Iteration 3953, the loss is 1426.5192997127333, parameters k is -43.64391490114247 and b is 293.3302756567841\n",
      "Iteration 3954, the loss is 1425.8257957561634, parameters k is -43.63092082999623 and b is 293.25122427338493\n",
      "Iteration 3955, the loss is 1425.1600238523174, parameters k is -43.61792675884998 and b is 293.16822032081575\n",
      "Iteration 3956, the loss is 1424.4668592907865, parameters k is -43.60493268770374 and b is 293.08916893741656\n",
      "Iteration 3957, the loss is 1423.8014167374192, parameters k is -43.59193861655749 and b is 293.0061649848474\n",
      "Iteration 3958, the loss is 1423.136144159553, parameters k is -43.57894454541125 and b is 292.9231610322782\n",
      "Iteration 3959, the loss is 1422.4434783680335, parameters k is -43.565950474265 and b is 292.844109648879\n",
      "Iteration 3960, the loss is 1421.7785351406467, parameters k is -43.552956403118756 and b is 292.7611056963098\n",
      "Iteration 3961, the loss is 1421.0862087441626, parameters k is -43.53996233197251 and b is 292.6820543129106\n",
      "Iteration 3962, the loss is 1420.4215948672493, parameters k is -43.526968260826266 and b is 292.59905036034144\n",
      "Iteration 3963, the loss is 1419.729607865802, parameters k is -43.51397418968002 and b is 292.51999897694225\n",
      "Iteration 3964, the loss is 1419.06532333937, parameters k is -43.500980118533775 and b is 292.43699502437306\n",
      "Iteration 3965, the loss is 1418.3736757329589, parameters k is -43.48798604738753 and b is 292.3579436409739\n",
      "Iteration 3966, the loss is 1417.709720557002, parameters k is -43.474991976241284 and b is 292.2749396884047\n",
      "Iteration 3967, the loss is 1417.0184123456252, parameters k is -43.46199790509504 and b is 292.1958883050055\n",
      "Iteration 3968, the loss is 1416.354786520148, parameters k is -43.449003833948794 and b is 292.1128843524363\n",
      "Iteration 3969, the loss is 1415.6913306701701, parameters k is -43.43600976280255 and b is 292.0298803998671\n",
      "Iteration 3970, the loss is 1415.0005212288083, parameters k is -43.4230156916563 and b is 291.95082901646794\n",
      "Iteration 3971, the loss is 1414.337394729306, parameters k is -43.41002162051006 and b is 291.86782506389875\n",
      "Iteration 3972, the loss is 1413.646924682982, parameters k is -43.39702754936381 and b is 291.78877368049956\n",
      "Iteration 3973, the loss is 1412.9841275339568, parameters k is -43.38403347821757 and b is 291.7057697279304\n",
      "Iteration 3974, the loss is 1412.2939968826697, parameters k is -43.37103940707132 and b is 291.6267183445312\n",
      "Iteration 3975, the loss is 1411.631529084123, parameters k is -43.35804533592508 and b is 291.543714391962\n",
      "Iteration 3976, the loss is 1410.941737827871, parameters k is -43.34505126477883 and b is 291.4646630085628\n",
      "Iteration 3977, the loss is 1410.2795993798006, parameters k is -43.332057193632586 and b is 291.3816590559936\n",
      "Iteration 3978, the loss is 1409.5901475185874, parameters k is -43.31906312248634 and b is 291.30260767259443\n",
      "Iteration 3979, the loss is 1408.928338420993, parameters k is -43.306069051340096 and b is 291.21960372002525\n",
      "Iteration 3980, the loss is 1408.2392259548137, parameters k is -43.29307498019385 and b is 291.14055233662606\n",
      "Iteration 3981, the loss is 1407.577746207698, parameters k is -43.280080909047605 and b is 291.0575483840569\n",
      "Iteration 3982, the loss is 1406.9164364360824, parameters k is -43.26708683790136 and b is 290.9745444314877\n",
      "Iteration 3983, the loss is 1406.2278227399174, parameters k is -43.254092766755114 and b is 290.8954930480885\n",
      "Iteration 3984, the loss is 1405.566842318779, parameters k is -43.24109869560887 and b is 290.8124890955193\n",
      "Iteration 3985, the loss is 1404.8785680176525, parameters k is -43.228104624462624 and b is 290.7334377121201\n",
      "Iteration 3986, the loss is 1404.2179169469905, parameters k is -43.21511055331638 and b is 290.65043375955094\n",
      "Iteration 3987, the loss is 1403.529982040897, parameters k is -43.20211648217013 and b is 290.57138237615175\n",
      "Iteration 3988, the loss is 1402.8696603207125, parameters k is -43.18912241102389 and b is 290.48837842358256\n",
      "Iteration 3989, the loss is 1402.182064809658, parameters k is -43.17612833987764 and b is 290.40932704018337\n",
      "Iteration 3990, the loss is 1401.522072439953, parameters k is -43.1631342687314 and b is 290.3263230876142\n",
      "Iteration 3991, the loss is 1400.834816323933, parameters k is -43.15014019758515 and b is 290.247271704215\n",
      "Iteration 3992, the loss is 1400.1751533047036, parameters k is -43.13714612643891 and b is 290.1642677516458\n",
      "Iteration 3993, the loss is 1399.5156602609761, parameters k is -43.12415205529266 and b is 290.0812637990766\n",
      "Iteration 3994, the loss is 1398.82890291497, parameters k is -43.111157984146416 and b is 290.00221241567743\n",
      "Iteration 3995, the loss is 1398.1697392217186, parameters k is -43.09816391300017 and b is 289.91920846310825\n",
      "Iteration 3996, the loss is 1397.4833212707474, parameters k is -43.085169841853926 and b is 289.84015707970906\n",
      "Iteration 3997, the loss is 1396.8244869279736, parameters k is -43.07217577070768 and b is 289.7571531271399\n",
      "Iteration 3998, the loss is 1396.1384083720418, parameters k is -43.059181699561435 and b is 289.6781017437407\n",
      "Iteration 3999, the loss is 1395.4799033797442, parameters k is -43.04618762841519 and b is 289.5950977911715\n",
      "Iteration 4000, the loss is 1394.7941642188491, parameters k is -43.033193557268945 and b is 289.5160464077723\n",
      "Iteration 4001, the loss is 1394.1359885770285, parameters k is -43.0201994861227 and b is 289.4330424552031\n",
      "Iteration 4002, the loss is 1393.4505888111698, parameters k is -43.007205414976454 and b is 289.35399107180393\n",
      "Iteration 4003, the loss is 1392.7927425198257, parameters k is -42.99421134383021 and b is 289.27098711923475\n",
      "Iteration 4004, the loss is 1392.1350662039831, parameters k is -42.98121727268396 and b is 289.18798316666556\n",
      "Iteration 4005, the loss is 1391.4501652081372, parameters k is -42.96822320153772 and b is 289.10893178326637\n",
      "Iteration 4006, the loss is 1390.7928182427725, parameters k is -42.95522913039147 and b is 289.0259278306972\n",
      "Iteration 4007, the loss is 1390.1082566419618, parameters k is -42.94223505924523 and b is 288.946876447298\n",
      "Iteration 4008, the loss is 1389.4512390270734, parameters k is -42.92924098809898 and b is 288.8638724947288\n",
      "Iteration 4009, the loss is 1388.7670168212994, parameters k is -42.91624691695274 and b is 288.7848211113296\n",
      "Iteration 4010, the loss is 1388.1103285568902, parameters k is -42.90325284580649 and b is 288.70181715876043\n",
      "Iteration 4011, the loss is 1387.4264457461538, parameters k is -42.890258774660246 and b is 288.62276577536124\n",
      "Iteration 4012, the loss is 1386.7700868322197, parameters k is -42.877264703514 and b is 288.53976182279206\n",
      "Iteration 4013, the loss is 1386.0865434165187, parameters k is -42.864270632367756 and b is 288.46071043939287\n",
      "Iteration 4014, the loss is 1385.4305138530626, parameters k is -42.85127656122151 and b is 288.3777064868237\n",
      "Iteration 4015, the loss is 1384.7746542651057, parameters k is -42.838282490075265 and b is 288.2947025342545\n",
      "Iteration 4016, the loss is 1384.0916096194183, parameters k is -42.82528841892902 and b is 288.2156511508553\n",
      "Iteration 4017, the loss is 1383.4360793819394, parameters k is -42.812294347782775 and b is 288.1326471982861\n",
      "Iteration 4018, the loss is 1382.7533741312905, parameters k is -42.79930027663653 and b is 288.05359581488693\n",
      "Iteration 4019, the loss is 1382.0981732442874, parameters k is -42.786306205490284 and b is 287.97059186231775\n",
      "Iteration 4020, the loss is 1381.415807388675, parameters k is -42.77331213434404 and b is 287.89154047891856\n",
      "Iteration 4021, the loss is 1380.7609358521504, parameters k is -42.76031806319779 and b is 287.80853652634937\n",
      "Iteration 4022, the loss is 1380.0789093915726, parameters k is -42.74732399205155 and b is 287.7294851429502\n",
      "Iteration 4023, the loss is 1379.4243672055247, parameters k is -42.7343299209053 and b is 287.646481190381\n",
      "Iteration 4024, the loss is 1378.7426801399836, parameters k is -42.72133584975906 and b is 287.5674298069818\n",
      "Iteration 4025, the loss is 1378.0884673044163, parameters k is -42.70834177861281 and b is 287.4844258544126\n",
      "Iteration 4026, the loss is 1377.407119633911, parameters k is -42.69534770746657 and b is 287.40537447101343\n",
      "Iteration 4027, the loss is 1376.7532361488186, parameters k is -42.68235363632032 and b is 287.32237051844425\n",
      "Iteration 4028, the loss is 1376.0995226392258, parameters k is -42.66935956517408 and b is 287.23936656587506\n",
      "Iteration 4029, the loss is 1375.4186737387354, parameters k is -42.65636549402783 and b is 287.16031518247587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4030, the loss is 1374.765289579619, parameters k is -42.643371422881586 and b is 287.0773112299067\n",
      "Iteration 4031, the loss is 1374.0847800741662, parameters k is -42.63037735173534 and b is 286.9982598465075\n",
      "Iteration 4032, the loss is 1373.4317252655258, parameters k is -42.617383280589095 and b is 286.9152558939383\n",
      "Iteration 4033, the loss is 1372.751555155109, parameters k is -42.60438920944285 and b is 286.8362045105391\n",
      "Iteration 4034, the loss is 1372.0988296969476, parameters k is -42.591395138296605 and b is 286.75320055796993\n",
      "Iteration 4035, the loss is 1371.4189989815673, parameters k is -42.57840106715036 and b is 286.67414917457074\n",
      "Iteration 4036, the loss is 1370.7666028738852, parameters k is -42.565406996004114 and b is 286.59114522200156\n",
      "Iteration 4037, the loss is 1370.0871115535392, parameters k is -42.55241292485787 and b is 286.51209383860237\n",
      "Iteration 4038, the loss is 1369.4350447963336, parameters k is -42.539418853711624 and b is 286.4290898860332\n",
      "Iteration 4039, the loss is 1368.7831480146272, parameters k is -42.52642478256538 and b is 286.346085933464\n",
      "Iteration 4040, the loss is 1368.1041554642952, parameters k is -42.51343071141913 and b is 286.2670345500648\n",
      "Iteration 4041, the loss is 1367.452588033066, parameters k is -42.50043664027289 and b is 286.1840305974956\n",
      "Iteration 4042, the loss is 1366.7739348777702, parameters k is -42.48744256912664 and b is 286.10497921409643\n",
      "Iteration 4043, the loss is 1366.122696797019, parameters k is -42.4744484979804 and b is 286.02197526152725\n",
      "Iteration 4044, the loss is 1365.44438303676, parameters k is -42.46145442683415 and b is 285.94292387812806\n",
      "Iteration 4045, the loss is 1364.7934743064866, parameters k is -42.44846035568791 and b is 285.85991992555887\n",
      "Iteration 4046, the loss is 1364.115499941264, parameters k is -42.43546628454166 and b is 285.7808685421597\n",
      "Iteration 4047, the loss is 1363.4649205614696, parameters k is -42.422472213395416 and b is 285.6978645895905\n",
      "Iteration 4048, the loss is 1362.787285591283, parameters k is -42.40947814224917 and b is 285.6188132061913\n",
      "Iteration 4049, the loss is 1362.1370355619617, parameters k is -42.396484071102925 and b is 285.5358092536221\n",
      "Iteration 4050, the loss is 1361.486955508144, parameters k is -42.38348999995668 and b is 285.45280530105293\n",
      "Iteration 4051, the loss is 1360.8098193079716, parameters k is -42.370495928810435 and b is 285.37375391765374\n",
      "Iteration 4052, the loss is 1360.1600686046297, parameters k is -42.35750185766419 and b is 285.29074996508456\n",
      "Iteration 4053, the loss is 1359.4832717994925, parameters k is -42.344507786517944 and b is 285.21169858168537\n",
      "Iteration 4054, the loss is 1358.8338504466267, parameters k is -42.3315137153717 and b is 285.1286946291162\n",
      "Iteration 4055, the loss is 1358.1573930365296, parameters k is -42.318519644225454 and b is 285.049643245717\n",
      "Iteration 4056, the loss is 1357.5083010341393, parameters k is -42.30552557307921 and b is 284.9666392931478\n",
      "Iteration 4057, the loss is 1356.8321830190764, parameters k is -42.29253150193296 and b is 284.8875879097486\n",
      "Iteration 4058, the loss is 1356.183420367167, parameters k is -42.27953743078672 and b is 284.80458395717943\n",
      "Iteration 4059, the loss is 1355.5076417471414, parameters k is -42.26654335964047 and b is 284.72553257378024\n",
      "Iteration 4060, the loss is 1354.8592084457073, parameters k is -42.25354928849423 and b is 284.64252862121106\n",
      "Iteration 4061, the loss is 1354.2109451197748, parameters k is -42.24055521734798 and b is 284.55952466864187\n",
      "Iteration 4062, the loss is 1353.5356652697592, parameters k is -42.22756114620174 and b is 284.4804732852427\n",
      "Iteration 4063, the loss is 1352.8877312943046, parameters k is -42.21456707505549 and b is 284.3974693326735\n",
      "Iteration 4064, the loss is 1352.2127908393306, parameters k is -42.201573003909246 and b is 284.3184179492743\n",
      "Iteration 4065, the loss is 1351.5651862143502, parameters k is -42.188578932763 and b is 284.2354139967051\n",
      "Iteration 4066, the loss is 1350.8905851544114, parameters k is -42.175584861616755 and b is 284.15636261330593\n",
      "Iteration 4067, the loss is 1350.2433098799095, parameters k is -42.16259079047051 and b is 284.07335866073674\n",
      "Iteration 4068, the loss is 1349.5690482150058, parameters k is -42.149596719324265 and b is 283.99430727733755\n",
      "Iteration 4069, the loss is 1348.9221022909815, parameters k is -42.13660264817802 and b is 283.91130332476837\n",
      "Iteration 4070, the loss is 1348.2481800211165, parameters k is -42.123608577031774 and b is 283.8322519413692\n",
      "Iteration 4071, the loss is 1347.6015634475689, parameters k is -42.11061450588553 and b is 283.7492479888\n",
      "Iteration 4072, the loss is 1346.9551168495232, parameters k is -42.097620434739284 and b is 283.6662440362308\n",
      "Iteration 4073, the loss is 1346.2816933496679, parameters k is -42.08462636359304 and b is 283.5871926528316\n",
      "Iteration 4074, the loss is 1345.635576102099, parameters k is -42.07163229244679 and b is 283.50418870026243\n",
      "Iteration 4075, the loss is 1344.9624919972837, parameters k is -42.05863822130055 and b is 283.42513731686324\n",
      "Iteration 4076, the loss is 1344.3167041001907, parameters k is -42.0456441501543 and b is 283.34213336429406\n",
      "Iteration 4077, the loss is 1343.6439593904101, parameters k is -42.03265007900806 and b is 283.26308198089487\n",
      "Iteration 4078, the loss is 1342.9985008437939, parameters k is -42.01965600786181 and b is 283.1800780283257\n",
      "Iteration 4079, the loss is 1342.3260955290518, parameters k is -42.00666193671557 and b is 283.1010266449265\n",
      "Iteration 4080, the loss is 1341.680966332915, parameters k is -41.99366786556932 and b is 283.0180226923573\n",
      "Iteration 4081, the loss is 1341.0089004132062, parameters k is -41.980673794423076 and b is 282.9389713089581\n",
      "Iteration 4082, the loss is 1340.364100567546, parameters k is -41.96767972327683 and b is 282.85596735638893\n",
      "Iteration 4083, the loss is 1339.6923740428733, parameters k is -41.954685652130586 and b is 282.77691597298974\n",
      "Iteration 4084, the loss is 1339.04790354769, parameters k is -41.94169158098434 and b is 282.69391202042056\n",
      "Iteration 4085, the loss is 1338.4036030280079, parameters k is -41.928697509838095 and b is 282.61090806785137\n",
      "Iteration 4086, the loss is 1337.7323752733519, parameters k is -41.91570343869185 and b is 282.5318566844522\n",
      "Iteration 4087, the loss is 1337.0884041041456, parameters k is -41.902709367545604 and b is 282.448852731883\n",
      "Iteration 4088, the loss is 1336.4175157445245, parameters k is -41.88971529639936 and b is 282.3698013484838\n",
      "Iteration 4089, the loss is 1335.8009716922363, parameters k is -41.876721225253114 and b is 282.2828448267447\n",
      "Iteration 4090, the loss is 1335.1575104495305, parameters k is -41.86372715410687 and b is 282.1998408741755\n",
      "Iteration 4091, the loss is 1334.5142191823281, parameters k is -41.85073308296062 and b is 282.1168369216063\n",
      "Iteration 4092, the loss is 1333.8710978906217, parameters k is -41.83773901181438 and b is 282.0338329690371\n",
      "Iteration 4093, the loss is 1333.2281465744202, parameters k is -41.82474494066813 and b is 281.95082901646794\n",
      "Iteration 4094, the loss is 1332.585365233716, parameters k is -41.81175086952189 and b is 281.86782506389875\n",
      "Iteration 4095, the loss is 1331.9427538685136, parameters k is -41.79875679837564 and b is 281.78482111132956\n",
      "Iteration 4096, the loss is 1331.300312478809, parameters k is -41.7857627272294 and b is 281.7018171587604\n",
      "Iteration 4097, the loss is 1330.6580410646077, parameters k is -41.77276865608315 and b is 281.6188132061912\n",
      "Iteration 4098, the loss is 1330.0159396259053, parameters k is -41.759774584936906 and b is 281.535809253622\n",
      "Iteration 4099, the loss is 1329.3740081627052, parameters k is -41.74678051379066 and b is 281.4528053010528\n",
      "Iteration 4100, the loss is 1328.7322466750013, parameters k is -41.733786442644416 and b is 281.36980134848363\n",
      "Iteration 4101, the loss is 1328.0906551628016, parameters k is -41.72079237149817 and b is 281.28679739591445\n",
      "Iteration 4102, the loss is 1327.4492336261007, parameters k is -41.707798300351925 and b is 281.20379344334526\n",
      "Iteration 4103, the loss is 1326.8079820649016, parameters k is -41.69480422920568 and b is 281.1207894907761\n",
      "Iteration 4104, the loss is 1326.166900479199, parameters k is -41.681810158059434 and b is 281.0377855382069\n",
      "Iteration 4105, the loss is 1325.5259888690011, parameters k is -41.66881608691319 and b is 280.9547815856377\n",
      "Iteration 4106, the loss is 1324.8579692589635, parameters k is -41.655822015766944 and b is 280.8757302022385\n",
      "Iteration 4107, the loss is 1324.217386999241, parameters k is -41.6428279446207 and b is 280.79272624966933\n",
      "Iteration 4108, the loss is 1323.5497067842398, parameters k is -41.62983387347445 and b is 280.71367486627014\n",
      "Iteration 4109, the loss is 1322.9094538749937, parameters k is -41.61683980232821 and b is 280.63067091370095\n",
      "Iteration 4110, the loss is 1322.2421130550315, parameters k is -41.60384573118196 and b is 280.55161953030176\n",
      "Iteration 4111, the loss is 1321.6021894962619, parameters k is -41.59085166003572 and b is 280.4686155777326\n",
      "Iteration 4112, the loss is 1320.935188071337, parameters k is -41.57785758888947 and b is 280.3895641943334\n",
      "Iteration 4113, the loss is 1320.2955938630457, parameters k is -41.56486351774323 and b is 280.3065602417642\n",
      "Iteration 4114, the loss is 1319.6289318331558, parameters k is -41.55186944659698 and b is 280.227508858365\n",
      "Iteration 4115, the loss is 1318.989666975341, parameters k is -41.538875375450736 and b is 280.1445049057958\n",
      "Iteration 4116, the loss is 1318.3233443404852, parameters k is -41.52588130430449 and b is 280.06545352239664\n",
      "Iteration 4117, the loss is 1317.6844088331509, parameters k is -41.512887233158246 and b is 279.98244956982745\n",
      "Iteration 4118, the loss is 1317.018425593332, parameters k is -41.499893162012 and b is 279.90339818642826\n",
      "Iteration 4119, the loss is 1316.3798194364726, parameters k is -41.486899090865755 and b is 279.8203942338591\n",
      "Iteration 4120, the loss is 1315.7141755916916, parameters k is -41.47390501971951 and b is 279.7413428504599\n",
      "Iteration 4121, the loss is 1315.0758987853096, parameters k is -41.460910948573265 and b is 279.6583388978907\n",
      "Iteration 4122, the loss is 1314.4105943355648, parameters k is -41.44791687742702 and b is 279.5792875144915\n",
      "Iteration 4123, the loss is 1313.7726468796595, parameters k is -41.434922806280774 and b is 279.4962835619223\n",
      "Iteration 4124, the loss is 1313.107681824951, parameters k is -41.42192873513453 and b is 279.41723217852314\n",
      "Iteration 4125, the loss is 1312.4700637195258, parameters k is -41.40893466398828 and b is 279.33422822595395\n",
      "Iteration 4126, the loss is 1311.805438059854, parameters k is -41.39594059284204 and b is 279.25517684255476\n",
      "Iteration 4127, the loss is 1311.1409924202392, parameters k is -41.38294652169579 and b is 279.17612545915557\n",
      "Iteration 4128, the loss is 1310.503863040266, parameters k is -41.36995245054955 and b is 279.0931215065864\n",
      "Iteration 4129, the loss is 1309.839756795692, parameters k is -41.3569583794033 and b is 279.0140701231872\n",
      "Iteration 4130, the loss is 1309.2029567661953, parameters k is -41.34396430825706 and b is 278.931066170618\n",
      "Iteration 4131, the loss is 1308.5391899166552, parameters k is -41.33097023711081 and b is 278.8520147872188\n",
      "Iteration 4132, the loss is 1307.9027192376377, parameters k is -41.317976165964566 and b is 278.76901083464963\n",
      "Iteration 4133, the loss is 1307.2392917831332, parameters k is -41.30498209481832 and b is 278.68995945125044\n",
      "Iteration 4134, the loss is 1306.6031504545913, parameters k is -41.291988023672076 and b is 278.60695549868126\n",
      "Iteration 4135, the loss is 1305.9400623951278, parameters k is -41.27899395252583 and b is 278.52790411528207\n",
      "Iteration 4136, the loss is 1305.3042504170623, parameters k is -41.265999881379585 and b is 278.4449001627129\n",
      "Iteration 4137, the loss is 1304.6415017526322, parameters k is -41.25300581023334 and b is 278.3658487793137\n",
      "Iteration 4138, the loss is 1304.0060191250443, parameters k is -41.240011739087095 and b is 278.2828448267445\n",
      "Iteration 4139, the loss is 1303.3436098556504, parameters k is -41.22701766794085 and b is 278.2037934433453\n",
      "Iteration 4140, the loss is 1302.7084565785415, parameters k is -41.214023596794604 and b is 278.12078949077613\n",
      "Iteration 4141, the loss is 1302.0463867041844, parameters k is -41.20102952564836 and b is 278.04173810737694\n",
      "Iteration 4142, the loss is 1301.41156277755, parameters k is -41.18803545450211 and b is 277.95873415480776\n",
      "Iteration 4143, the loss is 1300.7498322982315, parameters k is -41.17504138335587 and b is 277.87968277140857\n",
      "Iteration 4144, the loss is 1300.1153377220771, parameters k is -41.16204731220962 and b is 277.7966788188394\n",
      "Iteration 4145, the loss is 1299.4539466377933, parameters k is -41.14905324106338 and b is 277.7176274354402\n",
      "Iteration 4146, the loss is 1298.819781412115, parameters k is -41.13605916991713 and b is 277.634623482871\n",
      "Iteration 4147, the loss is 1298.1587297228668, parameters k is -41.12306509877089 and b is 277.5555720994718\n",
      "Iteration 4148, the loss is 1297.4978580536795, parameters k is -41.11007102762464 and b is 277.4765207160726\n",
      "Iteration 4149, the loss is 1296.8641815534563, parameters k is -41.097076956478396 and b is 277.39351676350344\n",
      "Iteration 4150, the loss is 1296.203649279304, parameters k is -41.08408288533215 and b is 277.31446538010425\n",
      "Iteration 4151, the loss is 1295.570302129556, parameters k is -41.071088814185906 and b is 277.23146142753507\n",
      "Iteration 4152, the loss is 1294.9101092504409, parameters k is -41.05809474303966 and b is 277.1524100441359\n",
      "Iteration 4153, the loss is 1294.2770914511711, parameters k is -41.045100671893415 and b is 277.0694060915667\n",
      "Iteration 4154, the loss is 1293.6172379670943, parameters k is -41.03210660074717 and b is 276.9903547081675\n",
      "Iteration 4155, the loss is 1292.9845495183, parameters k is -41.019112529600925 and b is 276.9073507555983\n",
      "Iteration 4156, the loss is 1292.3250354292593, parameters k is -41.00611845845468 and b is 276.8282993721991\n",
      "Iteration 4157, the loss is 1291.6926763309443, parameters k is -40.993124387308434 and b is 276.74529541962994\n",
      "Iteration 4158, the loss is 1291.033501636938, parameters k is -40.98013031616219 and b is 276.66624403623075\n",
      "Iteration 4159, the loss is 1290.4014718890999, parameters k is -40.96713624501594 and b is 276.58324008366156\n",
      "Iteration 4160, the loss is 1289.7426365901326, parameters k is -40.9541421738697 and b is 276.5041887002624\n",
      "Iteration 4161, the loss is 1289.1109361927706, parameters k is -40.94114810272345 and b is 276.4211847476932\n",
      "Iteration 4162, the loss is 1288.452440288838, parameters k is -40.92815403157721 and b is 276.342133364294\n",
      "Iteration 4163, the loss is 1287.8210692419545, parameters k is -40.91515996043096 and b is 276.2591294117248\n",
      "Iteration 4164, the loss is 1287.1629127330598, parameters k is -40.90216588928472 and b is 276.1800780283256\n",
      "Iteration 4165, the loss is 1286.5318710366505, parameters k is -40.88917181813847 and b is 276.09707407575644\n",
      "Iteration 4166, the loss is 1285.8740539227952, parameters k is -40.87617774699223 and b is 276.01802269235725\n",
      "Iteration 4167, the loss is 1285.2433415768649, parameters k is -40.86318367584598 and b is 275.93501873978806\n",
      "Iteration 4168, the loss is 1284.5858638580412, parameters k is -40.850189604699736 and b is 275.8559673563889\n",
      "Iteration 4169, the loss is 1283.9554808625899, parameters k is -40.83719553355349 and b is 275.7729634038197\n",
      "Iteration 4170, the loss is 1283.2983425388034, parameters k is -40.824201462407245 and b is 275.6939120204205\n",
      "Iteration 4171, the loss is 1282.6413842350769, parameters k is -40.811207391261 and b is 275.6148606370213\n",
      "Iteration 4172, the loss is 1282.011489965079, parameters k is -40.798213320114755 and b is 275.5318566844521\n",
      "Iteration 4173, the loss is 1281.3548710563878, parameters k is -40.78521924896851 and b is 275.45280530105293\n",
      "Iteration 4174, the loss is 1280.7253061368679, parameters k is -40.772225177822264 and b is 275.36980134848375\n",
      "Iteration 4175, the loss is 1280.0690266232132, parameters k is -40.75923110667602 and b is 275.29074996508456\n",
      "Iteration 4176, the loss is 1279.4397910541702, parameters k is -40.746237035529774 and b is 275.2077460125154\n",
      "Iteration 4177, the loss is 1278.7838509355533, parameters k is -40.73324296438353 and b is 275.1286946291162\n",
      "Iteration 4178, the loss is 1278.1549447169873, parameters k is -40.72024889323728 and b is 275.045690676547\n",
      "Iteration 4179, the loss is 1277.4993439934062, parameters k is -40.70725482209104 and b is 274.9666392931478\n",
      "Iteration 4180, the loss is 1276.8707671253192, parameters k is -40.69426075094479 and b is 274.8836353405786\n",
      "Iteration 4181, the loss is 1276.2155057967727, parameters k is -40.68126667979855 and b is 274.80458395717943\n",
      "Iteration 4182, the loss is 1275.5872582791617, parameters k is -40.6682726086523 and b is 274.72158000461025\n",
      "Iteration 4183, the loss is 1274.9323363456533, parameters k is -40.65527853750606 and b is 274.64252862121106\n",
      "Iteration 4184, the loss is 1274.3044181785208, parameters k is -40.64228446635981 and b is 274.55952466864187\n",
      "Iteration 4185, the loss is 1273.6498356400473, parameters k is -40.629290395213566 and b is 274.4804732852427\n",
      "Iteration 4186, the loss is 1273.0222468233933, parameters k is -40.61629632406732 and b is 274.3974693326735\n",
      "Iteration 4187, the loss is 1272.3680036799574, parameters k is -40.603302252921075 and b is 274.3184179492743\n",
      "Iteration 4188, the loss is 1271.7407442137755, parameters k is -40.59030818177483 and b is 274.2354139967051\n",
      "Iteration 4189, the loss is 1271.0868404653775, parameters k is -40.577314110628585 and b is 274.15636261330593\n",
      "Iteration 4190, the loss is 1270.4599103496744, parameters k is -40.56432003948234 and b is 274.07335866073674\n",
      "Iteration 4191, the loss is 1269.8063459963141, parameters k is -40.551325968336094 and b is 273.99430727733755\n",
      "Iteration 4192, the loss is 1269.179745231089, parameters k is -40.53833189718985 and b is 273.91130332476837\n",
      "Iteration 4193, the loss is 1268.5265202727635, parameters k is -40.525337826043604 and b is 273.8322519413692\n",
      "Iteration 4194, the loss is 1267.8734753344968, parameters k is -40.51234375489736 and b is 273.75320055797\n",
      "Iteration 4195, the loss is 1267.2473632947251, parameters k is -40.49934968375111 and b is 273.6701966054008\n",
      "Iteration 4196, the loss is 1266.5946577514962, parameters k is -40.48635561260487 and b is 273.5911452220016\n",
      "Iteration 4197, the loss is 1265.968875062203, parameters k is -40.47336154145862 and b is 273.50814126943243\n",
      "Iteration 4198, the loss is 1265.3165089140095, parameters k is -40.46036747031238 and b is 273.42908988603324\n",
      "Iteration 4199, the loss is 1264.6910555751926, parameters k is -40.44737339916613 and b is 273.34608593346405\n",
      "Iteration 4200, the loss is 1264.0390288220356, parameters k is -40.43437932801989 and b is 273.26703455006486\n",
      "Iteration 4201, the loss is 1263.413904833697, parameters k is -40.42138525687364 and b is 273.1840305974957\n",
      "Iteration 4202, the loss is 1262.7622174755766, parameters k is -40.408391185727396 and b is 273.1049792140965\n",
      "Iteration 4203, the loss is 1262.137422837714, parameters k is -40.39539711458115 and b is 273.0219752615273\n",
      "Iteration 4204, the loss is 1261.48607487463, parameters k is -40.382403043434905 and b is 272.9429238781281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4205, the loss is 1260.861609587247, parameters k is -40.36940897228866 and b is 272.8599199255589\n",
      "Iteration 4206, the loss is 1260.2106010192006, parameters k is -40.356414901142415 and b is 272.78086854215974\n",
      "Iteration 4207, the loss is 1259.5864650822923, parameters k is -40.34342082999617 and b is 272.69786458959055\n",
      "Iteration 4208, the loss is 1258.9357959092808, parameters k is -40.330426758849924 and b is 272.61881320619136\n",
      "Iteration 4209, the loss is 1258.3119893228504, parameters k is -40.31743268770368 and b is 272.5358092536222\n",
      "Iteration 4210, the loss is 1257.6349976234385, parameters k is -40.304438616557434 and b is 272.4607104393929\n",
      "Iteration 4211, the loss is 1257.0115097869616, parameters k is -40.29144454541119 and b is 272.37770648682374\n",
      "Iteration 4212, the loss is 1256.388191925986, parameters k is -40.27845047426494 and b is 272.29470253425455\n",
      "Iteration 4213, the loss is 1255.7383815631094, parameters k is -40.2654564031187 and b is 272.21565115085536\n",
      "Iteration 4214, the loss is 1255.088751220291, parameters k is -40.25246233197245 and b is 272.1365997674562\n",
      "Iteration 4215, the loss is 1254.4659220847675, parameters k is -40.23946826082621 and b is 272.053595814887\n",
      "Iteration 4216, the loss is 1253.8166311369866, parameters k is -40.22647418967996 and b is 271.9745444314878\n",
      "Iteration 4217, the loss is 1253.1675202092642, parameters k is -40.21348011853372 and b is 271.8954930480886\n",
      "Iteration 4218, the loss is 1252.5718015423963, parameters k is -40.20048604738747 and b is 271.8085365263495\n",
      "Iteration 4219, the loss is 1251.896408266511, parameters k is -40.187491976241226 and b is 271.73343771212024\n",
      "Iteration 4220, the loss is 1251.2743972069195, parameters k is -40.17449790509498 and b is 271.65043375955105\n",
      "Iteration 4221, the loss is 1250.6259650692673, parameters k is -40.161503833948736 and b is 271.57138237615186\n",
      "Iteration 4222, the loss is 1250.0042833601551, parameters k is -40.14850976280249 and b is 271.4883784235827\n",
      "Iteration 4223, the loss is 1249.3827716265425, parameters k is -40.135515691656245 and b is 271.4053744710135\n",
      "Iteration 4224, the loss is 1248.734838258905, parameters k is -40.12252162051 and b is 271.3263230876143\n",
      "Iteration 4225, the loss is 1248.1136558757703, parameters k is -40.109527549363754 and b is 271.2433191350451\n",
      "Iteration 4226, the loss is 1247.466061903171, parameters k is -40.09653347821751 and b is 271.1642677516459\n",
      "Iteration 4227, the loss is 1246.845208870512, parameters k is -40.083539407071264 and b is 271.08126379907674\n",
      "Iteration 4228, the loss is 1246.1979542929491, parameters k is -40.07054533592502 and b is 271.00221241567755\n",
      "Iteration 4229, the loss is 1245.5774306107678, parameters k is -40.05755126477877 and b is 270.91920846310836\n",
      "Iteration 4230, the loss is 1244.930515428241, parameters k is -40.04455719363253 and b is 270.8401570797092\n",
      "Iteration 4231, the loss is 1244.3103210965369, parameters k is -40.03156312248628 and b is 270.75715312714\n",
      "Iteration 4232, the loss is 1243.6637453090473, parameters k is -40.01856905134004 and b is 270.6781017437408\n",
      "Iteration 4233, the loss is 1243.04388032782, parameters k is -40.00557498019379 and b is 270.5950977911716\n",
      "Iteration 4234, the loss is 1242.4241853220935, parameters k is -39.99258090904755 and b is 270.5120938386024\n",
      "Iteration 4235, the loss is 1241.7515875629738, parameters k is -39.9795868379013 and b is 270.4369950243732\n",
      "Iteration 4236, the loss is 1241.1322113072, parameters k is -39.966592766755056 and b is 270.353991071804\n",
      "Iteration 4237, the loss is 1240.5130050269254, parameters k is -39.95359869560881 and b is 270.2709871192348\n",
      "Iteration 4238, the loss is 1239.8939687221543, parameters k is -39.940604624462566 and b is 270.1879831666656\n",
      "Iteration 4239, the loss is 1239.222069842225, parameters k is -39.92761055331632 and b is 270.11288435243637\n",
      "Iteration 4240, the loss is 1238.6033522874084, parameters k is -39.914616482170075 and b is 270.0298803998672\n",
      "Iteration 4241, the loss is 1237.9848047080893, parameters k is -39.90162241102383 and b is 269.946876447298\n",
      "Iteration 4242, the loss is 1237.339925895782, parameters k is -39.888628339877584 and b is 269.8678250638988\n",
      "Iteration 4243, the loss is 1236.7217076669403, parameters k is -39.87563426873134 and b is 269.7848211113296\n",
      "Iteration 4244, the loss is 1236.0771682496709, parameters k is -39.862640197585094 and b is 269.70576972793043\n",
      "Iteration 4245, the loss is 1235.4592793713064, parameters k is -39.84964612643885 and b is 269.62276577536124\n",
      "Iteration 4246, the loss is 1234.8150793490706, parameters k is -39.8366520552926 and b is 269.54371439196206\n",
      "Iteration 4247, the loss is 1234.197519821185, parameters k is -39.82365798414636 and b is 269.46071043939287\n",
      "Iteration 4248, the loss is 1233.5536591939874, parameters k is -39.81066391300011 and b is 269.3816590559937\n",
      "Iteration 4249, the loss is 1232.9364290165774, parameters k is -39.79766984185387 and b is 269.2986551034245\n",
      "Iteration 4250, the loss is 1232.2929077844155, parameters k is -39.78467577070762 and b is 269.2196037200253\n",
      "Iteration 4251, the loss is 1231.6760069574846, parameters k is -39.77168169956138 and b is 269.1365997674561\n",
      "Iteration 4252, the loss is 1231.0328251203591, parameters k is -39.75868762841513 and b is 269.05754838405693\n",
      "Iteration 4253, the loss is 1230.4162536439032, parameters k is -39.745693557268886 and b is 268.97454443148774\n",
      "Iteration 4254, the loss is 1229.773411201816, parameters k is -39.73269948612264 and b is 268.89549304808855\n",
      "Iteration 4255, the loss is 1229.1571690758376, parameters k is -39.719705414976396 and b is 268.81248909551937\n",
      "Iteration 4256, the loss is 1228.5146660287885, parameters k is -39.70671134383015 and b is 268.7334377121202\n",
      "Iteration 4257, the loss is 1227.8987532532858, parameters k is -39.693717272683905 and b is 268.650433759551\n",
      "Iteration 4258, the loss is 1227.2565896012704, parameters k is -39.68072320153766 and b is 268.5713823761518\n",
      "Iteration 4259, the loss is 1226.6410061762476, parameters k is -39.667729130391415 and b is 268.4883784235826\n",
      "Iteration 4260, the loss is 1225.9991819192678, parameters k is -39.65473505924517 and b is 268.4093270401834\n",
      "Iteration 4261, the loss is 1225.3575376823478, parameters k is -39.641740988098924 and b is 268.33027565678424\n",
      "Iteration 4262, the loss is 1224.7424429827793, parameters k is -39.62874691695268 and b is 268.24727170421505\n",
      "Iteration 4263, the loss is 1224.1011381408966, parameters k is -39.61575284580643 and b is 268.16822032081586\n",
      "Iteration 4264, the loss is 1223.4863727918046, parameters k is -39.60275877466019 and b is 268.0852163682467\n",
      "Iteration 4265, the loss is 1222.84540734496, parameters k is -39.58976470351394 and b is 268.0061649848475\n",
      "Iteration 4266, the loss is 1222.2309713463426, parameters k is -39.5767706323677 and b is 267.9231610322783\n",
      "Iteration 4267, the loss is 1221.590345294533, parameters k is -39.56377656122145 and b is 267.8441096488791\n",
      "Iteration 4268, the loss is 1220.9762386463963, parameters k is -39.55078249007521 and b is 267.7611056963099\n",
      "Iteration 4269, the loss is 1220.3359519896242, parameters k is -39.53778841892896 and b is 267.68205431291074\n",
      "Iteration 4270, the loss is 1219.7221746919638, parameters k is -39.524794347782716 and b is 267.59905036034155\n",
      "Iteration 4271, the loss is 1219.0822274302259, parameters k is -39.51180027663647 and b is 267.51999897694236\n",
      "Iteration 4272, the loss is 1218.4687794830431, parameters k is -39.498806205490226 and b is 267.4369950243732\n",
      "Iteration 4273, the loss is 1217.82917161634, parameters k is -39.48581213434398 and b is 267.357943640974\n",
      "Iteration 4274, the loss is 1217.216053019636, parameters k is -39.472818063197735 and b is 267.2749396884048\n",
      "Iteration 4275, the loss is 1216.5767845479734, parameters k is -39.45982399205149 and b is 267.1958883050056\n",
      "Iteration 4276, the loss is 1215.9639953017436, parameters k is -39.446829920905245 and b is 267.1128843524364\n",
      "Iteration 4277, the loss is 1215.3250662251164, parameters k is -39.433835849759 and b is 267.03383296903723\n",
      "Iteration 4278, the loss is 1214.7126063293645, parameters k is -39.420841778612754 and b is 266.95082901646805\n",
      "Iteration 4279, the loss is 1214.074016647775, parameters k is -39.40784770746651 and b is 266.87177763306886\n",
      "Iteration 4280, the loss is 1213.4618861025008, parameters k is -39.39485363632026 and b is 266.7887736804997\n",
      "Iteration 4281, the loss is 1212.823635815946, parameters k is -39.38185956517402 and b is 266.7097222971005\n",
      "Iteration 4282, the loss is 1212.2118346211496, parameters k is -39.36886549402777 and b is 266.6267183445313\n",
      "Iteration 4283, the loss is 1211.5739237296316, parameters k is -39.35587142288153 and b is 266.5476669611321\n",
      "Iteration 4284, the loss is 1210.9361928581723, parameters k is -39.34287735173528 and b is 266.4686155777329\n",
      "Iteration 4285, the loss is 1210.324880388831, parameters k is -39.32988328058904 and b is 266.38561162516373\n",
      "Iteration 4286, the loss is 1209.687488912408, parameters k is -39.31688920944279 and b is 266.30656024176454\n",
      "Iteration 4287, the loss is 1209.076505793542, parameters k is -39.30389513829655 and b is 266.22355628919536\n",
      "Iteration 4288, the loss is 1208.439453712159, parameters k is -39.2909010671503 and b is 266.14450490579617\n",
      "Iteration 4289, the loss is 1207.8287999437687, parameters k is -39.277906996004056 and b is 266.061500953227\n",
      "Iteration 4290, the loss is 1207.19208725742, parameters k is -39.26491292485781 and b is 265.9824495698278\n",
      "Iteration 4291, the loss is 1206.5817628395082, parameters k is -39.251918853711565 and b is 265.8994456172586\n",
      "Iteration 4292, the loss is 1205.9453895481959, parameters k is -39.23892478256532 and b is 265.8203942338594\n",
      "Iteration 4293, the loss is 1205.335394480764, parameters k is -39.225930711419075 and b is 265.73739028129023\n",
      "Iteration 4294, the loss is 1204.6993605844877, parameters k is -39.21293664027283 and b is 265.65833889789104\n",
      "Iteration 4295, the loss is 1204.089694867532, parameters k is -39.199942569126584 and b is 265.57533494532186\n",
      "Iteration 4296, the loss is 1203.454000366291, parameters k is -39.18694849798034 and b is 265.49628356192267\n",
      "Iteration 4297, the loss is 1202.8446639998117, parameters k is -39.173954426834094 and b is 265.4132796093535\n",
      "Iteration 4298, the loss is 1202.2093088936078, parameters k is -39.16096035568785 and b is 265.3342282259543\n",
      "Iteration 4299, the loss is 1201.6003018776069, parameters k is -39.1479662845416 and b is 265.2512242733851\n",
      "Iteration 4300, the loss is 1200.9652861664408, parameters k is -39.13497221339536 and b is 265.1721728899859\n",
      "Iteration 4301, the loss is 1200.3566085009152, parameters k is -39.12197814224911 and b is 265.08916893741673\n",
      "Iteration 4302, the loss is 1199.721932184785, parameters k is -39.10898407110287 and b is 265.01011755401754\n",
      "Iteration 4303, the loss is 1199.1135838697378, parameters k is -39.09598999995662 and b is 264.92711360144835\n",
      "Iteration 4304, the loss is 1198.4792469486445, parameters k is -39.08299592881038 and b is 264.84806221804917\n",
      "Iteration 4305, the loss is 1197.8712279840734, parameters k is -39.07000185766413 and b is 264.76505826548\n",
      "Iteration 4306, the loss is 1197.2372304580188, parameters k is -39.057007786517886 and b is 264.6860068820808\n",
      "Iteration 4307, the loss is 1196.603412952019, parameters k is -39.04401371537164 and b is 264.6069554986816\n",
      "Iteration 4308, the loss is 1195.9958827129035, parameters k is -39.031019644225395 and b is 264.5239515461124\n",
      "Iteration 4309, the loss is 1195.362404601943, parameters k is -39.01802557307915 and b is 264.4449001627132\n",
      "Iteration 4310, the loss is 1194.7552037133032, parameters k is -39.005031501932905 and b is 264.36189621014404\n",
      "Iteration 4311, the loss is 1194.1220649973782, parameters k is -38.99203743078666 and b is 264.28284482674485\n",
      "Iteration 4312, the loss is 1193.5151934592159, parameters k is -38.979043359640414 and b is 264.19984087417566\n",
      "Iteration 4313, the loss is 1192.882394138329, parameters k is -38.96604928849417 and b is 264.1207894907765\n",
      "Iteration 4314, the loss is 1192.275851950647, parameters k is -38.953055217347924 and b is 264.0377855382073\n",
      "Iteration 4315, the loss is 1191.6433920247948, parameters k is -38.94006114620168 and b is 263.9587341548081\n",
      "Iteration 4316, the loss is 1191.037179187588, parameters k is -38.92706707505543 and b is 263.8757302022389\n",
      "Iteration 4317, the loss is 1190.40505865677, parameters k is -38.91407300390919 and b is 263.7966788188397\n",
      "Iteration 4318, the loss is 1189.7991751700408, parameters k is -38.90107893276294 and b is 263.71367486627054\n",
      "Iteration 4319, the loss is 1189.1673940342628, parameters k is -38.8880848616167 and b is 263.63462348287135\n",
      "Iteration 4320, the loss is 1188.5618398980107, parameters k is -38.87509079047045 and b is 263.55161953030216\n",
      "Iteration 4321, the loss is 1187.9303981572675, parameters k is -38.86209671932421 and b is 263.472568146903\n",
      "Iteration 4322, the loss is 1187.3251733714937, parameters k is -38.84910264817796 and b is 263.3895641943338\n",
      "Iteration 4323, the loss is 1186.6940710257857, parameters k is -38.836108577031716 and b is 263.3105128109346\n",
      "Iteration 4324, the loss is 1186.0891755904881, parameters k is -38.82311450588547 and b is 263.2275088583654\n",
      "Iteration 4325, the loss is 1185.458412639819, parameters k is -38.810120434739225 and b is 263.1484574749662\n",
      "Iteration 4326, the loss is 1184.8538465549982, parameters k is -38.79712636359298 and b is 263.06545352239704\n",
      "Iteration 4327, the loss is 1184.2234229993653, parameters k is -38.784132292446735 and b is 262.98640213899785\n",
      "Iteration 4328, the loss is 1183.6191862650219, parameters k is -38.77113822130049 and b is 262.90339818642866\n",
      "Iteration 4329, the loss is 1182.989102104425, parameters k is -38.758144150154244 and b is 262.82434680302947\n",
      "Iteration 4330, the loss is 1182.3591979638886, parameters k is -38.745150079008 and b is 262.7452954196303\n",
      "Iteration 4331, the loss is 1181.7554499549988, parameters k is -38.732156007861754 and b is 262.6622914670611\n",
      "Iteration 4332, the loss is 1181.125885209499, parameters k is -38.71916193671551 and b is 262.5832400836619\n",
      "Iteration 4333, the loss is 1180.5224665510866, parameters k is -38.70616786556926 and b is 262.5002361310927\n",
      "Iteration 4334, the loss is 1179.893241200623, parameters k is -38.69317379442302 and b is 262.42118474769353\n",
      "Iteration 4335, the loss is 1179.2901518926876, parameters k is -38.68017972327677 and b is 262.33818079512434\n",
      "Iteration 4336, the loss is 1178.661265937262, parameters k is -38.66718565213053 and b is 262.25912941172516\n",
      "Iteration 4337, the loss is 1178.0585059798016, parameters k is -38.65419158098428 and b is 262.17612545915597\n",
      "Iteration 4338, the loss is 1177.429959419412, parameters k is -38.64119750983804 and b is 262.0970740757568\n",
      "Iteration 4339, the loss is 1176.8275288124298, parameters k is -38.62820343869179 and b is 262.0140701231876\n",
      "Iteration 4340, the loss is 1176.1993216470776, parameters k is -38.615209367545546 and b is 261.9350187397884\n",
      "Iteration 4341, the loss is 1175.597220390575, parameters k is -38.6022152963993 and b is 261.8520147872192\n",
      "Iteration 4342, the loss is 1174.9693526202577, parameters k is -38.589221225253056 and b is 261.77296340382003\n",
      "Iteration 4343, the loss is 1174.3675807142304, parameters k is -38.57622715410681 and b is 261.68995945125084\n",
      "Iteration 4344, the loss is 1173.7400523389488, parameters k is -38.563233082960565 and b is 261.61090806785165\n",
      "Iteration 4345, the loss is 1173.1386097833997, parameters k is -38.55023901181432 and b is 261.52790411528247\n",
      "Iteration 4346, the loss is 1172.5114208031553, parameters k is -38.537244940668074 and b is 261.4488527318833\n",
      "Iteration 4347, the loss is 1171.910307598085, parameters k is -38.52425086952183 and b is 261.3658487793141\n",
      "Iteration 4348, the loss is 1171.2834580128763, parameters k is -38.511256798375584 and b is 261.2867973959149\n",
      "Iteration 4349, the loss is 1170.6826741582815, parameters k is -38.49826272722934 and b is 261.2037934433457\n",
      "Iteration 4350, the loss is 1170.05616396811, parameters k is -38.48526865608309 and b is 261.1247420599465\n",
      "Iteration 4351, the loss is 1169.4557094639931, parameters k is -38.47227458493685 and b is 261.04173810737734\n",
      "Iteration 4352, the loss is 1168.8295386688555, parameters k is -38.4592805137906 and b is 260.96268672397815\n",
      "Iteration 4353, the loss is 1168.203547893782, parameters k is -38.44628644264436 and b is 260.88363534057896\n",
      "Iteration 4354, the loss is 1167.6035821151183, parameters k is -38.43329237149811 and b is 260.8006313880098\n",
      "Iteration 4355, the loss is 1166.9779307350784, parameters k is -38.42029830035187 and b is 260.7215800046106\n",
      "Iteration 4356, the loss is 1166.3782943068927, parameters k is -38.40730422920562 and b is 260.6385760520414\n",
      "Iteration 4357, the loss is 1165.7529823218908, parameters k is -38.394310158059376 and b is 260.5595246686422\n",
      "Iteration 4358, the loss is 1165.1536752441818, parameters k is -38.38131608691313 and b is 260.476520716073\n",
      "Iteration 4359, the loss is 1164.5287026542167, parameters k is -38.368322015766886 and b is 260.39746933267384\n",
      "Iteration 4360, the loss is 1163.929724926984, parameters k is -38.35532794462064 and b is 260.31446538010465\n",
      "Iteration 4361, the loss is 1163.3050917320556, parameters k is -38.342333873474395 and b is 260.23541399670546\n",
      "Iteration 4362, the loss is 1162.7064433553016, parameters k is -38.32933980232815 and b is 260.1524100441363\n",
      "Iteration 4363, the loss is 1162.0821495554073, parameters k is -38.316345731181904 and b is 260.0733586607371\n",
      "Iteration 4364, the loss is 1161.4838305291296, parameters k is -38.30335166003566 and b is 259.9903547081679\n",
      "Iteration 4365, the loss is 1160.8598761242722, parameters k is -38.290357588889414 and b is 259.9113033247687\n",
      "Iteration 4366, the loss is 1160.2618864484728, parameters k is -38.27736351774317 and b is 259.8282993721995\n",
      "Iteration 4367, the loss is 1159.638271438654, parameters k is -38.26436944659692 and b is 259.74924798880033\n",
      "Iteration 4368, the loss is 1159.0406111133295, parameters k is -38.25137537545068 and b is 259.66624403623115\n",
      "Iteration 4369, the loss is 1158.417335498547, parameters k is -38.23838130430443 and b is 259.58719265283196\n",
      "Iteration 4370, the loss is 1157.8200045237024, parameters k is -38.22538723315819 and b is 259.5041887002628\n",
      "Iteration 4371, the loss is 1157.197068303955, parameters k is -38.21239316201194 and b is 259.4251373168636\n",
      "Iteration 4372, the loss is 1156.6000666795858, parameters k is -38.1993990908657 and b is 259.3421333642944\n",
      "Iteration 4373, the loss is 1155.9774698548747, parameters k is -38.18640501971945 and b is 259.2630819808952\n",
      "Iteration 4374, the loss is 1155.355053050225, parameters k is -38.173410948573206 and b is 259.184030597496\n",
      "Iteration 4375, the loss is 1154.758540151311, parameters k is -38.16041687742696 and b is 259.10102664492683\n",
      "Iteration 4376, the loss is 1154.136462741697, parameters k is -38.147422806280716 and b is 259.02197526152764\n",
      "Iteration 4377, the loss is 1153.5402791932588, parameters k is -38.13442873513447 and b is 258.93897130895846\n",
      "Iteration 4378, the loss is 1152.9185411786807, parameters k is -38.121434663988225 and b is 258.85991992555927\n",
      "Iteration 4379, the loss is 1152.322686980722, parameters k is -38.10844059284198 and b is 258.7769159729901\n",
      "Iteration 4380, the loss is 1151.7012883611803, parameters k is -38.095446521695735 and b is 258.6978645895909\n",
      "Iteration 4381, the loss is 1151.1057635136967, parameters k is -38.08245245054949 and b is 258.6148606370217\n",
      "Iteration 4382, the loss is 1150.4847042891922, parameters k is -38.069458379403244 and b is 258.5358092536225\n",
      "Iteration 4383, the loss is 1149.8895087921856, parameters k is -38.056464308257 and b is 258.45280530105333\n",
      "Iteration 4384, the loss is 1149.2687889627189, parameters k is -38.04347023711075 and b is 258.37375391765414\n",
      "Iteration 4385, the loss is 1148.673922816191, parameters k is -38.03047616596451 and b is 258.29074996508496\n",
      "Iteration 4386, the loss is 1148.053542381759, parameters k is -38.01748209481826 and b is 258.21169858168577\n",
      "Iteration 4387, the loss is 1147.4590055857066, parameters k is -38.00448802367202 and b is 258.1286946291166\n",
      "Iteration 4388, the loss is 1146.8389645463142, parameters k is -37.99149395252577 and b is 258.0496432457174\n",
      "Iteration 4389, the loss is 1146.2447571007394, parameters k is -37.97849988137953 and b is 257.9666392931482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4390, the loss is 1145.6250554563799, parameters k is -37.96550581023328 and b is 257.887587909749\n",
      "Iteration 4391, the loss is 1145.031177361284, parameters k is -37.952511739087036 and b is 257.80458395717983\n",
      "Iteration 4392, the loss is 1144.4118151119621, parameters k is -37.93951766794079 and b is 257.72553257378064\n",
      "Iteration 4393, the loss is 1143.8182663673429, parameters k is -37.926523596794546 and b is 257.64252862121145\n",
      "Iteration 4394, the loss is 1143.199243513057, parameters k is -37.9135295256483 and b is 257.56347723781226\n",
      "Iteration 4395, the loss is 1142.6060241189152, parameters k is -37.900535454502055 and b is 257.4804732852431\n",
      "Iteration 4396, the loss is 1141.987340659664, parameters k is -37.88754138335581 and b is 257.4014219018439\n",
      "Iteration 4397, the loss is 1141.3688372204747, parameters k is -37.874547312209565 and b is 257.3223705184447\n",
      "Iteration 4398, the loss is 1140.7761065517875, parameters k is -37.86155324106332 and b is 257.2393665658755\n",
      "Iteration 4399, the loss is 1140.1579425076345, parameters k is -37.848559169917074 and b is 257.1603151824763\n",
      "Iteration 4400, the loss is 1139.565541189424, parameters k is -37.83556509877083 and b is 257.07731122990714\n",
      "Iteration 4401, the loss is 1138.947716540308, parameters k is -37.82257102762458 and b is 256.99825984650795\n",
      "Iteration 4402, the loss is 1138.3556445725726, parameters k is -37.80957695647834 and b is 256.91525589393876\n",
      "Iteration 4403, the loss is 1137.7381593184925, parameters k is -37.79658288533209 and b is 256.8362045105396\n",
      "Iteration 4404, the loss is 1137.1464167012368, parameters k is -37.78358881418585 and b is 256.7532005579704\n",
      "Iteration 4405, the loss is 1136.5292708421935, parameters k is -37.7705947430396 and b is 256.6741491745712\n",
      "Iteration 4406, the loss is 1135.937857575414, parameters k is -37.75760067189336 and b is 256.591145222002\n",
      "Iteration 4407, the loss is 1135.3210511114046, parameters k is -37.74460660074711 and b is 256.5120938386028\n",
      "Iteration 4408, the loss is 1134.7299671951034, parameters k is -37.731612529600866 and b is 256.42908988603364\n",
      "Iteration 4409, the loss is 1134.1135001261334, parameters k is -37.71861845845462 and b is 256.35003850263445\n",
      "Iteration 4410, the loss is 1133.5227455603076, parameters k is -37.705624387308376 and b is 256.26703455006526\n",
      "Iteration 4411, the loss is 1132.906617886374, parameters k is -37.69263031616213 and b is 256.1879831666661\n",
      "Iteration 4412, the loss is 1132.3161926710284, parameters k is -37.679636245015885 and b is 256.1049792140969\n",
      "Iteration 4413, the loss is 1131.7004043921293, parameters k is -37.66664217386964 and b is 256.0259278306977\n",
      "Iteration 4414, the loss is 1131.1103085272596, parameters k is -37.653648102723395 and b is 255.94292387812854\n",
      "Iteration 4415, the loss is 1130.4948596433987, parameters k is -37.64065403157715 and b is 255.86387249472932\n",
      "Iteration 4416, the loss is 1129.9050931290053, parameters k is -37.627659960430904 and b is 255.78086854216016\n",
      "Iteration 4417, the loss is 1129.2899836401805, parameters k is -37.61466588928466 and b is 255.70181715876095\n",
      "Iteration 4418, the loss is 1128.7005464762658, parameters k is -37.60167181813841 and b is 255.6188132061918\n",
      "Iteration 4419, the loss is 1128.0857763824765, parameters k is -37.58867774699217 and b is 255.53976182279257\n",
      "Iteration 4420, the loss is 1127.4711863087466, parameters k is -37.57568367584592 and b is 255.46071043939335\n",
      "Iteration 4421, the loss is 1126.882237870287, parameters k is -37.56268960469968 and b is 255.3777064868242\n",
      "Iteration 4422, the loss is 1126.2679871915943, parameters k is -37.54969553355343 and b is 255.29865510342498\n",
      "Iteration 4423, the loss is 1125.6793681036097, parameters k is -37.53670146240719 and b is 255.21565115085582\n",
      "Iteration 4424, the loss is 1125.065456819954, parameters k is -37.52370739126094 and b is 255.1365997674566\n",
      "Iteration 4425, the loss is 1124.4771670824468, parameters k is -37.5107133201147 and b is 255.05359581488744\n",
      "Iteration 4426, the loss is 1123.863595193828, parameters k is -37.49771924896845 and b is 254.97454443148823\n",
      "Iteration 4427, the loss is 1123.2756348067994, parameters k is -37.484725177822206 and b is 254.89154047891907\n",
      "Iteration 4428, the loss is 1122.6624023132154, parameters k is -37.47173110667596 and b is 254.81248909551985\n",
      "Iteration 4429, the loss is 1122.0747712766624, parameters k is -37.458737035529715 and b is 254.7294851429507\n",
      "Iteration 4430, the loss is 1121.4618781781169, parameters k is -37.44574296438347 and b is 254.65043375955148\n",
      "Iteration 4431, the loss is 1120.8745764920402, parameters k is -37.432748893237225 and b is 254.56742980698232\n",
      "Iteration 4432, the loss is 1120.2620227885322, parameters k is -37.41975482209098 and b is 254.4883784235831\n",
      "Iteration 4433, the loss is 1119.6750504529334, parameters k is -37.406760750944734 and b is 254.40537447101394\n",
      "Iteration 4434, the loss is 1119.0628361444594, parameters k is -37.39376667979849 and b is 254.32632308761472\n",
      "Iteration 4435, the loss is 1118.4761931593405, parameters k is -37.380772608652244 and b is 254.24331913504557\n",
      "Iteration 4436, the loss is 1117.8643182459011, parameters k is -37.367778537506 and b is 254.16426775164635\n",
      "Iteration 4437, the loss is 1117.278004611258, parameters k is -37.35478446635975 and b is 254.0812637990772\n",
      "Iteration 4438, the loss is 1116.6664690928574, parameters k is -37.34179039521351 and b is 254.00221241567797\n",
      "Iteration 4439, the loss is 1116.0804848086923, parameters k is -37.32879632406726 and b is 253.91920846310882\n",
      "Iteration 4440, the loss is 1115.4692886853265, parameters k is -37.31580225292102 and b is 253.8401570797096\n",
      "Iteration 4441, the loss is 1114.8836337516395, parameters k is -37.30280818177477 and b is 253.75715312714044\n",
      "Iteration 4442, the loss is 1114.272777023311, parameters k is -37.28981411062853 and b is 253.67810174374122\n",
      "Iteration 4443, the loss is 1113.6621003150428, parameters k is -37.27682003948228 and b is 253.599050360342\n",
      "Iteration 4444, the loss is 1113.0769341068087, parameters k is -37.263825968336036 and b is 253.51604640777285\n",
      "Iteration 4445, the loss is 1112.466596793578, parameters k is -37.25083189718979 and b is 253.43699502437363\n",
      "Iteration 4446, the loss is 1111.8817599358194, parameters k is -37.237837826043545 and b is 253.35399107180447\n",
      "Iteration 4447, the loss is 1111.2717620176259, parameters k is -37.2248437548973 and b is 253.27493968840525\n",
      "Iteration 4448, the loss is 1110.6872545103438, parameters k is -37.211849683751055 and b is 253.1919357358361\n",
      "Iteration 4449, the loss is 1110.0775959871864, parameters k is -37.19885561260481 and b is 253.11288435243688\n",
      "Iteration 4450, the loss is 1109.4934178303822, parameters k is -37.185861541458564 and b is 253.02988039986772\n",
      "Iteration 4451, the loss is 1108.884098702261, parameters k is -37.17286747031232 and b is 252.9508290164685\n",
      "Iteration 4452, the loss is 1108.300249895935, parameters k is -37.159873399166074 and b is 252.86782506389935\n",
      "Iteration 4453, the loss is 1107.6912701628505, parameters k is -37.14687932801983 and b is 252.78877368050013\n",
      "Iteration 4454, the loss is 1107.107750707001, parameters k is -37.13388525687358 and b is 252.70576972793097\n",
      "Iteration 4455, the loss is 1106.499110368951, parameters k is -37.12089118572734 and b is 252.62671834453175\n",
      "Iteration 4456, the loss is 1105.9159202635792, parameters k is -37.10789711458109 and b is 252.5437143919626\n",
      "Iteration 4457, the loss is 1105.3076193205677, parameters k is -37.09490304343485 and b is 252.46466300856338\n",
      "Iteration 4458, the loss is 1104.7247585656735, parameters k is -37.0819089722886 and b is 252.38165905599422\n",
      "Iteration 4459, the loss is 1104.1167970176978, parameters k is -37.06891490114236 and b is 252.302607672595\n",
      "Iteration 4460, the loss is 1103.5342656132818, parameters k is -37.05592082999611 and b is 252.21960372002584\n",
      "Iteration 4461, the loss is 1102.92664346034, parameters k is -37.042926758849866 and b is 252.14055233662663\n",
      "Iteration 4462, the loss is 1102.3444414064036, parameters k is -37.02993268770362 and b is 252.05754838405747\n",
      "Iteration 4463, the loss is 1101.7371586484994, parameters k is -37.016938616557375 and b is 251.97849700065825\n",
      "Iteration 4464, the loss is 1101.1300559106544, parameters k is -37.00394454541113 and b is 251.89944561725903\n",
      "Iteration 4465, the loss is 1100.548342582169, parameters k is -36.990950474264885 and b is 251.81644166468988\n",
      "Iteration 4466, the loss is 1099.9415792393609, parameters k is -36.97795640311864 and b is 251.73739028129066\n",
      "Iteration 4467, the loss is 1099.3601952613521, parameters k is -36.964962331972394 and b is 251.6543863287215\n",
      "Iteration 4468, the loss is 1098.753771313584, parameters k is -36.95196826082615 and b is 251.57533494532228\n",
      "Iteration 4469, the loss is 1098.1727166860517, parameters k is -36.938974189679904 and b is 251.49233099275312\n",
      "Iteration 4470, the loss is 1097.5666321333183, parameters k is -36.92598011853366 and b is 251.4132796093539\n",
      "Iteration 4471, the loss is 1096.9859068562644, parameters k is -36.91298604738741 and b is 251.33027565678475\n",
      "Iteration 4472, the loss is 1096.3801616985647, parameters k is -36.89999197624117 and b is 251.25122427338553\n",
      "Iteration 4473, the loss is 1095.7997657719889, parameters k is -36.88699790509492 and b is 251.16822032081637\n",
      "Iteration 4474, the loss is 1095.194360009329, parameters k is -36.87400383394868 and b is 251.08916893741716\n",
      "Iteration 4475, the loss is 1094.6142934332295, parameters k is -36.86100976280243 and b is 251.006164984848\n",
      "Iteration 4476, the loss is 1094.0092270656048, parameters k is -36.84801569165619 and b is 250.92711360144878\n",
      "Iteration 4477, the loss is 1093.429489839983, parameters k is -36.83502162050994 and b is 250.84410964887962\n",
      "Iteration 4478, the loss is 1092.8247628673948, parameters k is -36.822027549363696 and b is 250.7650582654804\n",
      "Iteration 4479, the loss is 1092.2453549922486, parameters k is -36.80903347821745 and b is 250.68205431291125\n",
      "Iteration 4480, the loss is 1091.640967414699, parameters k is -36.796039407071206 and b is 250.60300292951203\n",
      "Iteration 4481, the loss is 1091.0618888900296, parameters k is -36.78304533592496 and b is 250.51999897694287\n",
      "Iteration 4482, the loss is 1090.4578407075144, parameters k is -36.770051264778715 and b is 250.44094759354365\n",
      "Iteration 4483, the loss is 1089.8790915333236, parameters k is -36.75705719363247 and b is 250.3579436409745\n",
      "Iteration 4484, the loss is 1089.275382745845, parameters k is -36.744063122486224 and b is 250.27889225757528\n",
      "Iteration 4485, the loss is 1088.6969629221323, parameters k is -36.73106905133998 and b is 250.19588830500612\n",
      "Iteration 4486, the loss is 1088.0935935296895, parameters k is -36.718074980193734 and b is 250.1168369216069\n",
      "Iteration 4487, the loss is 1087.4904041573088, parameters k is -36.70508090904749 and b is 250.0377855382077\n",
      "Iteration 4488, the loss is 1086.912473059048, parameters k is -36.69208683790124 and b is 249.95478158563853\n",
      "Iteration 4489, the loss is 1086.3096230817016, parameters k is -36.679092766755 and b is 249.8757302022393\n",
      "Iteration 4490, the loss is 1085.7320213339217, parameters k is -36.66609869560875 and b is 249.79272624967015\n",
      "Iteration 4491, the loss is 1085.1295107516119, parameters k is -36.65310462446251 and b is 249.71367486627094\n",
      "Iteration 4492, the loss is 1084.552238354308, parameters k is -36.64011055331626 and b is 249.63067091370178\n",
      "Iteration 4493, the loss is 1083.9500671670335, parameters k is -36.62711648217002 and b is 249.55161953030256\n",
      "Iteration 4494, the loss is 1083.373124120207, parameters k is -36.61412241102377 and b is 249.4686155777334\n",
      "Iteration 4495, the loss is 1082.7712923279705, parameters k is -36.601128339877526 and b is 249.38956419433418\n",
      "Iteration 4496, the loss is 1082.1946786316196, parameters k is -36.58813426873128 and b is 249.30656024176503\n",
      "Iteration 4497, the loss is 1081.5931862344191, parameters k is -36.575140197585036 and b is 249.2275088583658\n",
      "Iteration 4498, the loss is 1081.016901888546, parameters k is -36.56214612643879 and b is 249.14450490579665\n",
      "Iteration 4499, the loss is 1080.4157488863827, parameters k is -36.549152055292545 and b is 249.06545352239743\n",
      "Iteration 4500, the loss is 1079.8397938909866, parameters k is -36.5361579841463 and b is 248.98244956982828\n",
      "Iteration 4501, the loss is 1079.2389802838584, parameters k is -36.523163913000054 and b is 248.90339818642906\n",
      "Iteration 4502, the loss is 1078.6633546389414, parameters k is -36.51016984185381 and b is 248.8203942338599\n",
      "Iteration 4503, the loss is 1078.062880426851, parameters k is -36.497175770707564 and b is 248.74134285046068\n",
      "Iteration 4504, the loss is 1077.4875841324097, parameters k is -36.48418169956132 and b is 248.65833889789153\n",
      "Iteration 4505, the loss is 1076.8874493153553, parameters k is -36.47118762841507 and b is 248.5792875144923\n",
      "Iteration 4506, the loss is 1076.312482371391, parameters k is -36.45819355726883 and b is 248.49628356192315\n",
      "Iteration 4507, the loss is 1075.712686949374, parameters k is -36.44519948612258 and b is 248.41723217852393\n",
      "Iteration 4508, the loss is 1075.1380493558877, parameters k is -36.43220541497634 and b is 248.33422822595477\n",
      "Iteration 4509, the loss is 1074.5385933289067, parameters k is -36.41921134383009 and b is 248.25517684255556\n",
      "Iteration 4510, the loss is 1073.9393173219846, parameters k is -36.40621727268385 and b is 248.17612545915634\n",
      "Iteration 4511, the loss is 1073.3651684539511, parameters k is -36.3932232015376 and b is 248.09312150658718\n",
      "Iteration 4512, the loss is 1072.7662318420678, parameters k is -36.380229130391356 and b is 248.01407012318796\n",
      "Iteration 4513, the loss is 1072.1924123245112, parameters k is -36.36723505924511 and b is 247.9310661706188\n",
      "Iteration 4514, the loss is 1071.5938151076627, parameters k is -36.354240988098866 and b is 247.8520147872196\n",
      "Iteration 4515, the loss is 1071.0203249405843, parameters k is -36.34124691695262 and b is 247.76901083465043\n",
      "Iteration 4516, the loss is 1070.4220671187727, parameters k is -36.328252845806375 and b is 247.6899594512512\n",
      "Iteration 4517, the loss is 1069.8489063021714, parameters k is -36.31525877466013 and b is 247.60695549868205\n",
      "Iteration 4518, the loss is 1069.2509878753954, parameters k is -36.302264703513885 and b is 247.52790411528284\n",
      "Iteration 4519, the loss is 1068.6781564092726, parameters k is -36.28927063236764 and b is 247.44490016271368\n",
      "Iteration 4520, the loss is 1068.0805773775332, parameters k is -36.276276561221394 and b is 247.36584877931446\n",
      "Iteration 4521, the loss is 1067.508075261887, parameters k is -36.26328249007515 and b is 247.2828448267453\n",
      "Iteration 4522, the loss is 1066.9108356251845, parameters k is -36.2502884189289 and b is 247.2037934433461\n",
      "Iteration 4523, the loss is 1066.3386628600147, parameters k is -36.23729434778266 and b is 247.12078949077693\n",
      "Iteration 4524, the loss is 1065.7417626183485, parameters k is -36.22430027663641 and b is 247.0417381073777\n",
      "Iteration 4525, the loss is 1065.1699192036558, parameters k is -36.21130620549017 and b is 246.95873415480855\n",
      "Iteration 4526, the loss is 1064.5733583570263, parameters k is -36.19831213434392 and b is 246.87968277140934\n",
      "Iteration 4527, the loss is 1064.0018442928122, parameters k is -36.18531806319768 and b is 246.79667881884018\n",
      "Iteration 4528, the loss is 1063.4056228412185, parameters k is -36.17232399205143 and b is 246.71762743544096\n",
      "Iteration 4529, the loss is 1062.8344381274812, parameters k is -36.159329920905186 and b is 246.6346234828718\n",
      "Iteration 4530, the loss is 1062.238556070924, parameters k is -36.14633584975894 and b is 246.55557209947258\n",
      "Iteration 4531, the loss is 1061.6677007076637, parameters k is -36.133341778612696 and b is 246.47256814690343\n",
      "Iteration 4532, the loss is 1061.0721580461434, parameters k is -36.12034770746645 and b is 246.3935167635042\n",
      "Iteration 4533, the loss is 1060.476795404684, parameters k is -36.107353636320205 and b is 246.314465380105\n",
      "Iteration 4534, the loss is 1059.9064287668778, parameters k is -36.09435956517396 and b is 246.23146142753583\n",
      "Iteration 4535, the loss is 1059.3114055204546, parameters k is -36.081365494027715 and b is 246.15241004413662\n",
      "Iteration 4536, the loss is 1058.7413682331246, parameters k is -36.06837142288147 and b is 246.06940609156746\n",
      "Iteration 4537, the loss is 1058.1466843817384, parameters k is -36.055377351735224 and b is 245.99035470816824\n",
      "Iteration 4538, the loss is 1057.5769764448853, parameters k is -36.04238328058898 and b is 245.90735075559908\n",
      "Iteration 4539, the loss is 1056.9826319885342, parameters k is -36.02938920944273 and b is 245.82829937219987\n",
      "Iteration 4540, the loss is 1056.4132534021596, parameters k is -36.01639513829649 and b is 245.7452954196307\n",
      "Iteration 4541, the loss is 1055.8192483408448, parameters k is -36.00340106715024 and b is 245.6662440362315\n",
      "Iteration 4542, the loss is 1055.2501991049478, parameters k is -35.990406996004 and b is 245.58324008366233\n",
      "Iteration 4543, the loss is 1054.656533438669, parameters k is -35.97741292485775 and b is 245.50418870026311\n",
      "Iteration 4544, the loss is 1054.087813553249, parameters k is -35.96441885371151 and b is 245.42118474769396\n",
      "Iteration 4545, the loss is 1053.494487282007, parameters k is -35.95142478256526 and b is 245.34213336429474\n",
      "Iteration 4546, the loss is 1052.9260967470643, parameters k is -35.93843071141902 and b is 245.25912941172558\n",
      "Iteration 4547, the loss is 1052.333109870861, parameters k is -35.92543664027277 and b is 245.18007802832636\n",
      "Iteration 4548, the loss is 1051.7650486863934, parameters k is -35.912442569126526 and b is 245.0970740757572\n",
      "Iteration 4549, the loss is 1051.1724012052255, parameters k is -35.89944849798028 and b is 245.018022692358\n",
      "Iteration 4550, the loss is 1050.6046693712365, parameters k is -35.886454426834035 and b is 244.93501873978883\n",
      "Iteration 4551, the loss is 1050.0123612851048, parameters k is -35.87346035568779 and b is 244.8559673563896\n",
      "Iteration 4552, the loss is 1049.4449588015932, parameters k is -35.860466284541545 and b is 244.77296340382046\n",
      "Iteration 4553, the loss is 1048.8529901104982, parameters k is -35.8474722133953 and b is 244.69391202042124\n",
      "Iteration 4554, the loss is 1048.2859169774633, parameters k is -35.834478142249054 and b is 244.61090806785208\n",
      "Iteration 4555, the loss is 1047.6942876814053, parameters k is -35.82148407110281 and b is 244.53185668445286\n",
      "Iteration 4556, the loss is 1047.1028384054052, parameters k is -35.808489999956564 and b is 244.45280530105364\n",
      "Iteration 4557, the loss is 1046.5362539978253, parameters k is -35.79549592881032 and b is 244.3698013484845\n",
      "Iteration 4558, the loss is 1045.945144116862, parameters k is -35.78250185766407 and b is 244.29074996508527\n",
      "Iteration 4559, the loss is 1045.378889059761, parameters k is -35.76950778651783 and b is 244.2077460125161\n",
      "Iteration 4560, the loss is 1044.788118573834, parameters k is -35.75651371537158 and b is 244.1286946291169\n",
      "Iteration 4561, the loss is 1044.2221928672082, parameters k is -35.74351964422534 and b is 244.04569067654774\n",
      "Iteration 4562, the loss is 1043.6317617763182, parameters k is -35.73052557307909 and b is 243.96663929314852\n",
      "Iteration 4563, the loss is 1043.0661654201701, parameters k is -35.71753150193285 and b is 243.88363534057936\n",
      "Iteration 4564, the loss is 1042.4760737243157, parameters k is -35.7045374307866 and b is 243.80458395718014\n",
      "Iteration 4565, the loss is 1041.9108067186444, parameters k is -35.691543359640356 and b is 243.72158000461098\n",
      "Iteration 4566, the loss is 1041.3210544178282, parameters k is -35.67854928849411 and b is 243.64252862121177\n",
      "Iteration 4567, the loss is 1040.7561167626347, parameters k is -35.665555217347865 and b is 243.5595246686426\n",
      "Iteration 4568, the loss is 1040.1667038568537, parameters k is -35.65256114620162 and b is 243.4804732852434\n",
      "Iteration 4569, the loss is 1039.6020955521376, parameters k is -35.639567075055375 and b is 243.39746933267423\n",
      "Iteration 4570, the loss is 1039.013022041393, parameters k is -35.62657300390913 and b is 243.31841794927502\n",
      "Iteration 4571, the loss is 1038.4487430871534, parameters k is -35.613578932762884 and b is 243.23541399670586\n",
      "Iteration 4572, the loss is 1037.860008971447, parameters k is -35.60058486161664 and b is 243.15636261330664\n",
      "Iteration 4573, the loss is 1037.2960593676842, parameters k is -35.587590790470394 and b is 243.07335866073748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4574, the loss is 1036.7076646470132, parameters k is -35.57459671932415 and b is 242.99430727733827\n",
      "Iteration 4575, the loss is 1036.144044393728, parameters k is -35.5616026481779 and b is 242.9113033247691\n",
      "Iteration 4576, the loss is 1035.5559890680936, parameters k is -35.54860857703166 and b is 242.8322519413699\n",
      "Iteration 4577, the loss is 1034.968113762518, parameters k is -35.53561450588541 and b is 242.75320055797067\n",
      "Iteration 4578, the loss is 1034.404982234688, parameters k is -35.52262043473917 and b is 242.67019660540151\n",
      "Iteration 4579, the loss is 1033.8174463241505, parameters k is -35.50962636359292 and b is 242.5911452220023\n",
      "Iteration 4580, the loss is 1033.2546441467955, parameters k is -35.49663229244668 and b is 242.50814126943314\n",
      "Iteration 4581, the loss is 1032.6674476312946, parameters k is -35.48363822130043 and b is 242.42908988603392\n",
      "Iteration 4582, the loss is 1032.1049748044181, parameters k is -35.470644150154186 and b is 242.34608593346476\n",
      "Iteration 4583, the loss is 1031.5181176839526, parameters k is -35.45765007900794 and b is 242.26703455006555\n",
      "Iteration 4584, the loss is 1030.9559742075528, parameters k is -35.444656007861695 and b is 242.1840305974964\n",
      "Iteration 4585, the loss is 1030.3694564821249, parameters k is -35.43166193671545 and b is 242.10497921409717\n",
      "Iteration 4586, the loss is 1029.807642356203, parameters k is -35.418667865569205 and b is 242.021975261528\n",
      "Iteration 4587, the loss is 1029.2214640258107, parameters k is -35.40567379442296 and b is 241.9429238781288\n",
      "Iteration 4588, the loss is 1028.6599792503662, parameters k is -35.392679723276714 and b is 241.85991992555964\n",
      "Iteration 4589, the loss is 1028.0741403150091, parameters k is -35.37968565213047 and b is 241.78086854216042\n",
      "Iteration 4590, the loss is 1027.5129848900428, parameters k is -35.366691580984224 and b is 241.69786458959126\n",
      "Iteration 4591, the loss is 1026.9274853497234, parameters k is -35.35369750983798 and b is 241.61881320619204\n",
      "Iteration 4592, the loss is 1026.3666592752331, parameters k is -35.34070343869173 and b is 241.5358092536229\n",
      "Iteration 4593, the loss is 1025.78149912995, parameters k is -35.32770936754549 and b is 241.45675787022367\n",
      "Iteration 4594, the loss is 1025.2210024059364, parameters k is -35.31471529639924 and b is 241.3737539176545\n",
      "Iteration 4595, the loss is 1024.6361816556896, parameters k is -35.301721225253 and b is 241.2947025342553\n",
      "Iteration 4596, the loss is 1024.076014282155, parameters k is -35.28872715410675 and b is 241.21169858168614\n",
      "Iteration 4597, the loss is 1023.4915329269445, parameters k is -35.27573308296051 and b is 241.13264719828692\n",
      "Iteration 4598, the loss is 1022.9316949038864, parameters k is -35.26273901181426 and b is 241.04964324571776\n",
      "Iteration 4599, the loss is 1022.3475529437126, parameters k is -35.249744940668016 and b is 240.97059186231854\n",
      "Iteration 4600, the loss is 1021.7635910035988, parameters k is -35.23675086952177 and b is 240.89154047891932\n",
      "Iteration 4601, the loss is 1021.2042417059938, parameters k is -35.223756798375526 and b is 240.80853652635017\n",
      "Iteration 4602, the loss is 1020.6206191609168, parameters k is -35.21076272722928 and b is 240.72948514295095\n",
      "Iteration 4603, the loss is 1020.0615992137914, parameters k is -35.197768656083035 and b is 240.6464811903818\n",
      "Iteration 4604, the loss is 1019.478316063749, parameters k is -35.18477458493679 and b is 240.56742980698257\n",
      "Iteration 4605, the loss is 1018.9196254670995, parameters k is -35.171780513790544 and b is 240.48442585441342\n",
      "Iteration 4606, the loss is 1018.3366817120941, parameters k is -35.1587864426443 and b is 240.4053744710142\n",
      "Iteration 4607, the loss is 1017.7783204659215, parameters k is -35.145792371498054 and b is 240.32237051844504\n",
      "Iteration 4608, the loss is 1017.1957161059535, parameters k is -35.13279830035181 and b is 240.24331913504582\n",
      "Iteration 4609, the loss is 1016.6376842102591, parameters k is -35.11980422920556 and b is 240.16031518247667\n",
      "Iteration 4610, the loss is 1016.0554192453276, parameters k is -35.10681015805932 and b is 240.08126379907745\n",
      "Iteration 4611, the loss is 1015.4977167001089, parameters k is -35.09381608691307 and b is 239.9982598465083\n",
      "Iteration 4612, the loss is 1014.915791130214, parameters k is -35.08082201576683 and b is 239.91920846310907\n",
      "Iteration 4613, the loss is 1014.358417935472, parameters k is -35.06782794462058 and b is 239.83620451053991\n",
      "Iteration 4614, the loss is 1013.7768317606138, parameters k is -35.05483387347434 and b is 239.7571531271407\n",
      "Iteration 4615, the loss is 1013.2197879163505, parameters k is -35.04183980232809 and b is 239.67414917457154\n",
      "Iteration 4616, the loss is 1012.638541136529, parameters k is -35.028845731181846 and b is 239.59509779117232\n",
      "Iteration 4617, the loss is 1012.0818266427423, parameters k is -35.0158516600356 and b is 239.51209383860316\n",
      "Iteration 4618, the loss is 1011.5009192579579, parameters k is -35.002857588889356 and b is 239.43304245520395\n",
      "Iteration 4619, the loss is 1010.9445341146475, parameters k is -34.98986351774311 and b is 239.3500385026348\n",
      "Iteration 4620, the loss is 1010.3639661248982, parameters k is -34.976869446596865 and b is 239.27098711923557\n",
      "Iteration 4621, the loss is 1009.807910332068, parameters k is -34.96387537545062 and b is 239.1879831666664\n",
      "Iteration 4622, the loss is 1009.2276817373544, parameters k is -34.950881304304374 and b is 239.1089317832672\n",
      "Iteration 4623, the loss is 1008.6476331627002, parameters k is -34.93788723315813 and b is 239.02988039986798\n",
      "Iteration 4624, the loss is 1008.0920660953242, parameters k is -34.924893162011884 and b is 238.94687644729882\n",
      "Iteration 4625, the loss is 1007.5123569157064, parameters k is -34.91189909086564 and b is 238.8678250638996\n",
      "Iteration 4626, the loss is 1006.9571191988056, parameters k is -34.89890501971939 and b is 238.78482111133044\n",
      "Iteration 4627, the loss is 1006.3777494142267, parameters k is -34.88591094857315 and b is 238.70576972793123\n",
      "Iteration 4628, the loss is 1005.8228410478035, parameters k is -34.8729168774269 and b is 238.62276577536207\n",
      "Iteration 4629, the loss is 1005.2438106582593, parameters k is -34.85992280628066 and b is 238.54371439196285\n",
      "Iteration 4630, the loss is 1004.6892316423144, parameters k is -34.84692873513441 and b is 238.4607104393937\n",
      "Iteration 4631, the loss is 1004.1105406478064, parameters k is -34.83393466398817 and b is 238.38165905599448\n",
      "Iteration 4632, the loss is 1003.5562909823377, parameters k is -34.82094059284192 and b is 238.29865510342532\n",
      "Iteration 4633, the loss is 1002.9779393828674, parameters k is -34.807946521695676 and b is 238.2196037200261\n",
      "Iteration 4634, the loss is 1002.4240190678755, parameters k is -34.79495245054943 and b is 238.13659976745694\n",
      "Iteration 4635, the loss is 1001.8460068634407, parameters k is -34.781958379403186 and b is 238.05754838405772\n",
      "Iteration 4636, the loss is 1001.292415898926, parameters k is -34.76896430825694 and b is 237.97454443148857\n",
      "Iteration 4637, the loss is 1000.714743089529, parameters k is -34.755970237110695 and b is 237.89549304808935\n",
      "Iteration 4638, the loss is 1000.1614814754934, parameters k is -34.74297616596445 and b is 237.8124890955202\n",
      "Iteration 4639, the loss is 999.584148061131, parameters k is -34.729982094818205 and b is 237.73343771212097\n",
      "Iteration 4640, the loss is 999.0312157975715, parameters k is -34.71698802367196 and b is 237.65043375955182\n",
      "Iteration 4641, the loss is 998.4542217782459, parameters k is -34.703993952525714 and b is 237.5713823761526\n",
      "Iteration 4642, the loss is 997.9016188651634, parameters k is -34.69099988137947 and b is 237.48837842358344\n",
      "Iteration 4643, the loss is 997.3249642408749, parameters k is -34.67800581023322 and b is 237.40932704018422\n",
      "Iteration 4644, the loss is 996.7726906782696, parameters k is -34.66501173908698 and b is 237.32632308761507\n",
      "Iteration 4645, the loss is 996.1963754490188, parameters k is -34.65201766794073 and b is 237.24727170421585\n",
      "Iteration 4646, the loss is 995.6202402398262, parameters k is -34.63902359679449 and b is 237.16822032081663\n",
      "Iteration 4647, the loss is 995.0684554026765, parameters k is -34.62602952564824 and b is 237.08521636824747\n",
      "Iteration 4648, the loss is 994.4926595885197, parameters k is -34.613035454502 and b is 237.00616498484825\n",
      "Iteration 4649, the loss is 993.9412041018453, parameters k is -34.60004138335575 and b is 236.9231610322791\n",
      "Iteration 4650, the loss is 993.3657476827264, parameters k is -34.587047312209506 and b is 236.84410964887988\n",
      "Iteration 4651, the loss is 992.8146215465296, parameters k is -34.57405324106326 and b is 236.76110569631072\n",
      "Iteration 4652, the loss is 992.2395045224472, parameters k is -34.561059169917016 and b is 236.6820543129115\n",
      "Iteration 4653, the loss is 991.6887077367264, parameters k is -34.54806509877077 and b is 236.59905036034235\n",
      "Iteration 4654, the loss is 991.1139301076818, parameters k is -34.535071027624525 and b is 236.51999897694313\n",
      "Iteration 4655, the loss is 990.5634626724393, parameters k is -34.52207695647828 and b is 236.43699502437397\n",
      "Iteration 4656, the loss is 989.9890244384289, parameters k is -34.509082885332035 and b is 236.35794364097475\n",
      "Iteration 4657, the loss is 989.4388863536637, parameters k is -34.49608881418579 and b is 236.2749396884056\n",
      "Iteration 4658, the loss is 988.8647875146901, parameters k is -34.483094743039544 and b is 236.19588830500638\n",
      "Iteration 4659, the loss is 988.3149787804026, parameters k is -34.4701006718933 and b is 236.11288435243722\n",
      "Iteration 4660, the loss is 987.7412193364665, parameters k is -34.45710660074705 and b is 236.033832969038\n",
      "Iteration 4661, the loss is 987.1917399526553, parameters k is -34.44411252960081 and b is 235.95082901646884\n",
      "Iteration 4662, the loss is 986.6183199037548, parameters k is -34.43111845845456 and b is 235.87177763306963\n",
      "Iteration 4663, the loss is 986.0691698704214, parameters k is -34.41812438730832 and b is 235.78877368050047\n",
      "Iteration 4664, the loss is 985.4960892165598, parameters k is -34.40513031616207 and b is 235.70972229710125\n",
      "Iteration 4665, the loss is 984.9472685337025, parameters k is -34.39213624501583 and b is 235.6267183445321\n",
      "Iteration 4666, the loss is 984.3745272748754, parameters k is -34.37914217386958 and b is 235.54766696113288\n",
      "Iteration 4667, the loss is 983.8019660361075, parameters k is -34.366148102723336 and b is 235.46861557773366\n",
      "Iteration 4668, the loss is 983.2536340787052, parameters k is -34.35315403157709 and b is 235.3856116251645\n",
      "Iteration 4669, the loss is 982.681412234975, parameters k is -34.340159960430846 and b is 235.30656024176528\n",
      "Iteration 4670, the loss is 982.1334096280495, parameters k is -34.3271658892846 and b is 235.22355628919613\n",
      "Iteration 4671, the loss is 981.5615271793549, parameters k is -34.314171818138355 and b is 235.1445049057969\n",
      "Iteration 4672, the loss is 981.013853922907, parameters k is -34.30117774699211 and b is 235.06150095322775\n",
      "Iteration 4673, the loss is 980.442310869249, parameters k is -34.288183675845865 and b is 234.98244956982853\n",
      "Iteration 4674, the loss is 979.9430989553535, parameters k is -34.27518960469962 and b is 234.89154047891944\n",
      "Iteration 4675, the loss is 979.3718540065668, parameters k is -34.262195533553374 and b is 234.81248909552022\n",
      "Iteration 4676, the loss is 978.8489634720079, parameters k is -34.24920146240713 and b is 234.7255325737811\n",
      "Iteration 4677, the loss is 978.278037273173, parameters k is -34.23620739126088 and b is 234.64648119038188\n",
      "Iteration 4678, the loss is 977.7555078906631, parameters k is -34.22321332011464 and b is 234.55952466864275\n",
      "Iteration 4679, the loss is 977.2090350632412, parameters k is -34.21021924896839 and b is 234.4765207160736\n",
      "Iteration 4680, the loss is 976.6627322113188, parameters k is -34.19722517782215 and b is 234.39351676350444\n",
      "Iteration 4681, the loss is 976.1407864030085, parameters k is -34.1842311066759 and b is 234.3065602417653\n",
      "Iteration 4682, the loss is 975.5948341026091, parameters k is -34.17123703552966 and b is 234.22355628919615\n",
      "Iteration 4683, the loss is 975.0490517777126, parameters k is -34.15824296438341 and b is 234.140552336627\n",
      "Iteration 4684, the loss is 974.5034394283159, parameters k is -34.14524889323717 and b is 234.05754838405784\n",
      "Iteration 4685, the loss is 973.9579970544191, parameters k is -34.13225482209092 and b is 233.97454443148868\n",
      "Iteration 4686, the loss is 973.4127246560232, parameters k is -34.119260750944676 and b is 233.89154047891952\n",
      "Iteration 4687, the loss is 972.8676222331262, parameters k is -34.10626667979843 and b is 233.80853652635037\n",
      "Iteration 4688, the loss is 972.3469823031116, parameters k is -34.093272608652185 and b is 233.72158000461124\n",
      "Iteration 4689, the loss is 971.8022304317398, parameters k is -34.08027853750594 and b is 233.63857605204208\n",
      "Iteration 4690, the loss is 971.2576485358672, parameters k is -34.067284466359695 and b is 233.55557209947293\n",
      "Iteration 4691, the loss is 970.7132366154958, parameters k is -34.05429039521345 and b is 233.47256814690377\n",
      "Iteration 4692, the loss is 970.1689946706271, parameters k is -34.041296324067204 and b is 233.3895641943346\n",
      "Iteration 4693, the loss is 969.6249227012546, parameters k is -34.02830225292096 and b is 233.30656024176545\n",
      "Iteration 4694, the loss is 969.1054080735121, parameters k is -34.015308181774714 and b is 233.21960372002633\n",
      "Iteration 4695, the loss is 968.5616866556654, parameters k is -34.00231411062847 and b is 233.13659976745717\n",
      "Iteration 4696, the loss is 968.0181352133181, parameters k is -33.98932003948222 and b is 233.053595814888\n",
      "Iteration 4697, the loss is 967.4747537464721, parameters k is -33.97632596833598 and b is 232.97059186231886\n",
      "Iteration 4698, the loss is 966.931542255127, parameters k is -33.96333189718973 and b is 232.8875879097497\n",
      "Iteration 4699, the loss is 966.3885007392814, parameters k is -33.95033782604349 and b is 232.80458395718054\n",
      "Iteration 4700, the loss is 965.8701114138097, parameters k is -33.93734375489724 and b is 232.71762743544141\n",
      "Iteration 4701, the loss is 965.3274204494878, parameters k is -33.924349683751 and b is 232.63462348287226\n",
      "Iteration 4702, the loss is 964.7848994606687, parameters k is -33.91135561260475 and b is 232.5516195303031\n",
      "Iteration 4703, the loss is 964.2425484473474, parameters k is -33.898361541458506 and b is 232.46861557773394\n",
      "Iteration 4704, the loss is 963.7003674095263, parameters k is -33.88536747031226 and b is 232.38561162516478\n",
      "Iteration 4705, the loss is 963.1583563472068, parameters k is -33.872373399166015 and b is 232.30260767259563\n",
      "Iteration 4706, the loss is 962.6410923240078, parameters k is -33.85937932801977 and b is 232.2156511508565\n",
      "Iteration 4707, the loss is 962.0994318132117, parameters k is -33.846385256873525 and b is 232.13264719828734\n",
      "Iteration 4708, the loss is 961.5579412779157, parameters k is -33.83339118572728 and b is 232.0496432457182\n",
      "Iteration 4709, the loss is 960.9920118529307, parameters k is -33.820397114581034 and b is 231.97059186231897\n",
      "Iteration 4710, the loss is 960.4754701338259, parameters k is -33.80740304343479 and b is 231.88363534057984\n",
      "Iteration 4711, the loss is 959.9098594587947, parameters k is -33.794408972288544 and b is 231.80458395718063\n",
      "Iteration 4712, the loss is 959.3690382249767, parameters k is -33.7814149011423 and b is 231.72158000461147\n",
      "Iteration 4713, the loss is 958.8283869666592, parameters k is -33.76842082999605 and b is 231.6385760520423\n",
      "Iteration 4714, the loss is 958.2879056838427, parameters k is -33.75542675884981 and b is 231.55557209947315\n",
      "Iteration 4715, the loss is 957.747594376526, parameters k is -33.74243268770356 and b is 231.472568146904\n",
      "Iteration 4716, the loss is 957.2074530447092, parameters k is -33.72943861655732 and b is 231.38956419433484\n",
      "Iteration 4717, the loss is 956.667481688393, parameters k is -33.71644454541107 and b is 231.30656024176568\n",
      "Iteration 4718, the loss is 956.1276803075763, parameters k is -33.70345047426483 and b is 231.22355628919652\n",
      "Iteration 4719, the loss is 955.5880489022621, parameters k is -33.69045640311858 and b is 231.14055233662737\n",
      "Iteration 4720, the loss is 955.0485874724466, parameters k is -33.677462331972336 and b is 231.0575483840582\n",
      "Iteration 4721, the loss is 954.5092960181316, parameters k is -33.66446826082609 and b is 230.97454443148905\n",
      "Iteration 4722, the loss is 953.9701745393169, parameters k is -33.651474189679845 and b is 230.8915404789199\n",
      "Iteration 4723, the loss is 953.4312230360017, parameters k is -33.6384801185336 and b is 230.80853652635074\n",
      "Iteration 4724, the loss is 952.8924415081892, parameters k is -33.625486047387355 and b is 230.72553257378158\n",
      "Iteration 4725, the loss is 952.3290827279162, parameters k is -33.61249197624111 and b is 230.64648119038236\n",
      "Iteration 4726, the loss is 951.7906305505796, parameters k is -33.599497905094864 and b is 230.5634772378132\n",
      "Iteration 4727, the loss is 951.2523483487433, parameters k is -33.58650383394862 and b is 230.48047328524405\n",
      "Iteration 4728, the loss is 950.689488338485, parameters k is -33.573509762802374 and b is 230.40142190184483\n",
      "Iteration 4729, the loss is 950.1515354871267, parameters k is -33.56051569165613 and b is 230.31841794927567\n",
      "Iteration 4730, the loss is 949.5890148719039, parameters k is -33.54752162050988 and b is 230.23936656587645\n",
      "Iteration 4731, the loss is 949.0513913710217, parameters k is -33.53452754936364 and b is 230.1563626133073\n",
      "Iteration 4732, the loss is 948.4892101508365, parameters k is -33.52153347821739 and b is 230.07731122990808\n",
      "Iteration 4733, the loss is 947.9519160004309, parameters k is -33.50853940707115 and b is 229.99430727733892\n",
      "Iteration 4734, the loss is 947.3900741752823, parameters k is -33.4955453359249 and b is 229.9152558939397\n",
      "Iteration 4735, the loss is 946.8531093753538, parameters k is -33.48255126477866 and b is 229.83225194137054\n",
      "Iteration 4736, the loss is 946.2916069452425, parameters k is -33.46955719363241 and b is 229.75320055797133\n",
      "Iteration 4737, the loss is 945.7549714957911, parameters k is -33.456563122486166 and b is 229.67019660540217\n",
      "Iteration 4738, the loss is 945.218506021843, parameters k is -33.44356905133992 and b is 229.587192652833\n",
      "Iteration 4739, the loss is 944.6575023617419, parameters k is -33.430574980193676 and b is 229.5081412694338\n",
      "Iteration 4740, the loss is 944.1213662382695, parameters k is -33.41758090904743 and b is 229.42513731686464\n",
      "Iteration 4741, the loss is 943.5607019732067, parameters k is -33.404586837901185 and b is 229.34608593346542\n",
      "Iteration 4742, the loss is 943.0248952002115, parameters k is -33.39159276675494 and b is 229.26308198089626\n",
      "Iteration 4743, the loss is 942.4645703301851, parameters k is -33.378598695608694 and b is 229.18403059749704\n",
      "Iteration 4744, the loss is 941.9290929076662, parameters k is -33.36560462446245 and b is 229.10102664492788\n",
      "Iteration 4745, the loss is 941.3691074326781, parameters k is -33.352610553316204 and b is 229.02197526152867\n",
      "Iteration 4746, the loss is 940.8339593606366, parameters k is -33.33961648216996 and b is 228.9389713089595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4747, the loss is 940.2743132806826, parameters k is -33.32662241102371 and b is 228.8599199255603\n",
      "Iteration 4748, the loss is 939.7394945591183, parameters k is -33.31362833987747 and b is 228.77691597299113\n",
      "Iteration 4749, the loss is 939.2048458130546, parameters k is -33.30063426873122 and b is 228.69391202042198\n",
      "Iteration 4750, the loss is 938.6456985031151, parameters k is -33.28764019758498 and b is 228.61486063702276\n",
      "Iteration 4751, the loss is 938.1113791075301, parameters k is -33.27464612643873 and b is 228.5318566844536\n",
      "Iteration 4752, the loss is 937.5525711926255, parameters k is -33.26165205529249 and b is 228.45280530105438\n",
      "Iteration 4753, the loss is 937.0185811475167, parameters k is -33.24865798414624 and b is 228.36980134848523\n",
      "Iteration 4754, the loss is 936.4601126276497, parameters k is -33.235663912999996 and b is 228.290749965086\n",
      "Iteration 4755, the loss is 935.9264519330192, parameters k is -33.22266984185375 and b is 228.20774601251685\n",
      "Iteration 4756, the loss is 935.3683228081873, parameters k is -33.209675770707506 and b is 228.12869462911763\n",
      "Iteration 4757, the loss is 934.8349914640335, parameters k is -33.19668169956126 and b is 228.04569067654847\n",
      "Iteration 4758, the loss is 934.277201734238, parameters k is -33.183687628415015 and b is 227.96663929314926\n",
      "Iteration 4759, the loss is 933.7441997405617, parameters k is -33.17069355726877 and b is 227.8836353405801\n",
      "Iteration 4760, the loss is 933.1867494058051, parameters k is -33.157699486122524 and b is 227.80458395718088\n",
      "Iteration 4761, the loss is 932.6540767626046, parameters k is -33.14470541497628 and b is 227.72158000461172\n",
      "Iteration 4762, the loss is 932.1215740949049, parameters k is -33.131711343830034 and b is 227.63857605204257\n",
      "Iteration 4763, the loss is 931.5646225301605, parameters k is -33.11871727268379 and b is 227.55952466864335\n",
      "Iteration 4764, the loss is 931.0324492129384, parameters k is -33.10572320153754 and b is 227.4765207160742\n",
      "Iteration 4765, the loss is 930.4758370432298, parameters k is -33.0927291303913 and b is 227.39746933267497\n",
      "Iteration 4766, the loss is 929.9439930764855, parameters k is -33.07973505924505 and b is 227.31446538010582\n",
      "Iteration 4767, the loss is 929.3877203018142, parameters k is -33.06674098809881 and b is 227.2354139967066\n",
      "Iteration 4768, the loss is 928.8562056855457, parameters k is -33.05374691695256 and b is 227.15241004413744\n",
      "Iteration 4769, the loss is 928.3002723059112, parameters k is -33.04075284580632 and b is 227.07335866073822\n",
      "Iteration 4770, the loss is 927.7690870401208, parameters k is -33.02775877466007 and b is 226.99035470816906\n",
      "Iteration 4771, the loss is 927.2134930555231, parameters k is -33.014764703513826 and b is 226.91130332476985\n",
      "Iteration 4772, the loss is 926.6826371402091, parameters k is -33.00177063236758 and b is 226.8282993722007\n",
      "Iteration 4773, the loss is 926.151951200395, parameters k is -32.988776561221336 and b is 226.74529541963153\n",
      "Iteration 4774, the loss is 925.5968559858104, parameters k is -32.97578249007509 and b is 226.6662440362323\n",
      "Iteration 4775, the loss is 925.0664993964748, parameters k is -32.962788418928845 and b is 226.58324008366316\n",
      "Iteration 4776, the loss is 924.5117435769265, parameters k is -32.9497943477826 and b is 226.50418870026394\n",
      "Iteration 4777, the loss is 923.9817163380679, parameters k is -32.936800276636355 and b is 226.42118474769478\n",
      "Iteration 4778, the loss is 923.4272999135568, parameters k is -32.92380620549011 and b is 226.34213336429556\n",
      "Iteration 4779, the loss is 922.8976020251747, parameters k is -32.910812134343864 and b is 226.2591294117264\n",
      "Iteration 4780, the loss is 922.3435249956993, parameters k is -32.89781806319762 and b is 226.1800780283272\n",
      "Iteration 4781, the loss is 921.8141564577938, parameters k is -32.88482399205137 and b is 226.09707407575803\n",
      "Iteration 4782, the loss is 921.2604188233549, parameters k is -32.87182992090513 and b is 226.0180226923588\n",
      "Iteration 4783, the loss is 920.7313796359304, parameters k is -32.85883584975888 and b is 225.93501873978965\n",
      "Iteration 4784, the loss is 920.2025104240025, parameters k is -32.84584177861264 and b is 225.8520147872205\n",
      "Iteration 4785, the loss is 919.6492715595781, parameters k is -32.83284770746639 and b is 225.77296340382128\n",
      "Iteration 4786, the loss is 919.1207316981277, parameters k is -32.81985363632015 and b is 225.68995945125212\n",
      "Iteration 4787, the loss is 918.5678322287381, parameters k is -32.8068595651739 and b is 225.6109080678529\n",
      "Iteration 4788, the loss is 918.0396217177663, parameters k is -32.793865494027656 and b is 225.52790411528375\n",
      "Iteration 4789, the loss is 917.4870616434138, parameters k is -32.78087142288141 and b is 225.44885273188453\n",
      "Iteration 4790, the loss is 916.9591804829188, parameters k is -32.767877351735166 and b is 225.36584877931537\n",
      "Iteration 4791, the loss is 916.4314692979235, parameters k is -32.75488328058892 and b is 225.2828448267462\n",
      "Iteration 4792, the loss is 915.9039280884292, parameters k is -32.741889209442675 and b is 225.19984087417706\n",
      "Iteration 4793, the loss is 915.3765568544356, parameters k is -32.72889513829643 and b is 225.1168369216079\n",
      "Iteration 4794, the loss is 914.8493555959427, parameters k is -32.715901067150185 and b is 225.03383296903874\n",
      "Iteration 4795, the loss is 914.3223243129494, parameters k is -32.70290699600394 and b is 224.95082901646958\n",
      "Iteration 4796, the loss is 913.7954630054566, parameters k is -32.689912924857694 and b is 224.86782506390043\n",
      "Iteration 4797, the loss is 913.2687716734641, parameters k is -32.67691885371145 and b is 224.78482111133127\n",
      "Iteration 4798, the loss is 912.7422503169706, parameters k is -32.6639247825652 and b is 224.7018171587621\n",
      "Iteration 4799, the loss is 912.2158989359782, parameters k is -32.65093071141896 and b is 224.61881320619295\n",
      "Iteration 4800, the loss is 911.689717530487, parameters k is -32.63793664027271 and b is 224.5358092536238\n",
      "Iteration 4801, the loss is 911.1637061004949, parameters k is -32.62494256912647 and b is 224.45280530105464\n",
      "Iteration 4802, the loss is 910.6378646460038, parameters k is -32.61194849798022 and b is 224.36980134848548\n",
      "Iteration 4803, the loss is 910.1121931670128, parameters k is -32.59895442683398 and b is 224.28679739591632\n",
      "Iteration 4804, the loss is 909.5866916635224, parameters k is -32.58596035568773 and b is 224.20379344334717\n",
      "Iteration 4805, the loss is 909.0613601355327, parameters k is -32.57296628454149 and b is 224.120789490778\n",
      "Iteration 4806, the loss is 908.5361985830427, parameters k is -32.55997221339524 and b is 224.03778553820885\n",
      "Iteration 4807, the loss is 908.0112070060527, parameters k is -32.546978142248996 and b is 223.9547815856397\n",
      "Iteration 4808, the loss is 907.4863854045641, parameters k is -32.53398407110275 and b is 223.87177763307054\n",
      "Iteration 4809, the loss is 906.9617337785752, parameters k is -32.520989999956505 and b is 223.78877368050138\n",
      "Iteration 4810, the loss is 906.4372521280867, parameters k is -32.50799592881026 and b is 223.70576972793222\n",
      "Iteration 4811, the loss is 905.9129404530981, parameters k is -32.495001857664015 and b is 223.62276577536306\n",
      "Iteration 4812, the loss is 905.3887987536098, parameters k is -32.48200778651777 and b is 223.5397618227939\n",
      "Iteration 4813, the loss is 904.8648270296231, parameters k is -32.469013715371524 and b is 223.45675787022475\n",
      "Iteration 4814, the loss is 904.3410252811344, parameters k is -32.45601964422528 and b is 223.3737539176556\n",
      "Iteration 4815, the loss is 903.817393508148, parameters k is -32.443025573079034 and b is 223.29074996508643\n",
      "Iteration 4816, the loss is 903.2939317106618, parameters k is -32.43003150193279 and b is 223.20774601251728\n",
      "Iteration 4817, the loss is 902.7706398886755, parameters k is -32.41703743078654 and b is 223.12474205994812\n",
      "Iteration 4818, the loss is 902.2475180421887, parameters k is -32.4040433596403 and b is 223.04173810737896\n",
      "Iteration 4819, the loss is 901.7245661712027, parameters k is -32.39104928849405 and b is 222.9587341548098\n",
      "Iteration 4820, the loss is 901.201784275718, parameters k is -32.37805521734781 and b is 222.87573020224065\n",
      "Iteration 4821, the loss is 900.6791723557334, parameters k is -32.36506114620156 and b is 222.7927262496715\n",
      "Iteration 4822, the loss is 900.1567304112484, parameters k is -32.35206707505532 and b is 222.70972229710233\n",
      "Iteration 4823, the loss is 899.6344584422641, parameters k is -32.33907300390907 and b is 222.62671834453317\n",
      "Iteration 4824, the loss is 899.1123564487805, parameters k is -32.326078932762826 and b is 222.54371439196402\n",
      "Iteration 4825, the loss is 898.5904244307961, parameters k is -32.31308486161658 and b is 222.46071043939486\n",
      "Iteration 4826, the loss is 898.0686623883137, parameters k is -32.300090790470335 and b is 222.3777064868257\n",
      "Iteration 4827, the loss is 897.5470703213297, parameters k is -32.28709671932409 and b is 222.29470253425654\n",
      "Iteration 4828, the loss is 897.0256482298469, parameters k is -32.274102648177845 and b is 222.2116985816874\n",
      "Iteration 4829, the loss is 896.5043961138648, parameters k is -32.2611085770316 and b is 222.12869462911823\n",
      "Iteration 4830, the loss is 895.9833139733829, parameters k is -32.248114505885354 and b is 222.04569067654907\n",
      "Iteration 4831, the loss is 895.4624018084003, parameters k is -32.23512043473911 and b is 221.9626867239799\n",
      "Iteration 4832, the loss is 894.941659618919, parameters k is -32.222126363592864 and b is 221.87968277141076\n",
      "Iteration 4833, the loss is 894.4210874049389, parameters k is -32.20913229244662 and b is 221.7966788188416\n",
      "Iteration 4834, the loss is 893.9006851664566, parameters k is -32.19613822130037 and b is 221.71367486627244\n",
      "Iteration 4835, the loss is 893.380452903477, parameters k is -32.18314415015413 and b is 221.63067091370328\n",
      "Iteration 4836, the loss is 892.8603906159956, parameters k is -32.17015007900788 and b is 221.54766696113413\n",
      "Iteration 4837, the loss is 892.3404983040163, parameters k is -32.15715600786164 and b is 221.46466300856497\n",
      "Iteration 4838, the loss is 891.8207759675371, parameters k is -32.14416193671539 and b is 221.3816590559958\n",
      "Iteration 4839, the loss is 891.3012236065582, parameters k is -32.13116786556915 and b is 221.29865510342665\n",
      "Iteration 4840, the loss is 890.7818412210793, parameters k is -32.1181737944229 and b is 221.2156511508575\n",
      "Iteration 4841, the loss is 890.2626288110995, parameters k is -32.105179723276656 and b is 221.13264719828834\n",
      "Iteration 4842, the loss is 889.7435863766212, parameters k is -32.09218565213041 and b is 221.04964324571918\n",
      "Iteration 4843, the loss is 889.2247139176444, parameters k is -32.079191580984165 and b is 220.96663929315002\n",
      "Iteration 4844, the loss is 888.7060114341659, parameters k is -32.06619750983792 and b is 220.88363534058087\n",
      "Iteration 4845, the loss is 888.1874789261882, parameters k is -32.053203438691675 and b is 220.8006313880117\n",
      "Iteration 4846, the loss is 887.6691163937122, parameters k is -32.04020936754543 and b is 220.71762743544255\n",
      "Iteration 4847, the loss is 887.1509238367356, parameters k is -32.027215296399184 and b is 220.6346234828734\n",
      "Iteration 4848, the loss is 886.6329012552588, parameters k is -32.01422122525294 and b is 220.55161953030424\n",
      "Iteration 4849, the loss is 886.1150486492825, parameters k is -32.001227154106694 and b is 220.46861557773508\n",
      "Iteration 4850, the loss is 885.597366018808, parameters k is -31.98823308296045 and b is 220.38561162516592\n",
      "Iteration 4851, the loss is 885.0798533638329, parameters k is -31.975239011814203 and b is 220.30260767259676\n",
      "Iteration 4852, the loss is 884.5625106843579, parameters k is -31.962244940667958 and b is 220.2196037200276\n",
      "Iteration 4853, the loss is 884.0453379803828, parameters k is -31.949250869521713 and b is 220.13659976745845\n",
      "Iteration 4854, the loss is 883.5283352519073, parameters k is -31.936256798375467 and b is 220.0535958148893\n",
      "Iteration 4855, the loss is 883.0115024989335, parameters k is -31.923262727229222 and b is 219.97059186232013\n",
      "Iteration 4856, the loss is 882.4948397214606, parameters k is -31.910268656082977 and b is 219.88758790975098\n",
      "Iteration 4857, the loss is 881.9783469194864, parameters k is -31.89727458493673 and b is 219.80458395718182\n",
      "Iteration 4858, the loss is 881.4620240930135, parameters k is -31.884280513790486 and b is 219.72158000461266\n",
      "Iteration 4859, the loss is 880.9458712420412, parameters k is -31.87128644264424 and b is 219.6385760520435\n",
      "Iteration 4860, the loss is 880.4298883665676, parameters k is -31.858292371497996 and b is 219.55557209947435\n",
      "Iteration 4861, the loss is 879.9140754665957, parameters k is -31.84529830035175 and b is 219.4725681469052\n",
      "Iteration 4862, the loss is 879.3984325421238, parameters k is -31.832304229205505 and b is 219.38956419433603\n",
      "Iteration 4863, the loss is 878.8829595931526, parameters k is -31.81931015805926 and b is 219.30656024176687\n",
      "Iteration 4864, the loss is 878.3676566196808, parameters k is -31.806316086913014 and b is 219.22355628919772\n",
      "Iteration 4865, the loss is 877.8525236217109, parameters k is -31.79332201576677 and b is 219.14055233662856\n",
      "Iteration 4866, the loss is 877.3375605992399, parameters k is -31.780327944620524 and b is 219.0575483840594\n",
      "Iteration 4867, the loss is 876.8227675522705, parameters k is -31.76733387347428 and b is 218.97454443149024\n",
      "Iteration 4868, the loss is 876.3081444808005, parameters k is -31.754339802328033 and b is 218.8915404789211\n",
      "Iteration 4869, the loss is 875.793691384831, parameters k is -31.741345731181788 and b is 218.80853652635193\n",
      "Iteration 4870, the loss is 875.2794082643605, parameters k is -31.728351660035543 and b is 218.72553257378277\n",
      "Iteration 4871, the loss is 874.7652951193918, parameters k is -31.715357588889297 and b is 218.64252862121361\n",
      "Iteration 4872, the loss is 874.2513519499237, parameters k is -31.702363517743052 and b is 218.55952466864446\n",
      "Iteration 4873, the loss is 873.7375787559547, parameters k is -31.689369446596807 and b is 218.4765207160753\n",
      "Iteration 4874, the loss is 873.2239755374867, parameters k is -31.67637537545056 and b is 218.39351676350614\n",
      "Iteration 4875, the loss is 872.7105422945207, parameters k is -31.663381304304316 and b is 218.31051281093698\n",
      "Iteration 4876, the loss is 872.1972790270513, parameters k is -31.65038723315807 and b is 218.22750885836783\n",
      "Iteration 4877, the loss is 871.6841857350854, parameters k is -31.637393162011826 and b is 218.14450490579867\n",
      "Iteration 4878, the loss is 871.1712624186193, parameters k is -31.62439909086558 and b is 218.0615009532295\n",
      "Iteration 4879, the loss is 870.6585090776522, parameters k is -31.611405019719335 and b is 217.97849700066035\n",
      "Iteration 4880, the loss is 870.1459257121863, parameters k is -31.59841094857309 and b is 217.8954930480912\n",
      "Iteration 4881, the loss is 869.6335123222202, parameters k is -31.585416877426844 and b is 217.81248909552204\n",
      "Iteration 4882, the loss is 869.1212689077552, parameters k is -31.5724228062806 and b is 217.72948514295288\n",
      "Iteration 4883, the loss is 868.6091954687901, parameters k is -31.559428735134354 and b is 217.64648119038372\n",
      "Iteration 4884, the loss is 868.0972920053258, parameters k is -31.54643466398811 and b is 217.56347723781457\n",
      "Iteration 4885, the loss is 867.5855585173607, parameters k is -31.533440592841863 and b is 217.4804732852454\n",
      "Iteration 4886, the loss is 867.073995004898, parameters k is -31.520446521695618 and b is 217.39746933267625\n",
      "Iteration 4887, the loss is 866.5626014679334, parameters k is -31.507452450549373 and b is 217.3144653801071\n",
      "Iteration 4888, the loss is 866.0513779064705, parameters k is -31.494458379403127 and b is 217.23146142753794\n",
      "Iteration 4889, the loss is 865.5403243205066, parameters k is -31.481464308256882 and b is 217.14845747496878\n",
      "Iteration 4890, the loss is 865.0294407100438, parameters k is -31.468470237110637 and b is 217.06545352239962\n",
      "Iteration 4891, the loss is 864.5187270750818, parameters k is -31.45547616596439 and b is 216.98244956983046\n",
      "Iteration 4892, the loss is 864.0081834156193, parameters k is -31.442482094818146 and b is 216.8994456172613\n",
      "Iteration 4893, the loss is 863.4978097316566, parameters k is -31.4294880236719 and b is 216.81644166469215\n",
      "Iteration 4894, the loss is 862.9876060231959, parameters k is -31.416493952525656 and b is 216.733437712123\n",
      "Iteration 4895, the loss is 862.4775722902345, parameters k is -31.40349988137941 and b is 216.65043375955383\n",
      "Iteration 4896, the loss is 861.9677085327751, parameters k is -31.390505810233165 and b is 216.56742980698468\n",
      "Iteration 4897, the loss is 861.4580147508129, parameters k is -31.37751173908692 and b is 216.48442585441552\n",
      "Iteration 4898, the loss is 860.948490944353, parameters k is -31.364517667940675 and b is 216.40142190184636\n",
      "Iteration 4899, the loss is 860.4391371133923, parameters k is -31.35152359679443 and b is 216.3184179492772\n",
      "Iteration 4900, the loss is 859.929953257934, parameters k is -31.338529525648184 and b is 216.23541399670805\n",
      "Iteration 4901, the loss is 859.4209393779735, parameters k is -31.32553545450194 and b is 216.1524100441389\n",
      "Iteration 4902, the loss is 858.9120954735157, parameters k is -31.312541383355693 and b is 216.06940609156973\n",
      "Iteration 4903, the loss is 858.4034215445565, parameters k is -31.299547312209448 and b is 215.98640213900057\n",
      "Iteration 4904, the loss is 857.8949175910986, parameters k is -31.286553241063203 and b is 215.90339818643142\n",
      "Iteration 4905, the loss is 857.3865836131409, parameters k is -31.273559169916958 and b is 215.82039423386226\n",
      "Iteration 4906, the loss is 856.8784196106819, parameters k is -31.260565098770712 and b is 215.7373902812931\n",
      "Iteration 4907, the loss is 856.3704255837264, parameters k is -31.247571027624467 and b is 215.65438632872394\n",
      "Iteration 4908, the loss is 855.862601532269, parameters k is -31.23457695647822 and b is 215.5713823761548\n",
      "Iteration 4909, the loss is 855.3549474563117, parameters k is -31.221582885331976 and b is 215.48837842358563\n",
      "Iteration 4910, the loss is 854.8474633558557, parameters k is -31.20858881418573 and b is 215.40537447101647\n",
      "Iteration 4911, the loss is 854.3401492308992, parameters k is -31.195594743039486 and b is 215.32237051844731\n",
      "Iteration 4912, the loss is 853.8072129237896, parameters k is -31.18260067189324 and b is 215.2433191350481\n",
      "Iteration 4913, the loss is 853.2744566367379, parameters k is -31.169606600746995 and b is 215.16426775164888\n",
      "Iteration 4914, the loss is 852.7418803697485, parameters k is -31.15661252960075 and b is 215.08521636824966\n",
      "Iteration 4915, the loss is 852.2094841228163, parameters k is -31.143618458454505 and b is 215.00616498485044\n",
      "Iteration 4916, the loss is 851.6772678959451, parameters k is -31.13062438730826 and b is 214.92711360145123\n",
      "Iteration 4917, the loss is 851.145231689133, parameters k is -31.117630316162014 and b is 214.848062218052\n",
      "Iteration 4918, the loss is 850.6133755023787, parameters k is -31.10463624501577 and b is 214.7690108346528\n",
      "Iteration 4919, the loss is 850.0816993356865, parameters k is -31.091642173869523 and b is 214.68995945125357\n",
      "Iteration 4920, the loss is 849.5502031890551, parameters k is -31.078648102723278 and b is 214.61090806785435\n",
      "Iteration 4921, the loss is 849.0444934143904, parameters k is -31.065654031577033 and b is 214.5279041152852\n",
      "Iteration 4922, the loss is 848.5133366627927, parameters k is -31.052659960430788 and b is 214.44885273188598\n",
      "Iteration 4923, the loss is 847.982359931255, parameters k is -31.039665889284542 and b is 214.36980134848676\n",
      "Iteration 4924, the loss is 847.4515632197781, parameters k is -31.026671818138297 and b is 214.29074996508754\n",
      "Iteration 4925, the loss is 846.9209465283599, parameters k is -31.01367774699205 and b is 214.21169858168832\n",
      "Iteration 4926, the loss is 846.3905098570019, parameters k is -31.000683675845806 and b is 214.1326471982891\n",
      "Iteration 4927, the loss is 845.8602532057031, parameters k is -30.98768960469956 and b is 214.0535958148899\n",
      "Iteration 4928, the loss is 845.3301765744634, parameters k is -30.974695533553316 and b is 213.97454443149067\n",
      "Iteration 4929, the loss is 844.8002799632831, parameters k is -30.96170146240707 and b is 213.89549304809145\n",
      "Iteration 4930, the loss is 844.2705633721633, parameters k is -30.948707391260825 and b is 213.81644166469223\n",
      "Iteration 4931, the loss is 843.741026801102, parameters k is -30.93571332011458 and b is 213.73739028129302\n",
      "Iteration 4932, the loss is 843.2116702501021, parameters k is -30.922719248968335 and b is 213.6583388978938\n",
      "Iteration 4933, the loss is 842.6824937191601, parameters k is -30.90972517782209 and b is 213.57928751449458\n",
      "Iteration 4934, the loss is 842.1534972082782, parameters k is -30.896731106675844 and b is 213.50023613109536\n",
      "Iteration 4935, the loss is 841.6246807174562, parameters k is -30.8837370355296 and b is 213.42118474769615\n",
      "Iteration 4936, the loss is 841.0960442466939, parameters k is -30.870742964383354 and b is 213.34213336429693\n",
      "Iteration 4937, the loss is 840.56758779599, parameters k is -30.85774889323711 and b is 213.2630819808977\n",
      "Iteration 4938, the loss is 840.0393113653458, parameters k is -30.844754822090863 and b is 213.1840305974985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4939, the loss is 839.5364809407919, parameters k is -30.831760750944618 and b is 213.10102664492933\n",
      "Iteration 4940, the loss is 839.0085439051844, parameters k is -30.818766679798372 and b is 213.02197526153012\n",
      "Iteration 4941, the loss is 838.4807868896362, parameters k is -30.805772608652127 and b is 212.9429238781309\n",
      "Iteration 4942, the loss is 837.9532098941485, parameters k is -30.792778537505882 and b is 212.86387249473168\n",
      "Iteration 4943, the loss is 837.4258129187198, parameters k is -30.779784466359636 and b is 212.78482111133246\n",
      "Iteration 4944, the loss is 836.8985959633518, parameters k is -30.76679039521339 and b is 212.70576972793324\n",
      "Iteration 4945, the loss is 836.3715590280419, parameters k is -30.753796324067146 and b is 212.62671834453403\n",
      "Iteration 4946, the loss is 835.8447021127919, parameters k is -30.7408022529209 and b is 212.5476669611348\n",
      "Iteration 4947, the loss is 835.318025217603, parameters k is -30.727808181774655 and b is 212.4686155777356\n",
      "Iteration 4948, the loss is 834.7915283424718, parameters k is -30.71481411062841 and b is 212.38956419433637\n",
      "Iteration 4949, the loss is 834.2652114874005, parameters k is -30.701820039482165 and b is 212.31051281093715\n",
      "Iteration 4950, the loss is 833.7390746523893, parameters k is -30.68882596833592 and b is 212.23146142753794\n",
      "Iteration 4951, the loss is 833.2131178374376, parameters k is -30.675831897189674 and b is 212.15241004413872\n",
      "Iteration 4952, the loss is 832.6873410425476, parameters k is -30.66283782604343 and b is 212.0733586607395\n",
      "Iteration 4953, the loss is 832.1617442677135, parameters k is -30.649843754897184 and b is 211.99430727734028\n",
      "Iteration 4954, the loss is 831.6363275129403, parameters k is -30.63684968375094 and b is 211.91525589394107\n",
      "Iteration 4955, the loss is 831.1110907782279, parameters k is -30.623855612604693 and b is 211.83620451054185\n",
      "Iteration 4956, the loss is 830.6109803288023, parameters k is -30.610861541458448 and b is 211.7532005579727\n",
      "Iteration 4957, the loss is 830.0860829891247, parameters k is -30.597867470312202 and b is 211.67414917457347\n",
      "Iteration 4958, the loss is 829.5613656695085, parameters k is -30.584873399165957 and b is 211.59509779117425\n",
      "Iteration 4959, the loss is 829.0368283699509, parameters k is -30.571879328019712 and b is 211.51604640777504\n",
      "Iteration 4960, the loss is 828.5124710904508, parameters k is -30.558885256873467 and b is 211.43699502437582\n",
      "Iteration 4961, the loss is 827.9882938310133, parameters k is -30.54589118572722 and b is 211.3579436409766\n",
      "Iteration 4962, the loss is 827.4642965916348, parameters k is -30.532897114580976 and b is 211.27889225757738\n",
      "Iteration 4963, the loss is 826.9404793723148, parameters k is -30.51990304343473 and b is 211.19984087417816\n",
      "Iteration 4964, the loss is 826.416842173056, parameters k is -30.506908972288485 and b is 211.12078949077895\n",
      "Iteration 4965, the loss is 825.8933849938552, parameters k is -30.49391490114224 and b is 211.04173810737973\n",
      "Iteration 4966, the loss is 825.3701078347157, parameters k is -30.480920829995995 and b is 210.9626867239805\n",
      "Iteration 4967, the loss is 824.8470106956343, parameters k is -30.46792675884975 and b is 210.8836353405813\n",
      "Iteration 4968, the loss is 824.3240935766127, parameters k is -30.454932687703504 and b is 210.80458395718207\n",
      "Iteration 4969, the loss is 823.8013564776503, parameters k is -30.44193861655726 and b is 210.72553257378286\n",
      "Iteration 4970, the loss is 823.2787993987475, parameters k is -30.428944545411014 and b is 210.64648119038364\n",
      "Iteration 4971, the loss is 822.7564223399049, parameters k is -30.41595047426477 and b is 210.56742980698442\n",
      "Iteration 4972, the loss is 822.234225301121, parameters k is -30.402956403118523 and b is 210.4883784235852\n",
      "Iteration 4973, the loss is 821.7122082823988, parameters k is -30.389962331972278 and b is 210.40932704018599\n",
      "Iteration 4974, the loss is 821.2149771830811, parameters k is -30.376968260826033 and b is 210.32632308761683\n",
      "Iteration 4975, the loss is 820.693299559395, parameters k is -30.363974189679787 and b is 210.2472717042176\n",
      "Iteration 4976, the loss is 820.1718019557673, parameters k is -30.350980118533542 and b is 210.1682203208184\n",
      "Iteration 4977, the loss is 819.6504843721981, parameters k is -30.337986047387297 and b is 210.08916893741917\n",
      "Iteration 4978, the loss is 819.1293468086916, parameters k is -30.32499197624105 and b is 210.01011755401996\n",
      "Iteration 4979, the loss is 818.6083892652425, parameters k is -30.311997905094806 and b is 209.93106617062074\n",
      "Iteration 4980, the loss is 818.0876117418522, parameters k is -30.29900383394856 and b is 209.85201478722152\n",
      "Iteration 4981, the loss is 817.5670142385231, parameters k is -30.286009762802315 and b is 209.7729634038223\n",
      "Iteration 4982, the loss is 817.046596755252, parameters k is -30.27301569165607 and b is 209.69391202042308\n",
      "Iteration 4983, the loss is 816.5263592920436, parameters k is -30.260021620509825 and b is 209.61486063702387\n",
      "Iteration 4984, the loss is 816.0063018488922, parameters k is -30.24702754936358 and b is 209.53580925362465\n",
      "Iteration 4985, the loss is 815.4864244258005, parameters k is -30.234033478217334 and b is 209.45675787022543\n",
      "Iteration 4986, the loss is 814.9667270227707, parameters k is -30.22103940707109 and b is 209.3777064868262\n",
      "Iteration 4987, the loss is 814.4472096397978, parameters k is -30.208045335924844 and b is 209.298655103427\n",
      "Iteration 4988, the loss is 813.9278722768852, parameters k is -30.1950512647786 and b is 209.21960372002778\n",
      "Iteration 4989, the loss is 813.4087149340319, parameters k is -30.182057193632353 and b is 209.14055233662856\n",
      "Iteration 4990, the loss is 812.8897376112385, parameters k is -30.169063122486108 and b is 209.06150095322934\n",
      "Iteration 4991, the loss is 812.3952264870519, parameters k is -30.156069051339863 and b is 208.97849700066018\n",
      "Iteration 4992, the loss is 811.8765885592957, parameters k is -30.143074980193617 and b is 208.89944561726097\n",
      "Iteration 4993, the loss is 811.358130651598, parameters k is -30.130080909047372 and b is 208.82039423386175\n",
      "Iteration 4994, the loss is 810.8398527639605, parameters k is -30.117086837901127 and b is 208.74134285046253\n",
      "Iteration 4995, the loss is 810.321754896382, parameters k is -30.10409276675488 and b is 208.6622914670633\n",
      "Iteration 4996, the loss is 809.8038370488645, parameters k is -30.091098695608636 and b is 208.5832400836641\n",
      "Iteration 4997, the loss is 809.2860992214053, parameters k is -30.07810462446239 and b is 208.50418870026488\n",
      "Iteration 4998, the loss is 808.7685414140058, parameters k is -30.065110553316146 and b is 208.42513731686566\n",
      "Iteration 4999, the loss is 808.2511636266657, parameters k is -30.0521164821699 and b is 208.34608593346644\n"
     ]
    }
   ],
   "source": [
    "#initialized parameters\n",
    "\n",
    "k = random.random() * 200 - 100  # -100 100\n",
    "b = random.random() * 200 - 100  # -100 100\n",
    "\n",
    "learning_rate = 1e-0\n",
    "\n",
    "iteration_num = 5000\n",
    "losses = []\n",
    "for i in range(iteration_num):\n",
    "    \n",
    "    price_use_current_parameters = [price(r, k, b) for r in X_rm]  # \\hat{y}\n",
    "    \n",
    "    current_loss = loss(y, price_use_current_parameters)\n",
    "    losses.append(current_loss)\n",
    "    print(\"Iteration {}, the loss is {}, parameters k is {} and b is {}\".format(i,current_loss,k,b))\n",
    "    \n",
    "    k_gradient = partial_derivative_k(X_rm, y, price_use_current_parameters)\n",
    "    b_gradient = partial_derivative_b(y, price_use_current_parameters)\n",
    "    \n",
    "    k = k + (-1 * k_gradient) * learning_rate\n",
    "    b = b + (-1 * b_gradient) * learning_rate\n",
    "best_k = k\n",
    "best_b = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13bf4539470>]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(iteration_num)),losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x13bf45c64a8>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "price_use_best_parameters = [price(r, best_k, best_b) for r in X_rm]\n",
    "\n",
    "plt.scatter(X_rm,y)\n",
    "plt.scatter(X_rm,price_use_current_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 调了好多次学习率和学习次数，很难调到比较好的程度，loss函数非2次曲面导致学习效果差，模型欠拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
