{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习上课内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.理论题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why we need $\\gamma$ in reinforcement learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma是对过去的Reward的一种惩罚，它可以避免无限循环链出现奖励无穷大的情况。\n",
    "\n",
    "$G_t=R_{t+1}+\\gamma{R_{t+2}}+\\gamma^{2}{R_{t+3}}+\\gamma^{3}{R_{t+4}}+\\cdots$\n",
    "\n",
    "另外一种解释方法是：我们需要模型更偏爱眼前的利益，而对于未来的利益不那么敏感"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Please breifly explain what is value function and what is Q function ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Value Function:**\n",
    "\n",
    "$V_\\pi(s)=E_\\pi[G_t|S_t=s]$，是在某个状态s下，Rewards的期望值。适用于路线不唯一且具有随机性的步骤。\n",
    "\n",
    "其中 $G_t=R_{t+1}+\\gamma{R_{t+2}}+\\gamma^{2}{R_{t+3}}+\\gamma^{3}{R_{t+4}}+\\cdots$\n",
    "\n",
    "**Q Function:**\n",
    "\n",
    "$Q_\\pi(s,a)=E_\\pi[G_t|S_t=s,A_t=a]$，是在某个状态s下，已经采取动作a后，Rewards的期望值。\n",
    "\n",
    "其中 $G_t=R_{t+1}+\\gamma{R_{t+2}}+\\gamma^{2}{R_{t+3}}+\\gamma^{3}{R_{t+4}}+\\cdots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How temperal difference related to dynamic programming and monte-carlo methods ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**动态规划算法会遍历所有状态点来计算，而蒙特卡洛只会通过采样的状态点来计算**\n",
    "\n",
    "**动态规划适用于状态空间较小的情况，蒙特卡洛适合于复杂状态空间情况**\n",
    "\n",
    "**Temporal Difference Learning则是动态规划和蒙特卡洛的结合**\n",
    "\n",
    "**即，一部分用蒙特卡洛采样解，一部分用Bellman Equation解。**\n",
    "\n",
    "优劣势比较：\n",
    "\n",
    "**TD相较于蒙特卡洛，bias升高（需要用概率估计值），variance降低（减少了随机采样）**\n",
    "\n",
    "**TD相较于蒙特卡洛，计算更快；相较于DP，不需要知道Transition Probability。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Please briefly describe what are value iteration and policy iteration ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**值迭代**先对所有状态**初始化一个值**，然后使用Value Function$V(S_t)=r+\\sum_{P}{P(S_{t+1}|S_t)}V(S_{t+1})$进行值的更新，然后不断迭代，直到收敛 |W_t+1 - W_t| ≤ δ：\n",
    "![](http://uricc.ga/images/2020/02/07/Bellman-iteration.png)\n",
    "\n",
    "其中有两种做法，一种是一次计算，同时更新，另一种是将更新后的值作为下一个格子的输入值进行更新。这两种做法都是可取的。\n",
    "\n",
    "**策略迭代**先对所有状态**初始化一个随机策略**，然后使用Q-function，进行最佳策略的更新，然后不断迭代，直到收敛。\n",
    "\n",
    "![](http://uricc.ga/images/2020/02/07/Bellman-iteration2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How can we use deep lerning in reinforcement learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以用Deep Learning来当作Q函数的函数拟合方法。\n",
    "\n",
    "输入states，输出Q-value for different actions（每个动作所应该有的概率值）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 选做题 （实践）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.table import Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_image(image):\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.set_axis_off()\n",
    "    tb = Table(ax,bbox=[0,0,1,1])\n",
    "\n",
    "    nrows,ncols = image.shape\n",
    "    width,height = 1.0/ncols,1.0/nrows\n",
    "\n",
    "    for (i,j), val in np.ndenumerate(image):\n",
    "        if (i,j) == (0,1):\n",
    "            val = \"A\"\n",
    "        elif (i,j) == (0,3):\n",
    "            val = \"B\"\n",
    "        elif (i,j) == (4,1):\n",
    "            val = \"A'\"\n",
    "        elif (i,j) == (2,3):\n",
    "            val = \"B'\"\n",
    "        tb.add_cell(i,j,width,height,text=val,\n",
    "                    loc='center',facecolor='white')\n",
    "\n",
    "    for i in range(len(image)):\n",
    "        tb.add_cell(i,-1,width,height,text=i+1,loc='right',\n",
    "                    edgecolor='none',facecolor='none')\n",
    "        tb.add_cell(-1,i,width,height/2,text=i+1,loc='center',\n",
    "                    edgecolor='none',facecolor='none')\n",
    "    ax.add_table(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "WORLD_SIZE=5\n",
    "draw_image(np.zeros((WORLD_SIZE,WORLD_SIZE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure above shows a rectangular gridworld. The cell of the grid correspond to the state of the environment. At each cell, four actions with equal probability are possible: north, south, east and west, which deterministically cause the agent to move one cell in the respective direction on the grid. Actions that would take the agent off the grid leave its unchanged, but also result in a reward -1. Other actions result in a reward of 0, expect those taht move the agent out of the special states A and B. From state A, all four actions yield a reward of +10 and take the agent to A'. From state B, all actions yield a reward of +5 and take the agent to B'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try to compute the value of each state ? You can choose any algorithms we leanred in the class.\n",
    "Good luck and happy new year. !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
