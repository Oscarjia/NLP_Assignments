{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoEncoder，自编码器，本质上是一个Sequence2sequence模型，只不过它的输入和输出是完全一样的。这样的话这个sequence2sequence的目的就是对一个序列进行编码，将信息压缩到一个高维向量，然后对这个高维向量解码并能够还原出它原本的信息。\n",
    "如此一来，高维向量latent vector就能学习到输入信息最精华的特征表示。\n",
    "\n",
    "![](http://5b0988e595225.cdn.sohucs.com/images/20180228/7bec47a0511a43f196d03154a8a4ff3c.png)\n",
    "\n",
    "应用：图像降维，压缩，预训练，特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贪婪搜索：保证每个词单独的概率最大 $P(y^{i}|x)$\n",
    "\n",
    "束搜索：每个时间步给出k个概率最大的词，然后用这k个词分别到下一个节点去产生k个词，一直到碰到end of sentence（k为自定义的值），这样生成的答案就是一棵树，每条路径代表一种可能的答案。然后我们就可以求$P(y|x)=P(y^{1}|x)\\cdot P(y^{2}|x,y^{1})\\cdots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "人在解码信息比如翻译一句句子的时候，一般也并不是把句子全部看完理解了，再一下子翻译出来的，而是局部聚焦的。\n",
    "\n",
    "比如：看到I，那么自然可以翻译出我，看到swim，自然就可以翻译出游泳。因此可以把上文的词和下文的输出尝试直接联系起来。(而非仅仅依赖于latent vector)\n",
    "\n",
    "具体计算中，每一个时间步的输出$y^i$，会考虑三部分的信息：\n",
    "\n",
    "1. $s^{i-1}$\n",
    "2. $context^{i}$\n",
    "3. $y^{i-1}$\n",
    "\n",
    "其中，$context = \\sum_{i=1}^{T_x}{\\alpha^{<t, t'>}\\cdot a^{t}}$\n",
    "\n",
    "a为encoder时间步的输出，$\\alpha$为softmax(decoder前一时间步$s_{t-1}$与$a_{t}$的函数)\n",
    "\n",
    "$s_{t-1}$与$a_{t}$的函数有很多求法，其中三种：\n",
    "1. $S_{t-1}\\cdot w \\cdot a_{t'}$\n",
    "2. $S_{t-1}\\cdot a_{t'}$  (outputs a value)\n",
    "3. $V^T\\cdot tanh(W_1 \\cdot a_t + W_2 \\cdot S_{t-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 无法解决多义词的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 2层的双向LSTM\n",
    "2. 通过预测下一个词来训练模型\n",
    "3. 将输入层(embedding)，第一隐藏层，第二隐藏层的三条向量通过某种函数f变换，得到这个词最终的向量，比如加权平均（权重通过学习获得）\n",
    "\n",
    "使用方法：\n",
    "1. ELMo直接作为输入\n",
    "2. word embedding 和 ELMo 拼接\n",
    "\n",
    "ELMo可以作为新语料库的提前训练方法，来适应新的语料库。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 长距离依赖问题可以处理的更好\n",
    "2. 可以并行计算，计算速度快"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为通过attention计算后得到的Z，和原始的X分布是不统一的，所以拼接到一起后的（X+Z）需要做一个标准化，使他们每个维度的分布统一起来\n",
    "![](https://uricc.ga/images/2019/12/09/_20191209122549.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为Transformer舍弃了传统RNN的序列模式，舍弃了序列信息。\n",
    "\n",
    "因此需要把序列的信息通过position embedding的方式加入模型中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=1321670134,149782824&fm=26&gp=0.jpg)\n",
    "\n",
    "自注意力：将输入信息做线性变换，然后求它自己与其它输入序列的注意力的一种网络模型\n",
    "\n",
    "多头注意力：进行多次不同的Attention（Q，K，V）操作（参数不共享），每次都产生一个不同的Zi，最后把Zi拼接到一起"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://uricc.ga/images/2019/12/09/2.png)\n",
    "如果要训练语言模型，就用源语言当成输出\n",
    "\n",
    "如果要做分类，则输出类别\n",
    "\n",
    "如果要做序列标注（实体识别，句法分析，语言模型），则输出序列类别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask language model是使用Transformer来训练language model的一种模型。\n",
    "\n",
    "输入词有80%几率被替换成【MASK】标签，10%几率随机替换成别的标签，10%几率不变。然后网络的目标就是预测【MASK】位置的词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word/Token Embeddings + Segment Embeddings + Position Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://uricc.ga/images/2019/12/09/69abec74be7950f852c4bd83e1cdbdbc.png)\n",
    "如果要做分类，则输出类别\n",
    "\n",
    "如果要做阅读理解，则输入问题+短文，输出答案所在短文的位置（序列标注）\n",
    "\n",
    "如果要做序列标注则输出序列标注\n",
    "\n",
    "……"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT：单向的Transformer(采用mask，输出层只给以前的输入信息，对角阵)\n",
    "\n",
    "BERT: 双向Transformer结构（采用mask model）\n",
    "\n",
    "GPT2: 结构和GPT一样，但是使用了更多数据，更多参数（Transformer层数）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
